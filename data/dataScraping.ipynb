{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping UK asylum data (25 October 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scraps the tribunal decision's data related to asylum applications from https://tribunalsdecisions.service.gov.uk/\n",
    "\n",
    "The script mac_env.sh should be run from the terminal to install the necessary libraries and to activate the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment: /Users/albertamurgopacheco/anaconda3/envs/tfm/bin/python\n",
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import whois\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import *\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import urllib.request\n",
    "import wget\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# What environment am I using?\n",
    "print(f'Current environment: {sys.executable}')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/albertamurgopacheco/Documents/GitHub/TFM')\n",
    "# What's my working directory?\n",
    "print(f'Current working directory: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directories in colab and local execution\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    docs_path = '/content/gdrive/MyDrive/TFM/data/raw'\n",
    "    input_path = '/content/gdrive/MyDrive/TFM'\n",
    "    output_path = '/content/gdrive/MyDrive/TFM/output'\n",
    "\n",
    "else:\n",
    "    docs_path = './data/raw'\n",
    "    input_path = '.'\n",
    "    output_path = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scrap the data in the UIATC landing page using beautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(htmlSource):\n",
    "    \"\"\"\n",
    "    getData gets the mouse over links to the tribunal decisions & the dates\n",
    "\n",
    "    :param htmlSource: Source HTML for the page \n",
    "    :return: data as a list of tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    # Scraping tribunal decision names with BeautifulSoup\n",
    "    soup = BeautifulSoup(htmlSource, 'html')\n",
    "    \n",
    "    # SCRAPING MOUSE OVER LINKS\n",
    "    linksList = re.findall(r'(<a href=\"/utiac/(.*?)\">)', htmlSource)\n",
    "    linksList = [i[1] for i in linksList]\n",
    "    linksList = list(set(linksList))\n",
    "    #print(\"Number of links:\",len(linksList))\n",
    "    #print(linksList)\n",
    "    \n",
    "    # SCRAPING DATES\n",
    "\n",
    "    # Find by class the dates and store in list\n",
    "    datesList = list(soup.find_all(\"td\", class_=\"date\"))\n",
    "    # Convert the list elements to string  \n",
    "    datesList = [str(i) for i in datesList]\n",
    "    # Slice the part of string including data (date format yyyy-mm-dd)\n",
    "    datesList = [i[33:43] for i in datesList]\n",
    "    #print(datesList)\n",
    "    #print(\"Number of dates %s\",len(datesList))\n",
    "\n",
    "    # Assign data to tuples: # get the list of tuples from two lists and merge them by using zip()\n",
    "    tuplesList = list(zip(linksList, datesList))\n",
    "    \n",
    "    return tuplesList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to obtain the data from the court's decision detailed page using beautifulSoup.     TO DO: 2. Try with different files make sure it works, 3. Capture exceptions and 204 responses. 4. Create function, 5. How am I storing the dicts? In a list? 5. Try function with just a few obs. https://stackoverflow.com/questions/20638006/convert-list-of-dictionaries-to-a-pandas-dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetailedData(url):\n",
    "    \"\"\"\n",
    "    getDetailedData gets the detailed data linked to a tribunal decision\n",
    "    and saves a doc file in /Users/albertamurgopacheco/Documents/GitHub/TFM/data/raw\n",
    "\n",
    "    :param url: url (link) to the page containing the detailed info \n",
    "    :return: dictionary \n",
    "    \"\"\"\n",
    "    \n",
    "    # START WITH URL\n",
    "    try:\n",
    "        response = requests.get(url = url)\n",
    "        \n",
    "        # if response status code is 200 OK, then\n",
    "        if response.status_code == 200:\n",
    "        # load the data\n",
    "            data = response.text\n",
    "            soup = BeautifulSoup(data, 'html')\n",
    "    \n",
    "            # Scrape the reference number\n",
    "            refList = list(soup.find_all(\"h1\"))\n",
    "            # Convert the list elements to string  \n",
    "            refList = [str(i) for i in refList]\n",
    "            # Remove leading <h1> and trailing </h1>\n",
    "            refList = [i.replace('</h1>', '') for i in refList]\n",
    "            refList = [i.replace('<h1>', '') for i in refList]\n",
    "            #print(refList)\n",
    "    \n",
    "            # Find the link (docLink) to the document\n",
    "            lnk = re.findall(r'(<a class=\"doc-file\" href=\"https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/(.*?)\")', data)\n",
    "            # Build link using the second element in regex result (list of tuples)\n",
    "            docLink = \"https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/\" + lnk[0][1]\n",
    "            # Download files to raw folder\n",
    "            try:\n",
    "                filename = wget.download(url = docLink, out = docs_path)\n",
    "                downloaded = \"Yes\"\n",
    "            # Handle download exceptions \n",
    "            except Exception as err:\n",
    "                print(\"Could not download file {}\".format(docLink))\n",
    "                print(err)\n",
    "                downloaded = \"No\"\n",
    "                pass\n",
    "     \n",
    "            # Find detailed information\n",
    "            res = [item.get_text() for item in soup.select(\"span\")]\n",
    "            # Remove \\xa0 from strings\n",
    "            res = [elem if '\\xa0' not in elem else elem.replace('\\xa0', '') for elem in res]\n",
    "            # Remove trailing and leading spaces and \\n\n",
    "            res = [elem.strip() for elem in res]\n",
    "            #print(res)\n",
    "\n",
    "            # Split list of results into two lists (keys & values)\n",
    "            keysList = res[::2] # Keys: Elements from res starting from 0 iterating by 2\n",
    "            valuesList = res[1::2] # Values: Elements from res starting from 1 iterating by 2\n",
    "            #print(keysList)\n",
    "            #print(valuesList)\n",
    "    \n",
    "            # Create dictionary with results (resDict)\n",
    "            zip_iterator = zip(keysList, valuesList)\n",
    "            resDict = dict(zip_iterator)\n",
    "    \n",
    "            # Add reference number and link to document to the dictionary\n",
    "            resDict[\"Document\"] = docLink\n",
    "            resDict[\"Reference\"] = refList\n",
    "            resDict[\"Download\"] = downloaded\n",
    "            resDict[\"File\"] = lnk[0][1]\n",
    "            #print(resDict)\n",
    "            \n",
    "        else:\n",
    "            resDict = {\"URL not working:\": str(url)}\n",
    "            print(f\"URL not working: {url}\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:  # Capture exceptions\n",
    "        print (e.response.text)\n",
    "        raise SystemExit(e)   \n",
    "    \n",
    "    return resDict  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined all the necessary functions, we can open a browser and start scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tribunal decisions\n"
     ]
    }
   ],
   "source": [
    "# Using selenium, open the tribunal decision's website in firefox\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://tribunalsdecisions.service.gov.uk/\")\n",
    "\n",
    "# Getting current URL source code \n",
    "get_title = driver.title \n",
    "  \n",
    "# Printing the title of the URL \n",
    "print(get_title) \n",
    "assert \"Tribunal decisions\" in driver.title\n",
    "\n",
    "# Getting current URL source code \n",
    "get_source = driver.page_source\n",
    "time.sleep(2)\n",
    "#print(get_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Obtain a dataframe with the link and the date of each of the tribunal decisions. \n",
    "  1. GET DATES\n",
    " 2. Build function that gets data per page\n",
    " 3. Loop over 1667 pages and append the lists\n",
    " 4. Build dataframe or iterate over each list to 1) open url 2) extract additional content, and 3) download file using class doc-file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scrape current page data and browse to next page\n",
    "\n",
    "# List of tuples to store the results from getData()\n",
    "a = []\n",
    "\n",
    "#while True:\n",
    "i=1\n",
    "while i<1178:\n",
    "\n",
    "    # Getting current URL source code \n",
    "    get_source = driver.page_source\n",
    "    # Scrape the data\n",
    "    b = getData(get_source)\n",
    "    # Append list data b to list data a\n",
    "    a += b\n",
    "    i+=1\n",
    "    \n",
    "    # Click on next page\n",
    "    try:\n",
    "        delay = 15 # seconds\n",
    "        #element_present = EC.presence_of_element_located((By.CLASS_NAME, 'next_page'))\n",
    "        #WebDriverWait(driver, delay).until(element_present)\n",
    "        \n",
    "        wait = WebDriverWait(driver, delay)\n",
    "        element = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'next_page')))\n",
    "        element.click()\n",
    "            # wait       \n",
    "    except TimeoutException:\n",
    "        print(\"Loading took too much time!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of urls to scrap detailed data from: 35286\n"
     ]
    }
   ],
   "source": [
    "# List of links with the decision files\n",
    "decisionLinks = [tple[0] for tple in a]\n",
    "\n",
    "# Number of urls to crap detailed data\n",
    "print(f'Number of urls to scrap detailed data from: {len(decisionLinks)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we scrape the detailed data on each tribunal decission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download file https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/51943/AA055522014.doc\n",
      "HTTP Error 500: Internal Server Error\n",
      "Could not download file https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/40081/IA083642010___IA083692010___IA083752010.DOC\n",
      "HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fc/h07gydfn7nd0zq3mglwpn3r40000gn/T/ipykernel_3840/3548273256.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Scrap detailed data from all urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mscrapedItem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDetailedData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#print(scrapedItem)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscrapedList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrapedItem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fc/h07gydfn7nd0zq3mglwpn3r40000gn/T/ipykernel_3840/823983695.py\u001b[0m in \u001b[0;36mgetDetailedData\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mlnk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(<a class=\"doc-file\" href=\"https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/(.*?)\")'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Build link using the second element in regex result (list of tuples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mdocLink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlnk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Download files to raw folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mrawFolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Create a list of urls from links\n",
    "urls = [ \"https://tribunalsdecisions.service.gov.uk/utiac/\"+decision for decision in decisionLinks]\n",
    "# Item 35014 crashes the loop\n",
    "urls.pop(35014)\n",
    "\n",
    "# List of dict where each dict contains scraped detailed data\n",
    "scrapedList = []\n",
    "\n",
    "# Scrap detailed data from all urls\n",
    "for url in urls:\n",
    "    scrapedItem = getDetailedData(url)\n",
    "    #print(scrapedItem)\n",
    "    scrapedList.append(scrapedItem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempted download item urls[35014] crashes the loop\n",
    "# scraping remaining items\n",
    "\n",
    "# Scrap detailed data from all urls\n",
    "for url in urls[-245:]:\n",
    "    scrapedItem = getDetailedData(url)\n",
    "    #print(scrapedItem)\n",
    "    scrapedList.append(scrapedItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 35257 elements have been scraped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://tribunalsdecisions.service.gov.uk/utiac/2003-ukiat-7478'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of scraped court decisions\n",
    "print(f'A total of {len(scrapedList)} elements have been scraped')\n",
    "\n",
    "# Number of documents scraped\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could not download file https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/51943/AA055522014.doc\n",
    "HTTP Error 500: Internal Server Error\n",
    "Could not download file https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/40081/IA083642010___IA083692010___IA083752010.DOC\n",
    "HTTP Error 403: Forbidden\n",
    "\n",
    "https://tribunalsdecisions.service.gov.uk/utiac/2003-ukiat-7478\n",
    "\n",
    "urls[35014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Appellant name:\": \"\",\n",
      "    \"Case title:\": \"\",\n",
      "    \"Country:\": \"\",\n",
      "    \"Document\": \"https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/42595/OA140542013.doc\",\n",
      "    \"Download\": \"Yes\",\n",
      "    \"Hearing date:\": \"24 Jul 2014\",\n",
      "    \"Judges:\": \"\",\n",
      "    \"Last updated on:\": \"5 Dec 2014\",\n",
      "    \"Promulgation date:\": \"12 Aug 2014\",\n",
      "    \"Publication date:\": \"5 Dec 2014\",\n",
      "    \"Reference\": [\n",
      "        \"OA/14054/2013\"\n",
      "    ],\n",
      "    \"Status of case:\": \"Unreported\"\n",
      "}\n",
      "35004\n"
     ]
    }
   ],
   "source": [
    "# Save as json\n",
    "jsonData = json.dumps(scrapedList)\n",
    "# Save jsonData as jsonFile in working directory\n",
    "jsonFile = open(\"jsonData.json\", \"w\")\n",
    "jsonFile.write(jsonData)\n",
    "jsonFile.close()\n",
    "\n",
    "# Open json\n",
    "parsed = json.loads(jsonData)\n",
    "print(json.dumps(parsed[30000], indent = 4, sort_keys = True))\n",
    "print(len(scrapedList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as pickle\n",
    "with open('pickleData.pkl', 'wb') as f:\n",
    "    pickle.dump(scrapedList, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Open pickle file\n",
    "with open('pickleData.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Case title:': '', 'Appellant name:': '', 'Status of case:': 'Unreported', 'Hearing date:': '10 Aug 2021', 'Promulgation date:': '17 Sep 2021', 'Publication date:': '4 Oct 2021', 'Last updated on:': '4 Oct 2021', 'Country:': '', 'Judges:': '', 'Document': 'https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/73573/PA027032020.doc', 'Reference': ['PA/02703/2020'], 'Download': 'Yes'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Open jsonData file\n",
    "jsonData_path = os.path.join(os.getcwd(), 'jsonData.json')\n",
    "with open(jsonData_path) as json_file:\n",
    "    data = json.load(json_file)\n",
    "print(data[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da6e46dab1e0b3d9fa32aec1170dd2df7038a4f7be3a54c97a348d8ad782954"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

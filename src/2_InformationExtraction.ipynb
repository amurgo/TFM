{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction (18th November 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts additional information from the text of the tribunal decisions and stores it in the relevant dictionary.\n",
    "\n",
    "In particular, the notebook performs information extraction on:\n",
    "\n",
    "1. The label included in the name of the file ('Code label:').\n",
    "\n",
    "2. The court where the case was heard ('Heard at').\n",
    "\n",
    "3. The judges ('Judges:').\n",
    "\n",
    "4. The legal representation ('Representation:') for the appellant ('Representation appellant:') and the respondent ('Representation respondent:').\n",
    "\n",
    "5. The decision/ruling by the judge ('Decision:').\n",
    "\n",
    "6. The sense of the decision/ruling ('Decision label:').\n",
    "\n",
    "7. The nationality of the the subject of the case (appellant or respondent). \n",
    "\n",
    "\n",
    "Each of these fields is added to the dictionary of each judicial decision.\n",
    "\n",
    "The resulting data set - a list of updated dictionaries -  is serialised as a json object (jsonDataFinal.json).\n",
    "\n",
    "This notebook should run in the tfm environment, which can be created with the environment.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment: /Users/albertamurgopacheco/anaconda3/envs/tfm/bin/python\n",
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, getsize\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import whois\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import textract\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import stanza\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# What environment am I using?\n",
    "print(f'Current environment: {sys.executable}')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/albertamurgopacheco/Documents/GitHub/TFM')\n",
    "# What's my working directory?\n",
    "print(f'Current working directory: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directories in colab and local execution\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    docs_path = '/content/gdrive/MyDrive/TFM/data/raw'\n",
    "    input_path = '/content/gdrive/MyDrive/TFM'\n",
    "    output_path = '/content/gdrive/MyDrive/TFM/output'\n",
    "\n",
    "else:\n",
    "    docs_path = './data/raw'\n",
    "    input_path = '.'\n",
    "    output_path = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFORMATION EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Auxiliary functions and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture whether all elements exist in a list\n",
    "def sublist(sublist, lst):\n",
    "    \"\"\"\n",
    "    Given a list of sentences/lists all_exist checks whether sublist list exists in lst\n",
    "\n",
    "    :sublist: list to search\n",
    "    :lst: list to be searched\n",
    "    :return: the list with the match\n",
    "    \"\"\"\n",
    "    if not isinstance(sublist, list):\n",
    "        raise ValueError(\"sublist must be a list\")\n",
    "    if not isinstance(lst, list):\n",
    "        raise ValueError(\"lst must be a list\")\n",
    "\n",
    "    sublist_len = len(sublist)\n",
    "    k=0\n",
    "    s=None\n",
    "\n",
    "    if (sublist_len > len(lst)):\n",
    "        return False\n",
    "    elif (sublist_len == 0):\n",
    "        return True\n",
    "\n",
    "    for x in lst:\n",
    "        if x == sublist[k]:\n",
    "            if (k == 0): s = x\n",
    "            elif (x != s): s = None\n",
    "            k += 1\n",
    "            if k == sublist_len:\n",
    "                return True\n",
    "        elif k > 0 and sublist[k-1] != s:\n",
    "            k = 0\n",
    "\n",
    "    return False\n",
    "\n",
    "# Function to capture if all elements exist in a list\n",
    "def all_exist(avalue, bvalue):\n",
    "    \"\"\"\n",
    "    Given a list of sentences/lists all_exist checks whether avalue list exists in bvalue\n",
    "\n",
    "    :avalue: list to search\n",
    "    :bvalue: list to be searched\n",
    "    :return: the list with the match\n",
    "    \"\"\"\n",
    "    return all(any(x in y for y in bvalue) for x in avalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The label included in the name of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two categories of cases: the reported and the unreported ones. The reported cases include richer data while the unreported ones (the vast majority of cases) miss several data fields due to a request for annonimity from any of the parties involved in the legal dispute.\n",
    "\n",
    "The first two letters in the file name seem to follow some logic. Inspecting the documents reveals the following meanings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 1588874.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each decision and extract first two characters of the file's name\n",
    "for decision in tqdm(data):\n",
    "    # Only 'unteported' decisions include this 2-letter code\n",
    "    if decision.get('Status of case:') == 'Unreported':\n",
    "        string_code = decision.get('File')[:2]\n",
    "    else:\n",
    "        string_code = 'NA'\n",
    "    \n",
    "    # Add dictionary key 'Code label' with value string to the dictionary\n",
    "    decision.update({'Code label:': string_code})\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The court where the case was heard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inspection of a sample of judicial decisions reveals that the name of the court is located in the first part of the document and it usually follows the expression \"Heard at\".\n",
    "\n",
    "The strategy to capture this field will consist of a search using regular expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:01<00:00, 27089.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    decision_string = decision.get('String')\n",
    "    # Deal with empty/corrupt files that didn't upload a sentence string\n",
    "    if decision_string:\n",
    "        # Regex expression: What comes after \"Heard at\" until hitting 3 balnks or new line\n",
    "        #regex = '(?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        regex = 'Heard at(.*)[\\S\\r\\n]| (?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        catch = re.search(regex, decision_string)\n",
    "\n",
    "        # If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "            # Remove ':' if included in the catch\n",
    "            string = string.replace(':','')\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            # Avoids picking up parts of tables and '|'\n",
    "            string = string.split('   ')\n",
    "            string = string[0]\n",
    "            # Remove 'Heard at' if included in the catch\n",
    "            string = string.replace('Heard at ','')\n",
    "            # Remove 'manually' some strings often included in the catch\n",
    "            string = string.replace('|Decision & Reasons Promulgated','')\n",
    "            string = string.replace('|Decision and Reasons Promulgated','')\n",
    "            string = string.replace('| Decision & Reasons Promulgated','')\n",
    "            string = string.replace('Decision Promulgated','')\n",
    "            string = string.replace('|Decision & Reasons promulgated','')\n",
    "            string = string.replace('|Determination Promulgated','')\n",
    "            string = string.replace('Decision and Reasons Promulgated','')\n",
    "            string = string.replace('|Decision & Reasons  Promulgated','')\n",
    "            string = string.replace(' on 4 July 2003','')\n",
    "            string = string.replace('Determination Promulgated','')\n",
    "            string = string.replace('Decision & Reasons Promulgated','')\n",
    "            string = string.replace('|Decisions and Reasons Promulgated','')\n",
    "            string = string.replace('|Decision and Reasons','')\n",
    "            string = string.replace('UT(IAC)','')\n",
    "            string = string.replace('UT (IAC) ','')\n",
    "            string = string.replace('Date of Hearing  9 December 2005','')\n",
    "            string = string.replace(' | |SS (Risk-Manastry) Iran CG [2003] UKIAT 00035 |','')\n",
    "            # Strip of often found trailing characters\n",
    "            string = string.rstrip(',')\n",
    "            string = string.rstrip('|')\n",
    "            # Remove leading and trailing spaces (again)\n",
    "            string = string.strip()\n",
    "        else:\n",
    "            string = 'NA'\n",
    "        #print(string)\n",
    "        # Add dictionary key 'Heard at' with value string to the dictionary\n",
    "        decision.update({'Heard at:': string})\n",
    "    else:\n",
    "        continue\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The judges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:01<00:00, 24965.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    decision_string = decision.get('String')\n",
    "    # Deal with empty/corrupt files that didn't upload a sentence string\n",
    "    if decision_string:\n",
    "        # Regex expression: What comes in between 'Before' and 'Between'\n",
    "        regex = '(?<=Before)([\\s\\S]*?)(?=Between)'\n",
    "        catch = re.search(regex, decision_string)\n",
    "        #If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "\n",
    "            # Get rid of some table delimiters\n",
    "            string = string.replace('|','')\n",
    "            string = string.replace('?','')\n",
    "            string = string.replace(',','')\n",
    "\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            \n",
    "            # Split strings (spaces > 3 usually indicates two \"joint\" names)\n",
    "            # Alternative approach tried and discarded base on sentence tokenization \n",
    "            # from nltk.tokenize import sent_tokenize\n",
    "            listNames = string.split(\"   \")\n",
    "            # Make list of names with strings containijng names\n",
    "            # Capitalize the first letter of each word & delete \n",
    "            listNames = [name.strip().title() for name in listNames if name.strip()]\n",
    "\n",
    "            # Discard content in brackets as it's mostly titles and clutter\n",
    "            listNames = [re.sub('[\\(\\[].*?[\\)\\]]', '', x).strip() for x in listNames]\n",
    "\n",
    "            # Finally, delete titles, positions held and other clutter around the name\n",
    "            clutter = ['Judge', 'Tribunal', 'Court', 'Upper', 'Deputy', 'Senior', 'Of', 'The', 'Mr', 'Dr', 'Vice', 'President',\n",
    "            ':', 'Honourable', 'Hon.', '', '- - - - - - - - - - - - - - - - - - - -', 'Ut', 'Trinbunal', '-And-', 'Mrs', 'President,',\n",
    "            'Tribnunal', '-', 'Hon', 'And', 'Chairman', 'Vice-President', 'Immigration', 'Asylum Chamber', '-Vice', '(Senior',\n",
    "            '...............', 'Designated', 'His Honour', 'Respondent Representation: For Appellant', 'Secretary State For Home Department',\n",
    "            'Appellant', 'Lord', 'Sir', 'In Matter An Application For Judicial Review', 'I) Eu Regulation Number 604/2013 Human',\n",
    "            'Miss', 'Ms.', ':-']\n",
    "\n",
    "            # \n",
    "            listNames = [' '.join(filter(lambda x: x not in clutter,  name.split())) for name in listNames]\n",
    "            # Remove remaining 'issues' with empty strings ''\n",
    "            listNames = list(filter(None, listNames))\n",
    "            # Add a . following individual letters\n",
    "\n",
    "            #print(listNames)\n",
    "            \n",
    "        else:\n",
    "            listNames = ['NA']\n",
    "        \n",
    "        #print(decision.get('File'))\n",
    "        #print(listNames)\n",
    "        # Add dictionary key 'Judges:' with value list of strings to the dictionary\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 2771630.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# 'Manually' fix some mistakes with some judges\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "\n",
    "    if decision.get('File') == '00046_ukut_iac_2020_ps_iran_cg':\n",
    "        listNames = ['J Barnes', 'A R Mackey', 'S L Batiste']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00393_ukut_iac_2019__jw_ors_ijr':\n",
    "        listNames = ['Rimington Jackson']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '2004_ukiat_00248_gh_iraq_cg':\n",
    "        listNames = ['Rintoul', 'Bruce']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00270_ukut_iac_2015_mmw_ijr':\n",
    "        listNames = ['Justice Mccloskey']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00271_ukut_iac_2015_bh_ijr':\n",
    "        listNames = ['Justice Mccloskey', \"O'Connor\"]\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == 'AA082212015':\n",
    "        listNames = ['Alis', 'I K']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The legal representation for the appellant and the respondent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The legal team consists of the representation for the appellant and the respondent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [5:05:36<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "representation = []\n",
    "files_legal = []\n",
    "\n",
    "# nlp sentence tokenizer with Stanford\n",
    "nlp = stanza.Pipeline(lang = 'en', processors = 'tokenize', tokenize_no_ssplit = True)\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    decision_string = decision.get('String')\n",
    "    file_name = decision.get('File')\n",
    "    files_legal.append(file_name)\n",
    "    #print(file_name)\n",
    "    # Use only first third of text\n",
    "    string = decision_string[:len(decision_string)//3]\n",
    "    # All text in lower\n",
    "    string = string.lower()\n",
    "    # Apply stanford nlp to string\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # List to store the ruling sentences\n",
    "    catch = []\n",
    "\n",
    "    # Make sentences\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sente = [token.text for token in sentence.tokens]\n",
    "        # Keep only the alpha tokens\n",
    "        sente = [e for e in sente if e.isalpha()]\n",
    "        catch.append(sente)\n",
    "        #print(catch)\n",
    "    \n",
    "    # Look for partial hits (representation_leads_part) in string \n",
    "    representation_leads_part = [['representation', 'for', 'the', 'appellant'], ['representation', 'for', 'the', 'claimant'],\n",
    "    ['for', 'the', 'appellant'], ['representation', 'for', 'the', 'appellants'], ['for', 'the', 'first', 'appellant']]\n",
    "    \n",
    "    # Representation has not been found yet (flag = 0)\n",
    "    flag = 0\n",
    "\n",
    "    for element in catch:\n",
    "        for part in representation_leads_part:\n",
    "            # find index of part hit\n",
    "            idx_part = representation_leads_part.index(part)\n",
    "            # Condition flag == 0 to avoid greedy behaviour (several matches) Only matters 1st hit\n",
    "            if sublist(representation_leads_part[idx_part], element) and flag == 0:\n",
    "                index = catch.index(element)\n",
    "                # representaion lead found in catch\n",
    "                flag = 1\n",
    "                # Keep only sentence with the hit (it includes all needed info)\n",
    "                new_catch = catch[index]\n",
    "                representation.append(new_catch)\n",
    "                decision.update({'Representation:': new_catch})\n",
    "                #print(new_catch)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    # If information on the representation has not been found (flag = 0)\n",
    "    if flag == 0:\n",
    "        #print(f'Did not find a nationality {file_name} in catch: {catch}')\n",
    "        representation.append(np.nan)\n",
    "        decision.update({'Representation:': np.nan})\n",
    "        #print('Did not find a representation')\n",
    "        #print(catch)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information on the legal representatives has been captured for a large number of decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File                 0\n",
       "Representation    3372\n",
       "dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_representation = {'File':files_legal,'Representation':representation}\n",
    "\n",
    "df_representation = pd.DataFrame(dict_representation, columns=['File','Representation'])\n",
    "df_representation.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field Representation: includes a string with the information on the legal representatives. The following breaks it down into two pieces:\n",
    "- The legal representation of the appellant (legalAppellant).\n",
    "- The legal representation of the defendant (legalDefendant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Split the legal representation into two parts: appellent and respondent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The decision of the judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision of the judge is the most challenging piece of information to extract from the documents. # First isolate the part of the document most likely to include the decission the second half of the document. Second, get rid of annexes and appendixes. third, # classifying judgments is not the same as classifying cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 20:28:12 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-11-13 20:28:12 INFO: Use device: cpu\n",
      "2021-11-13 20:28:12 INFO: Loading: tokenize\n",
      "2021-11-13 20:28:12 INFO: Done loading processors!\n",
      "100%|██████████| 35305/35305 [2:00:38<00:00,  4.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# nlp sentence tokenizer with Stanford\n",
    "nlp = stanza.Pipeline(lang = 'en', processors = 'tokenize', tokenize_no_ssplit = True)\n",
    "\n",
    "# Store decisions in a list to make a df\n",
    "decisions = []\n",
    "files_judge = []\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the full text of the court decision\n",
    "    string = decision.get('String')\n",
    "    file_name = decision.get('File')\n",
    "    files_judge.append(file_name)\n",
    "\n",
    "    # Use only second half of text (skip references to annxes and appendixes)\n",
    "    string = string[len(string)//2:]\n",
    "\n",
    "    # Discard text following appendix and annexes\n",
    "    string = string.rsplit(\"appendix\", 1)\n",
    "    string = string[0]\n",
    "    string = string.rsplit(\"annex\", 1)\n",
    "    string = string[0]\n",
    "\n",
    "    # Narrow down the search from the end\n",
    "    # Split on last occurrence of \"Signed\"\n",
    "    string = string.rsplit(\"Signed\", 1)\n",
    "    string = string[0].lower()\n",
    "\n",
    "\n",
    "    # Keep a max of 2000 characters\n",
    "    string = string[ min(-2000, len(string)):]\n",
    "\n",
    "    # Get rid of text after the last occurrence of 'anonymity'\n",
    "    string = string.rsplit(\"anonymity\", 1)\n",
    "    string = string[0]\n",
    "\n",
    "    # Apply stanford nlp\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # List to store the ruling sentences\n",
    "    catch = []\n",
    "    # Flag = 1 when decision found\n",
    "    flag = 0\n",
    "        \n",
    "    # Make sentences\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sente = [token.text for token in sentence.tokens]\n",
    "        # Keep only the alpha tokens\n",
    "        sente = [e for e in sente if e.isalpha()]\n",
    "        #print(type(sente))\n",
    "        catch.append(sente)\n",
    "        \n",
    "    # Identify decision leads in sentences\n",
    "    decision_leads = [['notice', 'of', 'decision'], ['decision'], ['decisions'], ['conclusions'], ['conclusion']]\n",
    "        \n",
    "    # When decision lead found, trim catch and update flag value \n",
    "    for lead in decision_leads:\n",
    "        try:\n",
    "            # Find index of decision lead in ruling\n",
    "            index = catch.index(lead)\n",
    "            # Remove sentences before the decision lead sentence\n",
    "            del catch[0:index]\n",
    "            # Flatten the list of lists/sentences\n",
    "            flat_catch = [item for sublist in catch for item in sublist]\n",
    "            # Decision found\n",
    "            flag = 1\n",
    "            # Store decision in decisions list\n",
    "            decisions.append(flat_catch)\n",
    "            decision.update({'Decision:': flat_catch})\n",
    "            #print('Found decision 1')\n",
    "            #print(flat_catch)\n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # If a decision has not been found yet (flag = 0)\n",
    "    if flag == 0:\n",
    "    # Look for partial hits in text \n",
    "        decision_leads_part = [['for', 'the', 'above', 'reasons'], ['for', 'the', 'reasons', 'i', 'have', 'given'], ['general', 'conclusions'],\n",
    "        ['for', 'the', 'reasons', 'set', 'out', 'above'], ['for', 'all', 'of', 'these', 'reasons'], ['decision', 'and', 'directions'], ['conclusions'],\n",
    "        ['notice', 'of', 'decision'], ['decision','the', 'application', 'for', 'judicial', 'review', 'is'], ['there', 'is', 'no', 'material', 'error', 'of', 'law', 'in'],\n",
    "        ['decision', 'the', 'decision', 'of', 'tribunal', 'judge', 'dean', 'promulgated'], ['the', 'decision', 'of', 'the', 'ftt', 'is', 'set', 'aside'],\n",
    "        ['i', 'grant', 'permission', 'to', 'appeal', 'i', 'set', 'aside', 'the', 'decision', 'of', 'the', 'tribunal'], ['i', 'set', 'aside', 'that', 'decision'],\n",
    "        ['the', 'appellant', 'appeal', 'as', 'originally', 'brought', 'to', 'the', 'ftt', 'is', 'dismissed'], ['i', 'do', 'not', 'set', 'aside', 'the', 'decision']]\n",
    "            \n",
    "        for element in catch:\n",
    "            for part in decision_leads_part:\n",
    "                idx_part = decision_leads_part.index(part)\n",
    "                if all_exist(decision_leads_part[idx_part], element):\n",
    "                    index = catch.index(element)\n",
    "                    # Decision found in catch\n",
    "                    flag = 1\n",
    "                    # Remove sentences before the decision lead sentence\n",
    "                    del catch[0:index]\n",
    "                    # Flatten the list of lists/sentences\n",
    "                    flat_catch = [item for sublist in catch for item in sublist]\n",
    "                    #print('Found decision 2')\n",
    "                    #print(flat_catch)\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "        # If a decision has still not been found (flag = 0)\n",
    "        if flag == 0:\n",
    "            decisions.append(np.nan)\n",
    "            decision.update({'Decision:': np.nan})\n",
    "            #print('Did not find a decision')\n",
    "            #print(catch)\n",
    "        else:\n",
    "            # Store decision in decisions list\n",
    "            decisions.append(flat_catch)\n",
    "            decision.update({'Decision:': flat_catch})\n",
    "            continue\n",
    "\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35305\n",
      "35305\n",
      "['decision', 'the', 'determination', 'of', 'the', 'tribunal', 'having', 'been', 'found', 'to', 'contain', 'a', 'material', 'error', 'of', 'law', 'i', 'substitute', 'the', 'following', 'decision', 'the', 'appellant', 'appeal', 'is', 'allowed', 'under', 'the', 'immigration', 'rules']\n",
      "HU166042017\n",
      "['the', 'judge', 'also', 'went', 'on', 'to', 'consider', 'whether', 'or', 'not', 'any', 'exceptional', 'circumstances', 'existed', 'in', 'this', 'particular', 'case', 'the', 'findings', 'and', 'conclusions', 'are', 'comprehensive', 'and', 'when', 'the', 'decision', 'is', 'viewed', 'holistically', 'the', 'judge', 'consideration', 'is', 'entirely', 'sound', 'in', 'light', 'of', 'the', 'above', 'the', 'appellant', 'appeal', 'to', 'the', 'upper', 'tribunal', 'is', 'dismissed', 'and', 'the', 'decision', 'of', 'the', 'tribunal', 'stands', 'anonymity', 'i', 'make', 'no']\n",
      "PA098412018\n",
      "['notice', 'of', 'decision', 'for', 'the', 'above', 'reasons', 'the', 'decision', 'i', 'on', 'the', 'appellant', 'appeal', 'is', 'to', 'allow', 'it', 'on', 'asylum', 'grounds', 'to', 'conclude', 'as', 'found', 'in', 'my', 'previous', 'error', 'of', 'law', 'decision', 'the', 'ftt', 'judge', 'materially', 'erred', 'in', 'law', 'the', 'decision', 'i', 'is', 'to', 'allow', 'the', 'appellant', 'appeal', 'direction', 'regarding', 'anonymity', 'rule', 'of', 'the', 'tribunal', 'procedure', 'upper', 'tribunal', 'rules', 'unless', 'and', 'until', 'a', 'tribunal', 'or', 'court', 'directs', 'otherwise', 'the', 'appellant', 'is', 'granted']\n",
      "File           0\n",
      "Decision    5220\n",
      "dtype: int64\n",
      "35305\n",
      "35305\n"
     ]
    }
   ],
   "source": [
    "dict_decisions = {'File':files_judge,'Decision':decisions}\n",
    "\n",
    "df = pd.DataFrame(dict_decisions, columns=['File','Decision'])\n",
    "df.isna().sum()\n",
    "#print(data[49])\n",
    "print(len(files_judge))\n",
    "print(len(decisions))\n",
    "#print(decisions[32488])\n",
    "#print(files[5000])\n",
    "#rint(decisions[5000])\n",
    "#print(files[6000])\n",
    "print(decisions[6002])\n",
    "#print(df[df['Decision'].isnull()])\n",
    "print(df.isnull().sum(axis = 0))\n",
    "\n",
    "#print(json.dumps(data[32554], indent = 4, sort_keys = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sense of the decision.\n",
    "\n",
    "The decision has been isolated. However, no information on whether the sentence accepts/rejects or is neutral. Sense of decision depends on the appellent. If appellent is home office, then... The decision of the First-tier Tribunal did not involve the  making  of an error of law and I uphold it\n",
    "is accepted, otherwise is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:01<00:00, 28700.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Store decisions in a list to make a df\n",
    "decision_text = []\n",
    "decision_label = []\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the full text of the court decision\n",
    "    string = decision.get('Decision:')\n",
    "    #file_name = decision.get('File')\n",
    "    if isinstance(string, float):\n",
    "        continue\n",
    "    else:\n",
    "        # If a label has not been found (flag = 0)\n",
    "        flag = 0\n",
    "        # The decisions are stored as a listt of tokens\n",
    "        string = ' '.join(x for x in string)\n",
    "        # print(string)\n",
    "        decision_text.append(string)\n",
    "        # Look for partial hits in text \n",
    "        decision_labels_reject = ['appeal dismissed', 'appeal is dismissed','this application is refused', 'the appeal is dismissed', 'the decision of the first-tier Tribunal stands',\n",
    "            'the decision of the tribunal did not involve the making of a material error of law', 'the original decision shall stand',\n",
    "            'did not involve the making of an error on a point of law', 'appeal remains dismissed', 'decision stands', 'not satisfied that the judge erred',\n",
    "            'not involve an error on', 'do not set aside the decision', 'this application is refused', 'i therefore uphold the decision of',\n",
    "            'does not contain a material error of law and shall stand', 'it shall stand', 'did not contain any error of law', 'did not involve the making of a material error of law',\n",
    "            'dismissing the claimant appeal', 'does not contain a material error of law','no material error of law has been established',\n",
    "            'the decision to dismiss the appellant appeal shall stand', 'the appeal is statutorily abandoned', 'tribunal decision is not vitiated by legal error',\n",
    "            'there is no material error of law in the tribunal judge decision', 'the appeal by the secretary of state is dismissed',\n",
    "            'decision of the tribunal does not disclose an error on a point of law', 'errors are not material such that the decision should be set aside',\n",
    "            'the appeal of the appellant is dismissed', 'reveals no error of law and stands', 'the appeal to the upper tribunal is dismissed',\n",
    "            'the respondent appeal be dismissed', 'the judge did not materially err in law', 'did not involve a making of or a material error of law',\n",
    "            'appeal to the upper tribunal is dismissed', 'i am dismissing', 'did not contain a material error of law', 'the decision of the ftt judge must stand',\n",
    "            'there are no errors of law', 'there is no material error of law', 'this appeal is dismissed', 'i dismiss the appeal', 'tribunal did not involve the making of an error of law',\n",
    "            'contains no material error of law', 'the decision of the tribunal stands', 'the decision of the tribunal did not involve the making of an error of law',\n",
    "            'this appeal is therefore dismissed', 'i refuse permission to appeal', 'the applicant appeal to the upper tribunal is therefore dismissed',\n",
    "            'decision of the ftt did not involve the making of an error of law', 'i do not set it aside', 'the decision of the tribunal did not involve the making of any error on a point of law',\n",
    "            'the appellant appeal to the upper tribunal is therefore dismissed', 'permission to appeal to the court of appeal is refused', 'the decision of the tribunal is upheld', 'the appeals are dismissed',\n",
    "            'did not make a material error of law', 'the decision of the tribunal shall stand', 'the appeal of the secretary of state is dismissed',\n",
    "            'the appeal of the secretary of state is dismissed', 'do not disclose any material error of law', 'the tribunal did not err in law',\n",
    "            'judge did not make an error of law', 'i uphold the decision to dismiss', 'no material error of law was made', 'did not make a material error on a point of law',\n",
    "            'there was no error of law made by the judge', 'we dismiss the secretary of state appeal', 'the appeals to the upper tribunal are dismissed',\n",
    "            'decision of the tribunal does not contain an error on a point', 'is disused', 'decision of the tribunal does not contain an error on a point of law',\n",
    "            'the appellants appeals are each dismissed', 'the decision by dismissing the appeal', 'i uphold the tribunal determination and dismiss the appeal'\n",
    "            'this appeal is dismissed', 'appeals dismissed for all the appellants', 'decision of the tribunal does not disclose an error in law', 'tribunal does not show a material error on a point of law',\n",
    "            'i uphold the tribunal determination and dismiss the appeal', 'these appeals are dismissed', 'determination does not disclose any material error of law',\n",
    "            'tribunal did not involve the making of a material error on a point of law', 'the decision of the tribunal containing no material error of law shall stand',\n",
    "            'decision of the tribunal does not disclose a material error on a point of law', 'there was no material error of law made by the judge',\n",
    "            'is free of legal error accordingly it must stand', 'decision of the tribunal does not contain errors of law that decision shall stand',\n",
    "            'the ftt judge did not err in law', 'i find no error of law', 'there is no error in law', 'no material error has been established',\n",
    "            'no material error of law has been demonstrated', 'there being no material error of law in the decision', 'the appeals of the appellants are dismissed',\n",
    "            'no material error of law is established in the decision', 'i dismissed the appellant appeal', 'i dismiss the appellant appeal', 'the decision of the tribunal contained a material error of law',\n",
    "            'i dismiss the appellant appeal', 'there is no error of law in the judge findings', 'i find no material error of law', 'the appeal of the claimant is dismissed',\n",
    "            'the decision of the original judge will stand', 'the appeal is to the upper tribunal is dismissed', 'did not involve the making of an error of law',\n",
    "            'the appeal is therefore dismissed', 'is dismissed', 'is refused', 'the appellants appeals are dismissed', 'i remake the decision on the appeal dismissing it',\n",
    "            'determination of the tribunal contains no error of law and it is upheld', 'the judge did not err in law', 'the appeal is as dismissed',\n",
    "            'the decision of the tribunal contains no error of law and shall stand', 'the claim is therefore dismissed', 'i find that there is no valid appeal before the tribunal',\n",
    "            'we have given we dismiss this appeal', 'this appeal must be dismissed', 'we conclude that no material error of law has been shown', 'there is no error of law',\n",
    "            'decision of the tribunal contains no error of law']\n",
    "\n",
    "        decision_labels_accept = ['i set it aside', 'did involve the making of an', 'decision of the tribunal is set aside', 'is set aside',\n",
    "            'i allow the claimant', 'appeal remains allowed', 'the appeal is allowed', 'appeal is granted', 'the appeal is allowed', 'the appellant is granted',\n",
    "            'i set aside the judge decision', 'i therefore set aside the decision', 'i set aside the decision', 'appeal allowed', 'appeal is allowed',\n",
    "            'set aside the decision', 'the decision of the first-tier tribunal has already been set aside', 'i allow the claimant', 'appeals allowed',\n",
    "            'i allow the appeal', 'is allowed', 'the first-tier tribunal erred in law', 'these appeals are allowed', 'the judge materially erred in law',\n",
    "            'does not contain an error of law', 'the appeals are allowed', 'the tribunal erred in law', 'the decision of the tribunal contained an error of law',\n",
    "            'involved the making of a material error of law', 'decision of the tribunal is tainted by material errors of law', 'i set the decision aside',\n",
    "            'tribunal involved an error on a point of law', 'did involve a material error of law', 'the appellants are granted', 'there is no material error of law in the determination',\n",
    "            'the judge erred in allowing this appeal', 'i remake the decision allowing the appellant appeal', 'did err in the making of', 'there are material errors of law',\n",
    "            'is therefore set aside', 'the appellant is granted', 'there is a material error of law', 'i allow the appellant eea appeal', 'should be set aside', 'by allowing the appeal',\n",
    "            'the tribunal decision involved the making of an error on a point of law', 'involved the making of an error on a point of law', 'by allowing the appellant appeal',\n",
    "            'involved the making of an error of law', 'the decision of the tribunal does not contain errors of law and it is upheld', 'decision of the tribunal has been set aside',\n",
    "            'the claimant appeal to the ftt is remade and allowed', 'tribunal was vitiated by legal error', 'discloses an error of law', 'we set aside that decision',\n",
    "            'the appeal is remitted to the tribunal for a hearing afresh', 'contains a material error of law', 'the tribunal made errors of law', 'the determination of the tribunal does not disclose a material error of law',\n",
    "            'the determination of the tribunal contained an error of law', 'the judge made an error on a point of law', 'the decision of the tribunal is hereby set aside for material error',\n",
    "            'the tribunal judge made errors of law', 'the appeal against the judge decision is therefore allowed', 'i allow the claim for asylum and on human rights grounds',\n",
    "            'the tribunal decision is vitiated by a material error of law', 'i find material error in law', 'the appellants appeals are allowed', 'the human rights appeals are allowed',\n",
    "            'i therefore allow the appeal', 'does not disclose an error of law and stands', 'determination does contain a material error of law', 'we allow the appeal',\n",
    "            'we have decided to allow this appeal', 'this appeal is accordingly allowed', 'both appeals are allowed', 'the appeals of the four appellants are allowed']\n",
    "\n",
    "\n",
    "        # Check first evidence of reject, if reject be done with it\n",
    "\n",
    "        for label in decision_labels_reject:\n",
    "            if string != None and label in string:\n",
    "                #print(\"Rejected!\")\n",
    "                flag = -1\n",
    "                decision_label.append('Rejected')\n",
    "                break\n",
    "            else:\n",
    "                'Not found!'\n",
    "        if flag == -1:\n",
    "            continue\n",
    "        else:\n",
    "            # Check evidence for Accept\n",
    "            for label in decision_labels_accept:\n",
    "                if string != None and label in string:\n",
    "                    #print(\"Accepted!\")\n",
    "                    flag = 1\n",
    "                    decision_label.append('Accepted')\n",
    "                    break\n",
    "                else:\n",
    "                    'Not found!'\n",
    "        if flag == 0:\n",
    "            decision_label.append('Neutral')\n",
    "\n",
    "    #print(flag)\n",
    "\n",
    "\n",
    "# The decision/ruling by the judge ('Decision:')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30085\n",
      "30085\n",
      "                                           Decision text Decision label\n",
      "0      notice of decision directions the decision of ...       Accepted\n",
      "1      in light of my conclusions on that point littl...       Rejected\n",
      "2      notice of decision the decision of the tribuna...       Rejected\n",
      "3      notice of decision the decision of the tribuna...       Accepted\n",
      "4      decision the decision of tribunal judge malcol...       Accepted\n",
      "...                                                  ...            ...\n",
      "30080  for the reasons we have given in paragraph we ...       Accepted\n",
      "30081  conclusions a northern cyprus is not capable o...       Accepted\n",
      "30082  for the reasons we have given we dismiss this ...       Rejected\n",
      "30083  our conclusions on the general issues relating...       Rejected\n",
      "30084  conclusions for each of the main reason a and ...       Accepted\n",
      "\n",
      "[30085 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision text</th>\n",
       "      <th>Decision label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30085</td>\n",
       "      <td>30085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>24093</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>notice of decision the appeal is dismissed no</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>231</td>\n",
       "      <td>13711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Decision text Decision label\n",
       "count                                           30085          30085\n",
       "unique                                          24093              3\n",
       "top     notice of decision the appeal is dismissed no       Accepted\n",
       "freq                                              231          13711"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(decision_text))\n",
    "print(len(decision_label))\n",
    "\n",
    "dict_sense_decisions = {'Decision text':decision_text,'Decision label':decision_label}\n",
    "\n",
    "df_decision_sense = pd.DataFrame(dict_sense_decisions, columns=['Decision text','Decision label'])\n",
    "df_decision_sense.isna().sum()\n",
    "df_decision_sense.sum()\n",
    "\n",
    "print(df_decision_sense)\n",
    "\n",
    "df_decision_sense.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decision label\n",
       "Accepted          13711\n",
       "Rejected          12778\n",
       "Neutral            3596\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_decision_sense[['Decision label']].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_df = df_decision_sense[df_decision_sense['Decision label'] == 'Neutral']\n",
    "rslt_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in decision_labels_accept:\n",
    "    idx_part = decision_labels_accept.index(label)\n",
    "    if all_exist(decision_labels_accept[idx_part], example):\n",
    "        flag = 1\n",
    "        break\n",
    "        #print('Accept')\n",
    "    else:\n",
    "        flag = 0\n",
    "        #print('Reject')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Nationality of the appellant. \n",
    "The field country is empty to a large extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of countries and nationalities to be checked against the text\n",
    "countries = ['Afghanistan', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua', \n",
    "'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus',\n",
    "'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia', 'Bolivia, Plurinational State of', 'Bonaire', 'Bonaire, Sint Eustatius and Saba', \n",
    "'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', \n",
    "'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', \n",
    "'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Congo', 'Cook Islands', 'Costa Rica', \"Côte d'Ivoire\", \n",
    "'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', \n",
    "'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', \n",
    "'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', \n",
    "'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', \n",
    "'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', 'Iraq', \n",
    "'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', \n",
    "\"Korea, Democratic People's Republic of\", 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', \n",
    "'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', \n",
    "'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia', \n",
    "'Federated States of', 'Micronesia', 'Moldova, Republic of', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', \n",
    "'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', \n",
    "'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Palestine', 'Panama', 'Papua New Guinea', \n",
    "'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russian Federation', 'Russia', \n",
    "'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', \n",
    "'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', \n",
    "'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', \n",
    "'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', \n",
    "'Sweden', 'Switzerland', 'Syria', 'Syrian Arab Republic', 'Taiwan', 'Taiwan, Province of China', 'Tajikistan', 'Tanzania', \n",
    "'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', \n",
    "'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United States', 'United States Minor Outlying Islands', 'Uruguay', \n",
    "'Uzbekistan', 'Vanuatu', 'Venezuela', 'Venezuela, Bolivarian Republic of', 'Vietnam', 'Viet Nam', 'Virgin Islands, British', 'Virgin Islands, U.S.', \n",
    "'Wallis and Futuna', 'Yemen', 'Zambia', 'Zimbabwe', 'Pakistani', 'Iranian', 'Bangladeshi', 'Indian', 'Egyptian', 'Afghan', 'Albanian', 'Algerian', \n",
    "'American', 'Andorran', 'Angolan', 'Antiguans', 'Argentinean', 'Armenian', 'Australian', 'Austrian', 'Azerbaijani', 'Bahamian', 'Bahraini', 'Bangladeshi', 'Barbadian', 'Barbudans', 'Batswana', 'Belarusian', 'Belgian', 'Belizean', 'Beninese', 'Bhutanese', 'Bolivian', 'Bosnian', 'Brazilian', 'Bruneian', 'Bulgarian', 'Burkinabe', 'Burmese', 'Burundian', 'Cambodian', 'Cameroonian', 'Canadian', 'Cape Verdean', 'Central African', 'Chadian', 'Chilean', 'Chinese', 'Colombian', 'Comoran', 'Congolese', 'Costa Rican', 'Croatian', 'Cuban', 'Cypriot', 'Czech', 'Danish', 'Djibouti', 'Dominican', 'Dutch', 'Dutchman', 'Dutchwoman', 'East Timorese', 'Ecuadorean', 'Egyptian', 'Emirian', 'Equatorial Guinean', 'Eritrean', 'Estonian', 'Ethiopian', 'Fijian', 'Filipino', 'Finnish', 'French', 'Gabonese', 'Gambian', 'Georgian', 'German', 'Ghanaian', 'Greek', 'Grenadian', 'Guatemalan', 'Guinea-Bissauan', 'Guinean', 'Guyanese', 'Haitian', 'Herzegovinian', 'Honduran', 'Hungarian', 'I-Kiribati', 'Icelander', 'Indian', 'Indonesian', 'Iranian', 'Iraqi', 'Irish', 'Israeli', 'Italian', 'Ivorian', 'Jamaican', 'Japanese', 'Jordanian', 'Kazakhstani', 'Kenyan', 'Kittian and Nevisian', 'Kuwaiti', 'Kyrgyz', 'Laotian', 'Latvian', 'Lebanese', 'Liberian', 'Libyan', 'Liechtensteiner', 'Lithuanian', 'Luxembourger', 'Macedonian', 'Malagasy', 'Malawian', 'Malaysian', 'Maldivan', 'Malian', 'Maltese', 'Marshallese', 'Mauritanian', 'Mauritian', 'Mexican', 'Micronesian', 'Moldovan', 'Monacan', 'Mongolian', 'Moroccan', 'Mosotho', 'Motswana', 'Mozambican', 'Namibian', 'Nauruan', 'Nepalese', 'Netherlander', 'New Zealander', 'Ni-Vanuatu', 'Nicaraguan', 'Nigerian', 'Nigerien', 'North Korean', 'Northern Irish', 'Norwegian', 'Omani', 'Pakistani', 'Palauan', 'Panamanian', 'Papua New Guinean', 'Paraguayan', 'Peruvian', 'Polish', 'Portuguese', 'Qatari', 'Romanian', 'Russian', 'Rwandan', 'Saint Lucian', 'Salvadoran', 'Samoan', 'San Marinese', 'Sao Tomean', 'Saudi', 'Scottish', 'Senegalese', 'Serbian', 'Seychellois', 'Sierra Leonean', 'Singaporean', 'Slovakian', 'Slovenian', 'Solomon Islander', 'Somali', 'South African', 'South Korean', 'Spanish', 'Sri Lankan', 'Sudanese', 'Surinamer', 'Swazi', 'Swedish', 'Swiss', 'Syrian', 'Taiwanese', 'Tajik', 'Tanzanian', 'Thai', 'Togolese', 'Tongan', 'Trinidadian or Tobagonian', 'Tunisian', 'Turkish', 'Tuvaluan', 'Ugandan', 'Ukrainian', 'Uruguayan', 'Uzbekistani', 'Venezuelan', 'Vietnamese', 'Welsh', 'Yemenite', 'Zambian', 'Zimbabwean']\n",
    "\n",
    "countriesLower = [x.lower() for x in countries]\n",
    "#print(countriesLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 18:00:58 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-11-14 18:00:58 INFO: Use device: cpu\n",
      "2021-11-14 18:00:58 INFO: Loading: tokenize\n",
      "2021-11-14 18:00:58 INFO: Done loading processors!\n",
      "100%|██████████| 35305/35305 [5:04:39<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Store decisions in a list to make a df\n",
    "nationalities = []\n",
    "files = []\n",
    "\n",
    "# nlp sentence tokenizer with Stanford\n",
    "nlp = stanza.Pipeline(lang = 'en', processors = 'tokenize', tokenize_no_ssplit = True)\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the full text of the court decision\n",
    "    string = decision.get('String')\n",
    "    file_name = decision.get('File')\n",
    "    files.append(file_name)\n",
    "\n",
    "    # Use only first third of text\n",
    "    string = string[:len(string)//3]\n",
    "    # All text in lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Apply stanford nlp\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # List to store the ruling sentences\n",
    "    catch = []\n",
    "\n",
    "    # Make sentences\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sente = [token.text for token in sentence.tokens]\n",
    "        # Keep only the alpha tokens\n",
    "        sente = [e for e in sente if e.isalpha()]\n",
    "        #print(type(sente))\n",
    "        catch.append(sente)\n",
    "    # Look for partial hits in text\n",
    "    nationality_leads_part = [['the', 'appellant', 'is', 'a', 'national', 'of'], ['the', 'appellant', 'is', 'a', 'citizen', 'of'],\n",
    "    ['the', 'respondent', 'is', 'a', 'citizen', 'of'], ['the', 'appellants', 'are', 'all', 'citizens', 'of'], ['citizen', 'of'],\n",
    "    ['national', 'of'], ['citizens', 'of']]\n",
    "    # Nationality not yet found (flag = 0)\n",
    "    flag = 0\n",
    "    for element in catch:\n",
    "        for part in nationality_leads_part:\n",
    "            idx_part = nationality_leads_part.index(part)\n",
    "            if sublist(nationality_leads_part[idx_part], element):\n",
    "                #print(nationality_leads_part[idx_part])\n",
    "                index = catch.index(element)\n",
    "                # Nationality lead found in catch\n",
    "                # Remove sentences before sentence with decision lead\n",
    "                new_catch = catch[index]\n",
    "                # flag2 = 1 when nationality is found in 1\n",
    "                for token in new_catch:\n",
    "                    #indx_country = countriesLower.index(country)\n",
    "                    if sublist([token], countriesLower):\n",
    "                        # Nationality found (flag = 1)\n",
    "                        flag = 1\n",
    "                        nationalities.append(token)\n",
    "                        decision.update({'Nationality:': token})\n",
    "                        #print(f'FOUND A NATIONALITY {token} in {file_name}')\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                    break\n",
    "            if flag == 1:\n",
    "                break\n",
    "        if flag == 1:\n",
    "            break\n",
    "    # If a decision has still not been found (flag = 0)\n",
    "    if flag == 0:\n",
    "        #print(f'Did not find a nationality {file_name} in catch: {catch}')\n",
    "        nationalities.append(np.nan)\n",
    "        decision.update({'Nationality:': np.nan})\n",
    "        #print(f'Did not find a nationality in {catch}')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Assign country name to nationalities found\n",
    "\n",
    "# Create dictionary with nationality key and country as value\n",
    "\n",
    "# Iterate over all nationality fields in the data collection of decisions\n",
    "# Look up country, if country, leave as it is, else look up in dict and replace. Update data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35305\n",
      "35305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "File               0\n",
       "Nationality    12537\n",
       "dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(files))\n",
    "print(len(nationalities))\n",
    "#print(nationalities)\n",
    "dict_nationalities = {'File':files,'Nationality':nationalities}\n",
    "df = pd.DataFrame(dict_nationalities, columns=['File','Nationality'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with empty/corrupt files that didn't upload a sentence string\n",
    "# Regex expression: What comes in between 'Before' and 'Between'\n",
    "# regex = '(?<=Before)([\\s\\S]*?)(?=Between)'\n",
    "regex = 'representation:([\\S\\s]*)for the respondent'\n",
    "catch = re.search(regex, decision_string.lower())\n",
    "#If the catch is successful\n",
    "if catch :\n",
    "    string = catch.group(0)\n",
    "    delimiters = ['|', '?', ':']\n",
    "    # Get rid of some table delimiters\n",
    "    for i in delimiters:\n",
    "        string = string.replace(i,'')\n",
    "    # Remove leading and trailing spaces\n",
    "    string = string.strip()\n",
    "    print(string)\n",
    "\n",
    "# Path to the txt documents\n",
    "txt_path = './data/processed/txt_files_test/'\n",
    "print(os.listdir(txt_path))\n",
    "# Loop over each text file and extract Court information\n",
    "for text in os.listdir(txt_path):\n",
    "    print(text)\n",
    "\n",
    "    with open(txt_path + text, 'r') as file:\n",
    "        decision_string = file.read()\n",
    "        # Regex expression: What comes after \"Heard at\" until hitting 3 balnks or new line\n",
    "        #regex = '(?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        #regex = 'Before([\\S\\s]*)Between'\n",
    "        regex = '(?<=Before)([\\s\\S]*?)(?=Between)'\n",
    "\n",
    "        catch = re.search(regex, decision_string)\n",
    "        #If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "\n",
    "            # Keep only alpha numeric\n",
    "            string = string.replace('|','')\n",
    "            #string = re.sub(r'[^A-Za-z0-9 ]+', '', string)\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            print(string)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "    # Loading string with court decision to data\n",
    "for txt_file in  tqdm(os.listdir(txt_path)):\n",
    "    \n",
    "    # Open file and obtain string and file_name\n",
    "    with open(txt_path + txt_file, 'r') as file:\n",
    "        string = file.read()\n",
    "        f_name, f_ext = os.path.splitext(file.name)\n",
    "        head, file_name = os.path.split(f_name)\n",
    "    # Search data list of dictionaries for dict where {\"File\":} = file_name\n",
    "    for d in data:\n",
    "        if d.get('File') == file_name:\n",
    "            # Add dictionary key 'String' with value string\n",
    "            d.update({'String': string})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for it in first half of string\n",
    "# GPE Countries, cities, states.\n",
    "# LOC Non-GPE locations, mountain ranges, bodies of water.\n",
    "#\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "# loop over every row in the 'Bio' column\n",
    "for text in df['Bio'].tolist():\n",
    "    # use spacy to extract the entities\n",
    "    doc = sp(text)\n",
    "    for ent in doc.ents:    \n",
    "        # check if entity is equal 'LOC' or 'GPE'\n",
    "        if ent.label_ in ['GPE']:\n",
    "            print(ent.text, ent.label_)  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da6e46dab1e0b3d9fa32aec1170dd2df7038a4f7be3a54c97a348d8ad782954"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction (20th November 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts additional information from the text of the tribunal decisions and stores it in the relevant dictionary.\n",
    "\n",
    "In particular, the notebook performs information extraction on:\n",
    "\n",
    "1. The label included in the name of the file ('Code label:').\n",
    "\n",
    "2. The court where the case was heard ('Heard at').\n",
    "\n",
    "3. The judges ('Judges:').\n",
    "\n",
    "4. The legal representation ('Representation:') for the appellant ('Representation appellant:') and the respondent ('Representation respondent:').\n",
    "\n",
    "5. The decision/ruling by the judge ('Decision:').\n",
    "\n",
    "6. The sense of the decision/ruling ('Decision label:').\n",
    "\n",
    "7. The nationality of the the subject of the case (appellant or respondent).\n",
    "\n",
    "Each of these fields is added to the dictionary of each judicial decision.\n",
    "\n",
    "The resulting data set - a list of updated dictionaries -  is serialised as a json object (jsonDataFinal.json).\n",
    "\n",
    "This notebook should run in the tfm environment, which can be created with the environment.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment: /Users/albertamurgopacheco/anaconda3/envs/tfm/bin/python\n",
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, getsize\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import whois\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import textract\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import stanza\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# What environment am I using?\n",
    "print(f'Current environment: {sys.executable}')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/albertamurgopacheco/Documents/GitHub/TFM')\n",
    "# What's my working directory?\n",
    "print(f'Current working directory: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directories in colab and local execution\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    docs_path = '/content/gdrive/MyDrive/TFM/data/raw'\n",
    "    input_path = '/content/gdrive/MyDrive/TFM'\n",
    "    output_path = '/content/gdrive/MyDrive/TFM/output'\n",
    "\n",
    "else:\n",
    "    docs_path = './data/raw'\n",
    "    input_path = '.'\n",
    "    output_path = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFORMATION EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Auxiliary functions and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture whether all elements exist in a list\n",
    "def sublist(sublist, lst):\n",
    "    \"\"\"\n",
    "    Given a list of sentences/lists all_exist checks whether sublist list exists in lst\n",
    "\n",
    "    :sublist: list to search\n",
    "    :lst: list to be searched\n",
    "    :return: the list with the match\n",
    "    \"\"\"\n",
    "    if not isinstance(sublist, list):\n",
    "        raise ValueError(\"sublist must be a list\")\n",
    "    if not isinstance(lst, list):\n",
    "        raise ValueError(\"lst must be a list\")\n",
    "\n",
    "    sublist_len = len(sublist)\n",
    "    k=0\n",
    "    s=None\n",
    "\n",
    "    if (sublist_len > len(lst)):\n",
    "        return False\n",
    "    elif (sublist_len == 0):\n",
    "        return True\n",
    "\n",
    "    for x in lst:\n",
    "        if x == sublist[k]:\n",
    "            if (k == 0): s = x\n",
    "            elif (x != s): s = None\n",
    "            k += 1\n",
    "            if k == sublist_len:\n",
    "                return True\n",
    "        elif k > 0 and sublist[k-1] != s:\n",
    "            k = 0\n",
    "\n",
    "    return False\n",
    "\n",
    "# Function to capture if all elements exist in a list\n",
    "def all_exist(avalue, bvalue):\n",
    "    \"\"\"\n",
    "    Given a list of sentences/lists all_exist checks whether avalue list exists in bvalue\n",
    "\n",
    "    :avalue: list to search\n",
    "    :bvalue: list to be searched\n",
    "    :return: the list with the match\n",
    "    \"\"\"\n",
    "    return all(any(x in y for y in bvalue) for x in avalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The label included in the name of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two categories of cases: the reported and the unreported ones. The reported cases include richer data while the unreported ones (the vast majority of cases) miss several data fields due to a request for annonimity from any of the parties involved in the legal dispute.\n",
    "\n",
    "The first two letters in the file name seem to follow some logic. Inspecting the documents reveals the following meanings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 1588874.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each decision and extract first two characters of the file's name\n",
    "for decision in tqdm(data):\n",
    "    # Only 'unteported' decisions include this 2-letter code\n",
    "    if decision.get('Status of case:') == 'Unreported':\n",
    "        string_code = decision.get('File')[:2]\n",
    "    else:\n",
    "        string_code = 'NA'\n",
    "    \n",
    "    # Add dictionary key 'Code label' with value string to the dictionary\n",
    "    decision.update({'Code label:': string_code})\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The court where the case was heard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inspection of a sample of judicial decisions reveals that the name of the court is located in the first part of the document and it usually follows the expression \"Heard at\".\n",
    "\n",
    "The strategy to capture this field will consist of a search using regular expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:01<00:00, 27089.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    decision_string = decision.get('String')\n",
    "    # Deal with empty/corrupt files that didn't upload a sentence string\n",
    "    if decision_string:\n",
    "        # Regex expression: What comes after \"Heard at\" until hitting 3 balnks or new line\n",
    "        #regex = '(?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        regex = 'Heard at(.*)[\\S\\r\\n]| (?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        catch = re.search(regex, decision_string)\n",
    "\n",
    "        # If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "            # Remove ':' if included in the catch\n",
    "            string = string.replace(':','')\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            # Avoids picking up parts of tables and '|'\n",
    "            string = string.split('   ')\n",
    "            string = string[0]\n",
    "            # Remove 'Heard at' if included in the catch\n",
    "            string = string.replace('Heard at ','')\n",
    "            # Remove 'manually' some strings often included in the catch\n",
    "            string = string.replace('|Decision & Reasons Promulgated','')\n",
    "            string = string.replace('|Decision and Reasons Promulgated','')\n",
    "            string = string.replace('| Decision & Reasons Promulgated','')\n",
    "            string = string.replace('Decision Promulgated','')\n",
    "            string = string.replace('|Decision & Reasons promulgated','')\n",
    "            string = string.replace('|Determination Promulgated','')\n",
    "            string = string.replace('Decision and Reasons Promulgated','')\n",
    "            string = string.replace('|Decision & Reasons  Promulgated','')\n",
    "            string = string.replace(' on 4 July 2003','')\n",
    "            string = string.replace('Determination Promulgated','')\n",
    "            string = string.replace('Decision & Reasons Promulgated','')\n",
    "            string = string.replace('|Decisions and Reasons Promulgated','')\n",
    "            string = string.replace('|Decision and Reasons','')\n",
    "            string = string.replace('UT(IAC)','')\n",
    "            string = string.replace('UT (IAC) ','')\n",
    "            string = string.replace('Date of Hearing  9 December 2005','')\n",
    "            string = string.replace(' | |SS (Risk-Manastry) Iran CG [2003] UKIAT 00035 |','')\n",
    "            # Strip of often found trailing characters\n",
    "            string = string.rstrip(',')\n",
    "            string = string.rstrip('|')\n",
    "            # Remove leading and trailing spaces (again)\n",
    "            string = string.strip()\n",
    "        else:\n",
    "            string = 'NA'\n",
    "        #print(string)\n",
    "        # Add dictionary key 'Heard at' with value string to the dictionary\n",
    "        decision.update({'Heard at:': string})\n",
    "    else:\n",
    "        continue\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The judges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:01<00:00, 24965.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    decision_string = decision.get('String')\n",
    "    # Deal with empty/corrupt files that didn't upload a sentence string\n",
    "    if decision_string:\n",
    "        # Regex expression: What comes in between 'Before' and 'Between'\n",
    "        regex = '(?<=Before)([\\s\\S]*?)(?=Between)'\n",
    "        catch = re.search(regex, decision_string)\n",
    "        #If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "\n",
    "            # Get rid of some table delimiters\n",
    "            string = string.replace('|','')\n",
    "            string = string.replace('?','')\n",
    "            string = string.replace(',','')\n",
    "\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            \n",
    "            # Split strings (spaces > 3 usually indicates two \"joint\" names)\n",
    "            # Alternative approach tried and discarded base on sentence tokenization \n",
    "            # from nltk.tokenize import sent_tokenize\n",
    "            listNames = string.split(\"   \")\n",
    "            # Make list of names with strings containijng names\n",
    "            # Capitalize the first letter of each word & delete \n",
    "            listNames = [name.strip().title() for name in listNames if name.strip()]\n",
    "\n",
    "            # Discard content in brackets as it's mostly titles and clutter\n",
    "            listNames = [re.sub('[\\(\\[].*?[\\)\\]]', '', x).strip() for x in listNames]\n",
    "\n",
    "            # Finally, delete titles, positions held and other clutter around the name\n",
    "            clutter = ['Judge', 'Tribunal', 'Court', 'Upper', 'Deputy', 'Senior', 'Of', 'The', 'Mr', 'Dr', 'Vice', 'President',\n",
    "            ':', 'Honourable', 'Hon.', '', '- - - - - - - - - - - - - - - - - - - -', 'Ut', 'Trinbunal', '-And-', 'Mrs', 'President,',\n",
    "            'Tribnunal', '-', 'Hon', 'And', 'Chairman', 'Vice-President', 'Immigration', 'Asylum Chamber', '-Vice', '(Senior',\n",
    "            '...............', 'Designated', 'His Honour', 'Respondent Representation: For Appellant', 'Secretary State For Home Department',\n",
    "            'Appellant', 'Lord', 'Sir', 'In Matter An Application For Judicial Review', 'I) Eu Regulation Number 604/2013 Human',\n",
    "            'Miss', 'Ms.', ':-']\n",
    "\n",
    "            # \n",
    "            listNames = [' '.join(filter(lambda x: x not in clutter,  name.split())) for name in listNames]\n",
    "            # Remove remaining 'issues' with empty strings ''\n",
    "            listNames = list(filter(None, listNames))\n",
    "            # Add a . following individual letters\n",
    "\n",
    "            #print(listNames)\n",
    "            \n",
    "        else:\n",
    "            listNames = ['NA']\n",
    "        \n",
    "        #print(decision.get('File'))\n",
    "        #print(listNames)\n",
    "        # Add dictionary key 'Judges:' with value list of strings to the dictionary\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 2771630.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# 'Manually' fix some mistakes with some judges\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "\n",
    "    if decision.get('File') == '00046_ukut_iac_2020_ps_iran_cg':\n",
    "        listNames = ['J Barnes', 'A R Mackey', 'S L Batiste']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00393_ukut_iac_2019__jw_ors_ijr':\n",
    "        listNames = ['Rimington Jackson']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '2004_ukiat_00248_gh_iraq_cg':\n",
    "        listNames = ['Rintoul', 'Bruce']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00270_ukut_iac_2015_mmw_ijr':\n",
    "        listNames = ['Justice Mccloskey']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00271_ukut_iac_2015_bh_ijr':\n",
    "        listNames = ['Justice Mccloskey', \"O'Connor\"]\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == 'AA082212015':\n",
    "        listNames = ['Alis', 'I K']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The legal representation for the appellant and the respondent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The legal team consists of the representation for the appellant and the respondent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [5:05:36<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "representation = []\n",
    "files_legal = []\n",
    "\n",
    "# nlp sentence tokenizer with Stanford\n",
    "nlp = stanza.Pipeline(lang = 'en', processors = 'tokenize', tokenize_no_ssplit = True)\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    decision_string = decision.get('String')\n",
    "    file_name = decision.get('File')\n",
    "    files_legal.append(file_name)\n",
    "    #print(file_name)\n",
    "    # Use only first third of text\n",
    "    string = decision_string[:len(decision_string)//3]\n",
    "    # All text in lower\n",
    "    string = string.lower()\n",
    "    # Apply stanford nlp to string\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # List to store the ruling sentences\n",
    "    catch = []\n",
    "\n",
    "    # Make sentences\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sente = [token.text for token in sentence.tokens]\n",
    "        # Keep only the alpha tokens\n",
    "        sente = [e for e in sente if e.isalpha()]\n",
    "        catch.append(sente)\n",
    "        #print(catch)\n",
    "    \n",
    "    # Look for partial hits (representation_leads_part) in string \n",
    "    representation_leads_part = [['representation', 'for', 'the', 'appellant'], ['representation', 'for', 'the', 'claimant'],\n",
    "    ['for', 'the', 'appellant'], ['representation', 'for', 'the', 'appellants'], ['for', 'the', 'first', 'appellant']]\n",
    "    \n",
    "    # Representation has not been found yet (flag = 0)\n",
    "    flag = 0\n",
    "\n",
    "    for element in catch:\n",
    "        for part in representation_leads_part:\n",
    "            # find index of part hit\n",
    "            idx_part = representation_leads_part.index(part)\n",
    "            # Condition flag == 0 to avoid greedy behaviour (several matches) Only matters 1st hit\n",
    "            if sublist(representation_leads_part[idx_part], element) and flag == 0:\n",
    "                index = catch.index(element)\n",
    "                # representaion lead found in catch\n",
    "                flag = 1\n",
    "                # Keep only sentence with the hit (it includes all needed info)\n",
    "                new_catch = catch[index]\n",
    "                representation.append(new_catch)\n",
    "                decision.update({'Representation:': new_catch})\n",
    "                #print(new_catch)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    # If information on the representation has not been found (flag = 0)\n",
    "    if flag == 0:\n",
    "        #print(f'Did not find a nationality {file_name} in catch: {catch}')\n",
    "        representation.append(np.nan)\n",
    "        decision.update({'Representation:': np.nan})\n",
    "        #print('Did not find a representation')\n",
    "        #print(catch)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information on the legal representatives has been captured for a large number of decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File                 0\n",
       "Representation    3372\n",
       "dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_representation = {'File':files_legal,'Representation':representation}\n",
    "\n",
    "df_representation = pd.DataFrame(dict_representation, columns=['File','Representation'])\n",
    "df_representation.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field Representation: includes a string with the information on the legal representatives. The following breaks it down into two pieces:\n",
    "- The legal representation of the appellant (legalAppellant).\n",
    "- The legal representation of the defendant (legalDefendant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 101300.39it/s]\n"
     ]
    }
   ],
   "source": [
    "files_appellant = []\n",
    "appellants = []\n",
    "respondents = []\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    representation_string = decision.get('Representation:')\n",
    "    file_name = decision.get('File')\n",
    "\n",
    "    #file_name = decision.get('File')\n",
    "    if isinstance(representation_string, float):\n",
    "        continue\n",
    "    else:\n",
    "        # If a label has not been found (flag = 0)\n",
    "        flag = 0\n",
    "        # The decisions are stored as a listt of tokens\n",
    "        string = ' '.join(x for x in representation_string)\n",
    "        #print(string)\n",
    "        files_appellant.append(string)\n",
    "        \n",
    "        # Catch what's between 'for the appellant/s and for the correspondent' \n",
    "        regex_appellant = '(?<=for the appellant)([\\s\\S]*?)(?=for the respondent)'\n",
    "        catch_appellant = re.search(regex_appellant, string)\n",
    "        if catch_appellant :\n",
    "            string_appellant = catch_appellant.group(0)\n",
    "            # Remove leading and trailing spaces\n",
    "            string_appellant = string_appellant.strip()\n",
    "            \n",
    "            if string_appellant.startswith('s'):\n",
    "                string_appellant = string_appellant[1:].strip()\n",
    "        \n",
    "        # Catch what's between 'for the claimant/s and for the correspondent' \n",
    "        regex_claimant = '(?<=for the claimant)([\\s\\S]*?)(?=for the respondent)'\n",
    "        catch_claimant = re.search(regex_claimant, string)\n",
    "        if catch_claimant :\n",
    "            string_appellant = catch_claimant.group(0)\n",
    "            # Remove leading and trailing spaces\n",
    "            string_appellant = string_appellant.strip()\n",
    "            \n",
    "            if string_appellant.startswith('s'):\n",
    "                string_appellant = string_appellant[1:].strip()\n",
    "                print(string_appellant)\n",
    "                \n",
    "        #print(string_appellant)\n",
    "        appellants.append(string_appellant)\n",
    "        decision.update({'Appellant:': string_appellant})\n",
    "\n",
    "        # Catch what's after 'for the correspondent' \n",
    "        regex_respondent = '(?<=for the respondent)([\\s\\S]*)'\n",
    "        catch_respondent = re.search(regex_respondent, string)\n",
    "        if catch_respondent :\n",
    "            string_respondent = catch_respondent.group(0)\n",
    "            # Remove leading and trailing spaces\n",
    "            string_respondent = string_respondent.strip()\n",
    "                \n",
    "            if string_respondent.startswith('s'):\n",
    "                string_respondent = string_respondent[1:].strip()\n",
    "\n",
    "        #print(string_respondent)\n",
    "        respondents.append(string_respondent)\n",
    "        decision.update({'Respondent:': string_respondent})\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31933\n",
      "31933\n",
      "31933\n",
      "                                          Representation  \\\n",
      "0      representation for the appellant mr j gajjar c...   \n",
      "1      representation for the appellant ms sardar cou...   \n",
      "2      representation for the appellant mr hussain fo...   \n",
      "3      representation for the appellant mr t melvin s...   \n",
      "4      for the appellant ms e rutherford instructed b...   \n",
      "...                                                  ...   \n",
      "31928  on august he lodged an appeal against the refu...   \n",
      "31929  the adjudicator rejected all the appellant cla...   \n",
      "31930  the appellant describes himself variously as a...   \n",
      "31931  ms s panagiotopoulou of counsel instructed by ...   \n",
      "31932  we are however concerned with the relevance of...   \n",
      "\n",
      "                                               Appellant  \\\n",
      "0      mr j gajjar counsel instructed by m a consulta...   \n",
      "1        ms sardar counsel instructed by duncan lewis co   \n",
      "2                                             mr hussain   \n",
      "3      mr t melvin senior home office presenting officer   \n",
      "4      ms e rutherford instructed by bond adam llp so...   \n",
      "...                                                  ...   \n",
      "31928            mrs g oliso of the refugee legal centre   \n",
      "31929            mrs g oliso of the refugee legal centre   \n",
      "31930            mrs g oliso of the refugee legal centre   \n",
      "31931            mrs g oliso of the refugee legal centre   \n",
      "31932            mrs g oliso of the refugee legal centre   \n",
      "\n",
      "                                              Respondent  \n",
      "0              mr e tufan home office presenting officer  \n",
      "1      mr whitwell senior home office presenting officer  \n",
      "2                   mr diwnycz senior presenting officer  \n",
      "3      mr a maqsood counsel instructed by iconsult im...  \n",
      "4            mr t lindsay home office presenting officer  \n",
      "...                                                  ...  \n",
      "31928  mr m blundell home office presenting officer t...  \n",
      "31929  mr m blundell home office presenting officer t...  \n",
      "31930  mr m blundell home office presenting officer t...  \n",
      "31931  mr m blundell home office presenting officer t...  \n",
      "31932  mr m blundell home office presenting officer t...  \n",
      "\n",
      "[31933 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Representation</th>\n",
       "      <th>Appellant</th>\n",
       "      <th>Respondent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31933</td>\n",
       "      <td>31933</td>\n",
       "      <td>31933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>30550</td>\n",
       "      <td>17626</td>\n",
       "      <td>8486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>for the appellant in person</td>\n",
       "      <td>no appearance</td>\n",
       "      <td>mr p duffy senior home office presenting officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>35</td>\n",
       "      <td>423</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Representation      Appellant  \\\n",
       "count                         31933          31933   \n",
       "unique                        30550          17626   \n",
       "top     for the appellant in person  no appearance   \n",
       "freq                             35            423   \n",
       "\n",
       "                                              Respondent  \n",
       "count                                              31933  \n",
       "unique                                              8486  \n",
       "top     mr p duffy senior home office presenting officer  \n",
       "freq                                                 550  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(files_appellant))\n",
    "print(len(appellants))\n",
    "print(len(respondents))\n",
    "\n",
    "dict_sense_representation = {'Representation':files_appellant,'Appellant':appellants, 'Respondent':respondents}\n",
    "\n",
    "df_sense_representation = pd.DataFrame(dict_sense_representation, columns=['Representation','Appellant', 'Respondent'])\n",
    "df_sense_representation.isna().sum()\n",
    "df_sense_representation.sum()\n",
    "\n",
    "print(df_sense_representation)\n",
    "\n",
    "df_sense_representation.describe()\n",
    "#df_sense_representation.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 116561.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the legal firm or entitity involved as an appellant or respondent\n",
    "\n",
    "appellants = []\n",
    "appellants_firm = []\n",
    "respondents = []\n",
    "respondents_firm = []\n",
    "\n",
    "home_office_tags = ['senior presenting officer', 'senior home office presenting officer', 'home office presenting officer', 'home office']\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    appellant_string = decision.get('Appellant:')\n",
    "    respondent_string = decision.get('Respondent:')\n",
    "    representation_string = decision.get('Representation:')\n",
    "\n",
    "    if isinstance(representation_string, float):\n",
    "        continue\n",
    "    else:\n",
    "        # First, proceed with appellant's related firm info\n",
    "        flag_appellant = 0\n",
    "        \n",
    "        # Catch what's after ' of ' \n",
    "        regex_appellant = '(?<= of )([\\s\\S]*)'\n",
    "        catch_appellant = re.search(regex_appellant, appellant_string)\n",
    "        if catch_appellant :\n",
    "            string_appellant_firm = catch_appellant.group(0)\n",
    "            # Remove leading and trailing spaces\n",
    "            string_appellant_firm = string_appellant_firm.strip()\n",
    "            flag_appellant = 1\n",
    "        \n",
    "        # Catch what's after 'legal representative' \n",
    "        regex_appellant = '(?<=legal representative)([\\s\\S]*)'\n",
    "        catch_appellant = re.search(regex_appellant, appellant_string)\n",
    "        if catch_appellant :\n",
    "            string_appellant_firm = catch_appellant.group(0)\n",
    "            # Remove leading and trailing spaces\n",
    "            string_appellant_firm = string_appellant_firm.strip()\n",
    "            flag_appellant = 1\n",
    "\n",
    "        # Catch what's after 'instructed by' \n",
    "        regex_appellant = '(?<=instructed by)([\\s\\S]*)'\n",
    "        catch_appellant = re.search(regex_appellant, appellant_string)\n",
    "        if catch_appellant :\n",
    "            string_appellant_firm = catch_appellant.group(0)\n",
    "            # Remove leading and trailing spaces\n",
    "            string_appellant_firm = string_appellant_firm.strip()\n",
    "            flag_appellant = 1\n",
    "\n",
    "        #print(appellant_string)\n",
    "        if flag_appellant == 1:\n",
    "            string_appellant_firm = string_appellant_firm\n",
    "            #print(string_appellant_firm)\n",
    "        else:\n",
    "            for tag in home_office_tags:\n",
    "                if appellant_string != None and tag in appellant_string:\n",
    "                    string_appellant_firm = 'Home Office'\n",
    "                    flag_appellant = 1\n",
    "                    break\n",
    "                else:\n",
    "                    string_appellant_firm = np.nan\n",
    "            #print(string_appellant_firm)\n",
    "        appellants_firm.append(string_appellant_firm)\n",
    "        appellants.append(appellant_string)\n",
    "        decision.update({'Appellant entity:': string_appellant_firm})\n",
    "\n",
    "\n",
    "        # Second, proceed with respondent's related firm info\n",
    "        flag_respondent = 0\n",
    "\n",
    "        # Catch what's after ' of ' \n",
    "        regex_respondent = '(?<= of )([\\s\\S]*)'\n",
    "        catch_respondent = re.search(regex_respondent, respondent_string)\n",
    "        if catch_respondent :\n",
    "            string_respondent_firm = catch_respondent.group(0)\n",
    "            # Remove leading and trailing spaces\n",
    "            string_respondent_firm = string_respondent_firm.strip()\n",
    "            flag_respondent = 1\n",
    "        \n",
    "        # Catch what's after 'legal representative' \n",
    "        regex_respondent = '(?<=legal representative)([\\s\\S]*)'\n",
    "        catch_respondent = re.search(regex_respondent, respondent_string)\n",
    "        if catch_respondent:\n",
    "            string_respondent_firm = catch_respondent.group(0)\n",
    "            # Remove leading and trailing spaces\n",
    "            string_respondent_firm = string_respondent_firm.strip()\n",
    "            flag_respondent = 1\n",
    "\n",
    "        # Catch what's after 'instructed by' \n",
    "        regex_respondent = '(?<=instructed by)([\\s\\S]*)'\n",
    "        catch_respondent = re.search(regex_respondent, respondent_string)\n",
    "        if catch_respondent :\n",
    "            string_respondent_firm = catch_respondent.group(0)\n",
    "            # Remove leading and trailing spaces\n",
    "            string_respondent_firm = string_respondent_firm.strip()\n",
    "            flag_respondent = 1\n",
    "\n",
    "        #print(respondent_string)\n",
    "        if flag_respondent == 1:\n",
    "            string_respondent_firm = string_respondent_firm\n",
    "            #print(string_respondent_firm)\n",
    "        else:\n",
    "            for tag in home_office_tags:\n",
    "                if respondent_string != None and tag in respondent_string:\n",
    "                    string_respondent_firm = 'Home Office'\n",
    "                    flag_respondent = 1\n",
    "                    break\n",
    "                else:\n",
    "                    string_respondent_firm = np.nan\n",
    "            #print(string_respondent_firm)\n",
    "        respondents_firm.append(string_respondent_firm)\n",
    "        respondents.append(respondent_string)\n",
    "        decision.update({'Respondent entity:': string_respondent_firm})\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31933\n",
      "31933\n",
      "31933\n",
      "31933\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(appellants))\n",
    "print(len(appellants_firm))\n",
    "\n",
    "print(len(respondents))\n",
    "print(len(respondents_firm))\n",
    "\n",
    "#data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The decision of the judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision of the judge is the most challenging piece of information to extract from the documents. # First isolate the part of the document most likely to include the decission the second half of the document. Second, get rid of annexes and appendixes. third, # classifying judgments is not the same as classifying cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 20:28:12 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-11-13 20:28:12 INFO: Use device: cpu\n",
      "2021-11-13 20:28:12 INFO: Loading: tokenize\n",
      "2021-11-13 20:28:12 INFO: Done loading processors!\n",
      "100%|██████████| 35305/35305 [2:00:38<00:00,  4.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# nlp sentence tokenizer with Stanford\n",
    "nlp = stanza.Pipeline(lang = 'en', processors = 'tokenize', tokenize_no_ssplit = True)\n",
    "\n",
    "# Store decisions in a list to make a df\n",
    "decisions = []\n",
    "files_judge = []\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the full text of the court decision\n",
    "    string = decision.get('String')\n",
    "    file_name = decision.get('File')\n",
    "    files_judge.append(file_name)\n",
    "\n",
    "    # Use only second half of text (skip references to annxes and appendixes)\n",
    "    string = string[len(string)//2:]\n",
    "\n",
    "    # Discard text following appendix and annexes\n",
    "    string = string.rsplit(\"appendix\", 1)\n",
    "    string = string[0]\n",
    "    string = string.rsplit(\"annex\", 1)\n",
    "    string = string[0]\n",
    "\n",
    "    # Narrow down the search from the end\n",
    "    # Split on last occurrence of \"Signed\"\n",
    "    string = string.rsplit(\"Signed\", 1)\n",
    "    string = string[0].lower()\n",
    "\n",
    "\n",
    "    # Keep a max of 2000 characters\n",
    "    string = string[ min(-2000, len(string)):]\n",
    "\n",
    "    # Get rid of text after the last occurrence of 'anonymity'\n",
    "    string = string.rsplit(\"anonymity\", 1)\n",
    "    string = string[0]\n",
    "\n",
    "    # Apply stanford nlp\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # List to store the ruling sentences\n",
    "    catch = []\n",
    "    # Flag = 1 when decision found\n",
    "    flag = 0\n",
    "        \n",
    "    # Make sentences\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sente = [token.text for token in sentence.tokens]\n",
    "        # Keep only the alpha tokens\n",
    "        sente = [e for e in sente if e.isalpha()]\n",
    "        #print(type(sente))\n",
    "        catch.append(sente)\n",
    "        \n",
    "    # Identify decision leads in sentences\n",
    "    decision_leads = [['notice', 'of', 'decision'], ['decision'], ['decisions'], ['conclusions'], ['conclusion']]\n",
    "        \n",
    "    # When decision lead found, trim catch and update flag value \n",
    "    for lead in decision_leads:\n",
    "        try:\n",
    "            # Find index of decision lead in ruling\n",
    "            index = catch.index(lead)\n",
    "            # Remove sentences before the decision lead sentence\n",
    "            del catch[0:index]\n",
    "            # Flatten the list of lists/sentences\n",
    "            flat_catch = [item for sublist in catch for item in sublist]\n",
    "            # Decision found\n",
    "            flag = 1\n",
    "            # Store decision in decisions list\n",
    "            decisions.append(flat_catch)\n",
    "            decision.update({'Decision:': flat_catch})\n",
    "            #print('Found decision 1')\n",
    "            #print(flat_catch)\n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # If a decision has not been found yet (flag = 0)\n",
    "    if flag == 0:\n",
    "    # Look for partial hits in text \n",
    "        decision_leads_part = [['for', 'the', 'above', 'reasons'], ['for', 'the', 'reasons', 'i', 'have', 'given'], ['general', 'conclusions'],\n",
    "        ['for', 'the', 'reasons', 'set', 'out', 'above'], ['for', 'all', 'of', 'these', 'reasons'], ['decision', 'and', 'directions'], ['conclusions'],\n",
    "        ['notice', 'of', 'decision'], ['decision','the', 'application', 'for', 'judicial', 'review', 'is'], ['there', 'is', 'no', 'material', 'error', 'of', 'law', 'in'],\n",
    "        ['decision', 'the', 'decision', 'of', 'tribunal', 'judge', 'dean', 'promulgated'], ['the', 'decision', 'of', 'the', 'ftt', 'is', 'set', 'aside'],\n",
    "        ['i', 'grant', 'permission', 'to', 'appeal', 'i', 'set', 'aside', 'the', 'decision', 'of', 'the', 'tribunal'], ['i', 'set', 'aside', 'that', 'decision'],\n",
    "        ['the', 'appellant', 'appeal', 'as', 'originally', 'brought', 'to', 'the', 'ftt', 'is', 'dismissed'], ['i', 'do', 'not', 'set', 'aside', 'the', 'decision']]\n",
    "            \n",
    "        for element in catch:\n",
    "            for part in decision_leads_part:\n",
    "                idx_part = decision_leads_part.index(part)\n",
    "                if all_exist(decision_leads_part[idx_part], element):\n",
    "                    index = catch.index(element)\n",
    "                    # Decision found in catch\n",
    "                    flag = 1\n",
    "                    # Remove sentences before the decision lead sentence\n",
    "                    del catch[0:index]\n",
    "                    # Flatten the list of lists/sentences\n",
    "                    flat_catch = [item for sublist in catch for item in sublist]\n",
    "                    #print('Found decision 2')\n",
    "                    #print(flat_catch)\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "        # If a decision has still not been found (flag = 0)\n",
    "        if flag == 0:\n",
    "            decisions.append(np.nan)\n",
    "            decision.update({'Decision:': np.nan})\n",
    "            #print('Did not find a decision')\n",
    "            #print(catch)\n",
    "        else:\n",
    "            # Store decision in decisions list\n",
    "            decisions.append(flat_catch)\n",
    "            decision.update({'Decision:': flat_catch})\n",
    "            continue\n",
    "\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35305\n",
      "35305\n",
      "['decision', 'the', 'determination', 'of', 'the', 'tribunal', 'having', 'been', 'found', 'to', 'contain', 'a', 'material', 'error', 'of', 'law', 'i', 'substitute', 'the', 'following', 'decision', 'the', 'appellant', 'appeal', 'is', 'allowed', 'under', 'the', 'immigration', 'rules']\n",
      "HU166042017\n",
      "['the', 'judge', 'also', 'went', 'on', 'to', 'consider', 'whether', 'or', 'not', 'any', 'exceptional', 'circumstances', 'existed', 'in', 'this', 'particular', 'case', 'the', 'findings', 'and', 'conclusions', 'are', 'comprehensive', 'and', 'when', 'the', 'decision', 'is', 'viewed', 'holistically', 'the', 'judge', 'consideration', 'is', 'entirely', 'sound', 'in', 'light', 'of', 'the', 'above', 'the', 'appellant', 'appeal', 'to', 'the', 'upper', 'tribunal', 'is', 'dismissed', 'and', 'the', 'decision', 'of', 'the', 'tribunal', 'stands', 'anonymity', 'i', 'make', 'no']\n",
      "PA098412018\n",
      "['notice', 'of', 'decision', 'for', 'the', 'above', 'reasons', 'the', 'decision', 'i', 'on', 'the', 'appellant', 'appeal', 'is', 'to', 'allow', 'it', 'on', 'asylum', 'grounds', 'to', 'conclude', 'as', 'found', 'in', 'my', 'previous', 'error', 'of', 'law', 'decision', 'the', 'ftt', 'judge', 'materially', 'erred', 'in', 'law', 'the', 'decision', 'i', 'is', 'to', 'allow', 'the', 'appellant', 'appeal', 'direction', 'regarding', 'anonymity', 'rule', 'of', 'the', 'tribunal', 'procedure', 'upper', 'tribunal', 'rules', 'unless', 'and', 'until', 'a', 'tribunal', 'or', 'court', 'directs', 'otherwise', 'the', 'appellant', 'is', 'granted']\n",
      "File           0\n",
      "Decision    5220\n",
      "dtype: int64\n",
      "35305\n",
      "35305\n"
     ]
    }
   ],
   "source": [
    "dict_decisions = {'File':files_judge,'Decision':decisions}\n",
    "\n",
    "df = pd.DataFrame(dict_decisions, columns=['File','Decision'])\n",
    "df.isna().sum()\n",
    "#print(data[49])\n",
    "print(len(files_judge))\n",
    "print(len(decisions))\n",
    "#print(decisions[32488])\n",
    "#print(files[5000])\n",
    "#rint(decisions[5000])\n",
    "#print(files[6000])\n",
    "print(decisions[6002])\n",
    "#print(df[df['Decision'].isnull()])\n",
    "print(df.isnull().sum(axis = 0))\n",
    "\n",
    "#print(json.dumps(data[32554], indent = 4, sort_keys = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sense of the decision.\n",
    "\n",
    "The decision has been isolated. However, no information on whether the sentence accepts/rejects or is neutral. Sense of decision depends on the appellent. If appellent is home office, then... The decision of the First-tier Tribunal did not involve the  making  of an error of law and I uphold it\n",
    "is accepted, otherwise is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:01<00:00, 28238.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Store decisions in a list to make a df\n",
    "decision_text = []\n",
    "decision_label = []\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the full text of the court decision\n",
    "    string = decision.get('Decision:')\n",
    "    #file_name = decision.get('File')\n",
    "    if isinstance(string, float):\n",
    "        continue\n",
    "    else:\n",
    "        # If a label has not been found (flag = 0)\n",
    "        flag = 0\n",
    "        # The decisions are stored as a listt of tokens\n",
    "        string = ' '.join(x for x in string)\n",
    "        # print(string)\n",
    "        decision_text.append(string)\n",
    "        # Look for partial hits in text \n",
    "        decision_labels_reject = ['appeal dismissed', 'appeal is dismissed','this application is refused', 'the appeal is dismissed', 'the decision of the first-tier Tribunal stands',\n",
    "            'the decision of the tribunal did not involve the making of a material error of law', 'the original decision shall stand',\n",
    "            'did not involve the making of an error on a point of law', 'appeal remains dismissed', 'decision stands', 'not satisfied that the judge erred',\n",
    "            'not involve an error on', 'do not set aside the decision', 'this application is refused', 'i therefore uphold the decision of',\n",
    "            'does not contain a material error of law and shall stand', 'it shall stand', 'did not contain any error of law', 'did not involve the making of a material error of law',\n",
    "            'dismissing the claimant appeal', 'does not contain a material error of law','no material error of law has been established',\n",
    "            'the decision to dismiss the appellant appeal shall stand', 'the appeal is statutorily abandoned', 'tribunal decision is not vitiated by legal error',\n",
    "            'there is no material error of law in the tribunal judge decision', 'the appeal by the secretary of state is dismissed',\n",
    "            'decision of the tribunal does not disclose an error on a point of law', 'errors are not material such that the decision should be set aside',\n",
    "            'the appeal of the appellant is dismissed', 'reveals no error of law and stands', 'the appeal to the upper tribunal is dismissed',\n",
    "            'the respondent appeal be dismissed', 'the judge did not materially err in law', 'did not involve a making of or a material error of law',\n",
    "            'appeal to the upper tribunal is dismissed', 'i am dismissing', 'did not contain a material error of law', 'the decision of the ftt judge must stand',\n",
    "            'there are no errors of law', 'there is no material error of law', 'this appeal is dismissed', 'i dismiss the appeal', 'tribunal did not involve the making of an error of law',\n",
    "            'contains no material error of law', 'the decision of the tribunal stands', 'the decision of the tribunal did not involve the making of an error of law',\n",
    "            'this appeal is therefore dismissed', 'i refuse permission to appeal', 'the applicant appeal to the upper tribunal is therefore dismissed',\n",
    "            'decision of the ftt did not involve the making of an error of law', 'i do not set it aside', 'the decision of the tribunal did not involve the making of any error on a point of law',\n",
    "            'the appellant appeal to the upper tribunal is therefore dismissed', 'permission to appeal to the court of appeal is refused', 'the decision of the tribunal is upheld', 'the appeals are dismissed',\n",
    "            'did not make a material error of law', 'the decision of the tribunal shall stand', 'the appeal of the secretary of state is dismissed',\n",
    "            'the appeal of the secretary of state is dismissed', 'do not disclose any material error of law', 'the tribunal did not err in law',\n",
    "            'judge did not make an error of law', 'i uphold the decision to dismiss', 'no material error of law was made', 'did not make a material error on a point of law',\n",
    "            'there was no error of law made by the judge', 'we dismiss the secretary of state appeal', 'the appeals to the upper tribunal are dismissed',\n",
    "            'decision of the tribunal does not contain an error on a point', 'is disused', 'decision of the tribunal does not contain an error on a point of law',\n",
    "            'the appellants appeals are each dismissed', 'the decision by dismissing the appeal', 'i uphold the tribunal determination and dismiss the appeal'\n",
    "            'this appeal is dismissed', 'appeals dismissed for all the appellants', 'decision of the tribunal does not disclose an error in law', 'tribunal does not show a material error on a point of law',\n",
    "            'i uphold the tribunal determination and dismiss the appeal', 'these appeals are dismissed', 'determination does not disclose any material error of law',\n",
    "            'tribunal did not involve the making of a material error on a point of law', 'the decision of the tribunal containing no material error of law shall stand',\n",
    "            'decision of the tribunal does not disclose a material error on a point of law', 'there was no material error of law made by the judge',\n",
    "            'is free of legal error accordingly it must stand', 'decision of the tribunal does not contain errors of law that decision shall stand',\n",
    "            'the ftt judge did not err in law', 'i find no error of law', 'there is no error in law', 'no material error has been established',\n",
    "            'no material error of law has been demonstrated', 'there being no material error of law in the decision', 'the appeals of the appellants are dismissed',\n",
    "            'no material error of law is established in the decision', 'i dismissed the appellant appeal', 'i dismiss the appellant appeal', 'the decision of the tribunal contained a material error of law',\n",
    "            'i dismiss the appellant appeal', 'there is no error of law in the judge findings', 'i find no material error of law', 'the appeal of the claimant is dismissed',\n",
    "            'the decision of the original judge will stand', 'the appeal is to the upper tribunal is dismissed', 'did not involve the making of an error of law',\n",
    "            'the appeal is therefore dismissed', 'is dismissed', 'is refused', 'the appellants appeals are dismissed', 'i remake the decision on the appeal dismissing it',\n",
    "            'determination of the tribunal contains no error of law and it is upheld', 'the judge did not err in law', 'the appeal is as dismissed',\n",
    "            'the decision of the tribunal contains no error of law and shall stand', 'the claim is therefore dismissed', 'i find that there is no valid appeal before the tribunal',\n",
    "            'we have given we dismiss this appeal', 'this appeal must be dismissed', 'we conclude that no material error of law has been shown', 'there is no error of law',\n",
    "            'decision of the tribunal contains no error of law']\n",
    "\n",
    "        decision_labels_accept = ['i set it aside', 'did involve the making of an', 'decision of the tribunal is set aside', 'is set aside',\n",
    "            'i allow the claimant', 'appeal remains allowed', 'the appeal is allowed', 'appeal is granted', 'the appeal is allowed', 'the appellant is granted',\n",
    "            'i set aside the judge decision', 'i therefore set aside the decision', 'i set aside the decision', 'appeal allowed', 'appeal is allowed',\n",
    "            'set aside the decision', 'the decision of the first-tier tribunal has already been set aside', 'i allow the claimant', 'appeals allowed',\n",
    "            'i allow the appeal', 'is allowed', 'the first-tier tribunal erred in law', 'these appeals are allowed', 'the judge materially erred in law',\n",
    "            'does not contain an error of law', 'the appeals are allowed', 'the tribunal erred in law', 'the decision of the tribunal contained an error of law',\n",
    "            'involved the making of a material error of law', 'decision of the tribunal is tainted by material errors of law', 'i set the decision aside',\n",
    "            'tribunal involved an error on a point of law', 'did involve a material error of law', 'the appellants are granted', 'there is no material error of law in the determination',\n",
    "            'the judge erred in allowing this appeal', 'i remake the decision allowing the appellant appeal', 'did err in the making of', 'there are material errors of law',\n",
    "            'is therefore set aside', 'the appellant is granted', 'there is a material error of law', 'i allow the appellant eea appeal', 'should be set aside', 'by allowing the appeal',\n",
    "            'the tribunal decision involved the making of an error on a point of law', 'involved the making of an error on a point of law', 'by allowing the appellant appeal',\n",
    "            'involved the making of an error of law', 'the decision of the tribunal does not contain errors of law and it is upheld', 'decision of the tribunal has been set aside',\n",
    "            'the claimant appeal to the ftt is remade and allowed', 'tribunal was vitiated by legal error', 'discloses an error of law', 'we set aside that decision',\n",
    "            'the appeal is remitted to the tribunal for a hearing afresh', 'contains a material error of law', 'the tribunal made errors of law', 'the determination of the tribunal does not disclose a material error of law',\n",
    "            'the determination of the tribunal contained an error of law', 'the judge made an error on a point of law', 'the decision of the tribunal is hereby set aside for material error',\n",
    "            'the tribunal judge made errors of law', 'the appeal against the judge decision is therefore allowed', 'i allow the claim for asylum and on human rights grounds',\n",
    "            'the tribunal decision is vitiated by a material error of law', 'i find material error in law', 'the appellants appeals are allowed', 'the human rights appeals are allowed',\n",
    "            'i therefore allow the appeal', 'does not disclose an error of law and stands', 'determination does contain a material error of law', 'we allow the appeal',\n",
    "            'we have decided to allow this appeal', 'this appeal is accordingly allowed', 'both appeals are allowed', 'the appeals of the four appellants are allowed']\n",
    "\n",
    "        # Check first evidence of reject, if reject be done with it\n",
    "\n",
    "        for label in decision_labels_reject:\n",
    "            if string != None and label in string:\n",
    "                #print(\"Rejected!\")\n",
    "                flag = -1\n",
    "                decision_label.append('Rejected')\n",
    "                decision.update({'Decision label:': 'Rejected'})\n",
    "                break\n",
    "            else:\n",
    "                'Not found!'\n",
    "        if flag == -1:\n",
    "            continue\n",
    "        else:\n",
    "            # Check evidence for Accept\n",
    "            for label in decision_labels_accept:\n",
    "                if string != None and label in string:\n",
    "                    #print(\"Accepted!\")\n",
    "                    flag = 1\n",
    "                    decision_label.append('Accepted')\n",
    "                    decision.update({'Decision label:': 'Accepted'})\n",
    "                    break\n",
    "                else:\n",
    "                    'Not found!'\n",
    "        if flag == 0:\n",
    "            decision_label.append('Neutral')\n",
    "            decision.update({'Decision label:': 'Neutral'})\n",
    "\n",
    "    #print(flag)\n",
    "# The decision/ruling by the judge ('Decision:').\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30085\n",
      "30085\n",
      "                                           Decision text Decision label\n",
      "0      notice of decision directions the decision of ...       Accepted\n",
      "1      in light of my conclusions on that point littl...       Rejected\n",
      "2      notice of decision the decision of the tribuna...       Rejected\n",
      "3      notice of decision the decision of the tribuna...       Accepted\n",
      "4      decision the decision of tribunal judge malcol...       Accepted\n",
      "...                                                  ...            ...\n",
      "30080  for the reasons we have given in paragraph we ...       Accepted\n",
      "30081  conclusions a northern cyprus is not capable o...       Accepted\n",
      "30082  for the reasons we have given we dismiss this ...       Rejected\n",
      "30083  our conclusions on the general issues relating...       Rejected\n",
      "30084  conclusions for each of the main reason a and ...       Accepted\n",
      "\n",
      "[30085 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision text</th>\n",
       "      <th>Decision label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30085</td>\n",
       "      <td>30085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>24093</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>notice of decision the appeal is dismissed no</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>231</td>\n",
       "      <td>13711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Decision text Decision label\n",
       "count                                           30085          30085\n",
       "unique                                          24093              3\n",
       "top     notice of decision the appeal is dismissed no       Accepted\n",
       "freq                                              231          13711"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(decision_text))\n",
    "print(len(decision_label))\n",
    "\n",
    "dict_sense_decisions = {'Decision text':decision_text,'Decision label':decision_label}\n",
    "\n",
    "df_decision_sense = pd.DataFrame(dict_sense_decisions, columns=['Decision text','Decision label'])\n",
    "df_decision_sense.isna().sum()\n",
    "df_decision_sense.sum()\n",
    "\n",
    "print(df_decision_sense)\n",
    "\n",
    "df_decision_sense.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decision label\n",
       "Accepted          13711\n",
       "Rejected          12778\n",
       "Neutral            3596\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_decision_sense[['Decision label']].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_df = df_decision_sense[df_decision_sense['Decision label'] == 'Neutral']\n",
    "rslt_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in decision_labels_accept:\n",
    "    idx_part = decision_labels_accept.index(label)\n",
    "    if all_exist(decision_labels_accept[idx_part], example):\n",
    "        flag = 1\n",
    "        break\n",
    "        #print('Accept')\n",
    "    else:\n",
    "        flag = 0\n",
    "        #print('Reject')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Nationality of the appellant. \n",
    "The field country is empty to a large extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of countries and nationalities to be checked against the text\n",
    "countries = ['Afghanistan', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua', \n",
    "'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus',\n",
    "'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia', 'Bolivia, Plurinational State of', 'Bonaire', 'Bonaire, Sint Eustatius and Saba', \n",
    "'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', \n",
    "'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', \n",
    "'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Congo', 'Cook Islands', 'Costa Rica', \"Côte d'Ivoire\", \n",
    "'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', \n",
    "'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', \n",
    "'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', \n",
    "'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', \n",
    "'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', 'Iraq', \n",
    "'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', \n",
    "\"Korea, Democratic People's Republic of\", 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', \n",
    "'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', \n",
    "'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia', \n",
    "'Federated States of', 'Micronesia', 'Moldova, Republic of', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', \n",
    "'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', \n",
    "'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Palestine', 'Panama', 'Papua New Guinea', \n",
    "'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russian Federation', 'Russia', \n",
    "'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', \n",
    "'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', \n",
    "'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', \n",
    "'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', \n",
    "'Sweden', 'Switzerland', 'Syria', 'Syrian Arab Republic', 'Taiwan', 'Taiwan, Province of China', 'Tajikistan', 'Tanzania', \n",
    "'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', \n",
    "'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United States', 'United States Minor Outlying Islands', 'Uruguay', \n",
    "'Uzbekistan', 'Vanuatu', 'Venezuela', 'Venezuela, Bolivarian Republic of', 'Vietnam', 'Viet Nam', 'Virgin Islands, British', 'Virgin Islands, U.S.', \n",
    "'Wallis and Futuna', 'Yemen', 'Zambia', 'Zimbabwe', 'Pakistani', 'Iranian', 'Bangladeshi', 'Indian', 'Egyptian', 'Afghan', 'Albanian', 'Algerian', \n",
    "'American', 'Andorran', 'Angolan', 'Antiguans', 'Argentinean', 'Armenian', 'Australian', 'Austrian', 'Azerbaijani', 'Bahamian', 'Bahraini', 'Bangladeshi', 'Barbadian', 'Barbudans', 'Batswana', 'Belarusian', 'Belgian', 'Belizean', 'Beninese', 'Bhutanese', 'Bolivian', 'Bosnian', 'Brazilian', 'Bruneian', 'Bulgarian', 'Burkinabe', 'Burmese', 'Burundian', 'Cambodian', 'Cameroonian', 'Canadian', 'Cape Verdean', 'Central African', 'Chadian', 'Chilean', 'Chinese', 'Colombian', 'Comoran', 'Congolese', 'Costa Rican', 'Croatian', 'Cuban', 'Cypriot', 'Czech', 'Danish', 'Djibouti', 'Dominican', 'Dutch', 'Dutchman', 'Dutchwoman', 'East Timorese', 'Ecuadorean', 'Egyptian', 'Emirian', 'Equatorial Guinean', 'Eritrean', 'Estonian', 'Ethiopian', 'Fijian', 'Filipino', 'Finnish', 'French', 'Gabonese', 'Gambian', 'Georgian', 'German', 'Ghanaian', 'Greek', 'Grenadian', 'Guatemalan', 'Guinea-Bissauan', 'Guinean', 'Guyanese', 'Haitian', 'Herzegovinian', 'Honduran', 'Hungarian', 'I-Kiribati', 'Icelander', 'Indian', 'Indonesian', 'Iranian', 'Iraqi', 'Irish', 'Israeli', 'Italian', 'Ivorian', 'Jamaican', 'Japanese', 'Jordanian', 'Kazakhstani', 'Kenyan', 'Kittian and Nevisian', 'Kuwaiti', 'Kyrgyz', 'Laotian', 'Latvian', 'Lebanese', 'Liberian', 'Libyan', 'Liechtensteiner', 'Lithuanian', 'Luxembourger', 'Macedonian', 'Malagasy', 'Malawian', 'Malaysian', 'Maldivan', 'Malian', 'Maltese', 'Marshallese', 'Mauritanian', 'Mauritian', 'Mexican', 'Micronesian', 'Moldovan', 'Monacan', 'Mongolian', 'Moroccan', 'Mosotho', 'Motswana', 'Mozambican', 'Namibian', 'Nauruan', 'Nepalese', 'Netherlander', 'New Zealander', 'Ni-Vanuatu', 'Nicaraguan', 'Nigerian', 'Nigerien', 'North Korean', 'Northern Irish', 'Norwegian', 'Omani', 'Pakistani', 'Palauan', 'Panamanian', 'Papua New Guinean', 'Paraguayan', 'Peruvian', 'Polish', 'Portuguese', 'Qatari', 'Romanian', 'Russian', 'Rwandan', 'Saint Lucian', 'Salvadoran', 'Samoan', 'San Marinese', 'Sao Tomean', 'Saudi', 'Scottish', 'Senegalese', 'Serbian', 'Seychellois', 'Sierra Leonean', 'Singaporean', 'Slovakian', 'Slovenian', 'Solomon Islander', 'Somali', 'South African', 'South Korean', 'Spanish', 'Sri Lankan', 'Sudanese', 'Surinamer', 'Swazi', 'Swedish', 'Swiss', 'Syrian', 'Taiwanese', 'Tajik', 'Tanzanian', 'Thai', 'Togolese', 'Tongan', 'Trinidadian or Tobagonian', 'Tunisian', 'Turkish', 'Tuvaluan', 'Ugandan', 'Ukrainian', 'Uruguayan', 'Uzbekistani', 'Venezuelan', 'Vietnamese', 'Welsh', 'Yemenite', 'Zambian', 'Zimbabwean']\n",
    "\n",
    "countriesLower = [x.lower() for x in countries]\n",
    "#print(countriesLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 18:00:58 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-11-14 18:00:58 INFO: Use device: cpu\n",
      "2021-11-14 18:00:58 INFO: Loading: tokenize\n",
      "2021-11-14 18:00:58 INFO: Done loading processors!\n",
      "100%|██████████| 35305/35305 [5:04:39<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Store decisions in a list to make a df\n",
    "nationalities = []\n",
    "files_nationalities = []\n",
    "\n",
    "# nlp sentence tokenizer with Stanford\n",
    "nlp = stanza.Pipeline(lang = 'en', processors = 'tokenize', tokenize_no_ssplit = True)\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the full text of the court decision\n",
    "    string = decision.get('String')\n",
    "    file_name = decision.get('File')\n",
    "    files_nationalities.append(file_name)\n",
    "\n",
    "    # Use only first third of text\n",
    "    string = string[:len(string)//3]\n",
    "    # All text in lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Apply stanford nlp\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # List to store the ruling sentences\n",
    "    catch = []\n",
    "\n",
    "    # Make sentences\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sente = [token.text for token in sentence.tokens]\n",
    "        # Keep only the alpha tokens\n",
    "        sente = [e for e in sente if e.isalpha()]\n",
    "        #print(type(sente))\n",
    "        catch.append(sente)\n",
    "    # Look for partial hits in text\n",
    "    nationality_leads_part = [['the', 'appellant', 'is', 'a', 'national', 'of'], ['the', 'appellant', 'is', 'a', 'citizen', 'of'],\n",
    "    ['the', 'respondent', 'is', 'a', 'citizen', 'of'], ['the', 'appellants', 'are', 'all', 'citizens', 'of'], ['citizen', 'of'],\n",
    "    ['national', 'of'], ['citizens', 'of']]\n",
    "    # Nationality not yet found (flag = 0)\n",
    "    flag = 0\n",
    "    for element in catch:\n",
    "        for part in nationality_leads_part:\n",
    "            idx_part = nationality_leads_part.index(part)\n",
    "            if sublist(nationality_leads_part[idx_part], element):\n",
    "                #print(nationality_leads_part[idx_part])\n",
    "                index = catch.index(element)\n",
    "                # Nationality lead found in catch\n",
    "                # Remove sentences before sentence with decision lead\n",
    "                new_catch = catch[index]\n",
    "                # flag2 = 1 when nationality is found in 1\n",
    "                for token in new_catch:\n",
    "                    #indx_country = countriesLower.index(country)\n",
    "                    if sublist([token], countriesLower):\n",
    "                        # Nationality found (flag = 1)\n",
    "                        flag = 1\n",
    "                        nationalities.append(token)\n",
    "                        decision.update({'Nationality:': token})\n",
    "                        #print(f'FOUND A NATIONALITY {token} in {file_name}')\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                    break\n",
    "            if flag == 1:\n",
    "                break\n",
    "        if flag == 1:\n",
    "            break\n",
    "    # If a decision has still not been found (flag = 0)\n",
    "    if flag == 0:\n",
    "        #print(f'Did not find a nationality {file_name} in catch: {catch}')\n",
    "        nationalities.append(np.nan)\n",
    "        decision.update({'Nationality:': np.nan})\n",
    "        #print(f'Did not find a nationality in {catch}')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field \"nationality\" includes a mix of country names and nationalities. Nationalities are transformed to country names to harmonize the field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with nationality key and country name as value\n",
    "dict_nationality_csv = pd.read_csv('data/countries.csv', header = None, index_col = 4, squeeze = False).to_dict()\n",
    "dict_nationality = dict_nationality_csv[3]\n",
    "#print(dict_nationality)\n",
    "\n",
    "# Dictionary from country name key to code3 as value\n",
    "dict_country_code3 = pd.read_csv('data/countries.csv', header = None, index_col = 3, squeeze = False).to_dict()\n",
    "dict_country = dict_country_code3[2]\n",
    "#print(dict_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 962063.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the full text of the court decision\n",
    "    string = decision.get('Nationality:')\n",
    "    #file_name = decision.get('File')\n",
    "    if isinstance(string, float):\n",
    "        continue\n",
    "    else:\n",
    "        # If we are dealing with a nationality and not a country name\n",
    "        if string.capitalize() in dict_nationality:\n",
    "            country = dict_nationality[string.capitalize()]\n",
    "        else:\n",
    "            # Simply capitalize the name of the country\n",
    "            country = string.capitalize()\n",
    "        # Update decision in dict\n",
    "        decision.update({'Nationality:': country})\n",
    "        #print(country)\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a \"country\" field to the decision which includes the 3-digit code of the country of the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 1059067.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the full text of the court decision\n",
    "    string = decision.get('Nationality:')\n",
    "    #file_name = decision.get('File')\n",
    "    if isinstance(string, float):\n",
    "        continue\n",
    "    else:\n",
    "        if string in dict_country:\n",
    "            # Get country code from dict_country\n",
    "            country = dict_country[string]\n",
    "        else:\n",
    "            country = np.nan\n",
    "        # Update decision in dict\n",
    "        decision.update({'Country:': country})\n",
    "        #print(country)\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Case title:': '',\n",
       " 'Appellant name:': '',\n",
       " 'Status of case:': 'Unreported',\n",
       " 'Hearing date:': '28 Jan 2016',\n",
       " 'Promulgation date:': '4 Feb 2016',\n",
       " 'Publication date:': '14 Nov 2016',\n",
       " 'Last updated on:': '14 Nov 2016',\n",
       " 'Country:': 'UGA',\n",
       " 'Judges:': ['Jm Holmes'],\n",
       " 'Document': 'https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/50711/VA066172014.doc',\n",
       " 'Reference': ['VA/06617/2014'],\n",
       " 'Download': 'Yes',\n",
       " 'File': 'VA066172014',\n",
       " 'String': '\\n\\n                                              [pic]\\nUpper Tribunal\\n(Immigration and Asylum Chamber)                       Appeal Number:\\nVA/06617/2014\\n\\n                            THE IMMIGRATION ACTS\\n\\n|Heard at North Shields             |            Decision & Reasons       |\\n|                                   |Promulgated                          |\\n|On 28 January 2016                 |            On 4  February 2016      |\\n|Prepared 28 January 2016           |                                     |\\n\\n\\n                                   Before\\n\\n                    DEPUTY UPPER TRIBUNAL JUDGE JM HOLMES\\n\\n                                   Between\\n\\n                                    A. N.\\n                          (ANONYMITY DIRECTION MADE)\\n                                                                   Appellant\\n                                     And\\n\\n                      ENTRY CLEARANCE OFFICER PRETORIA\\n                                                                  Respondent\\n\\n\\nRepresentation:\\n\\nFor the Appellant:     Sponsor\\nFor the Respondent:    Mr Diwnycz, Home Office Presenting Officer\\n\\n\\n                          DETERMINATION AND REASONS\\n\\n        1. The Appellant, a citizen of Uganda,  born  on  29  August  2007,\\n           applied for leave to enter the United Kingdom for four weeks  as\\n           a family visitor to her brother, sister, and aunt.\\n        2. The Respondent first refused entry clearance to the Appellant by\\n           decision made on 23 April 2014 [E4], and the Appellant\\'s renewed\\n           application was refused on 24 September  2014  by  reference  to\\n           paragraph 41 (vi) and (vii) of the Immigration Rules. The Notice\\n           of Decision informed the Appellant that her right of appeal  was\\n           limited to the grounds identified in s84(1)(c) of the 2002  Act.\\n           She appealed that decision,  although  she  requested  that  the\\n           appeal be determined upon the papers before the Tribunal without\\n           a hearing. The Respondent raised no objection to that.\\n        3.  The  grounds  of  appeal  made   no   reference   to   unlawful\\n           discrimination, but they can and should  be  read  as  asserting\\n           that the decision was unlawful under s6 of the Human Rights  Act\\n           1998. The ECM reviewed the decision to refuse entry clearance on\\n           that basis on 2 February 2015, but chose to uphold it.\\n        4. The appeal was heard on the papers, and allowed  in  a  decision\\n           promulgated on 1 July  2015  by  First  Tier  Tribunal  Judge  K\\n           Lester.\\n        5. By a decision of First Tier Tribunal Judge  Nicholson  dated  29\\n           September 2015 the First Tier Tribunal  granted  the  Respondent\\n           permission to appeal on the basis  it  was  arguable  there  had\\n           been a failure by the Judge to dispose of the appeal pursuant to\\n           the restricted grounds permitted by s88A of  the  2002  Act  (as\\n           amended), because the  Judge  had  failed  to  ask  the  primary\\n           question of whether Article 8 was engaged  by  the  decision  at\\n           all; Adeji (visit visas) Article 8) [2015] UKUT  261,  and,  had\\n           failed to identify that this was at best a \"private life\"  entry\\n           clearance  appeal  which  could  not   therefore   succeed;   SS\\n           (Malaysia) [2004] UKAIT 91, and to the extent that the Judge had\\n           considered the appeal as a \"family life\" appeal  had  failed  to\\n           apply the appropriate test.\\n        6. The Appellant has filed no Rule 24 Notice. Thus the matter comes\\n           before me.\\n\\n\\n      Error of law?\\n        7. I remind myself that  s85A  of  the  2002  Act  applied  to  the\\n           evidence admissible upon the appeal, and of  the  guidance  upon\\n           the proper approach to Article 8  cases  involving  applications\\n           for leave to enter that is to be found in Mostafa (Article 8  in\\n           entry clearance) [2015] UKUT 112, and SS (Congo) [2015] EWCA Civ\\n           387.\\n        8. As the Upper Tribunal set out in Mostafa  [9]  and  Kaur  (visit\\n           appeals; Article 8) [2015] UKUT 487, the Judge  was  obliged  to\\n           assess the evidence to decide  whether  the  Appellant  met  the\\n           substance of the Immigration Rules at the date of  decision,  as\\n           she claimed she did, or whether she did not, as  the  Respondent\\n           had claimed. Although that question did not arise directly as  a\\n           result of the limited ground of appeal  available,  it  was  the\\n           context in which the decision upon the Article 8 appeal  was  to\\n           be made. Thus the Judge did not fall into the  error  identified\\n           in Virk & Others [2013] EWCA Civ 652, or Mostafa  [11],  because\\n           she did not purport to allow the appeal  under  the  Immigration\\n           Rules.\\n        9. As set out in Kaur, even if an appellant can establish that they\\n           met the requirements of paragraph 41,  it  does  not  mean  that\\n           their Article 8 appeal is strengthened to the point that it must\\n           be allowed. Article 8 must first be shown to be engaged. As  was\\n           said in Mostafa the appellant must show that the denial  of  the\\n           visit has a material impact on their Article  8  rights,  before\\n           there is any consideration of the balance between the  competing\\n           interests of the individual, and the  state\\'s  interest  in  the\\n           maintenance of effective immigration  controls.  In  SS  (Congo)\\n           [2015] EWCA Civ 387 it was said that  unless  an  appellant  can\\n           show that she has individual interests at stake that are covered\\n           by Article 8, and that are of \"a particularly  pressing  nature\"\\n           so  as  to  give  rise  to  a  \"strong  claim  that   compelling\\n           circumstances may exist to justify the grant of LTE outside  the\\n           rules\" she is exceedingly unlikely to succeed in  an  Article  8\\n           appeal.\\n       10. Although the Judge did direct herself that the \"facts  as  found\\n           constitute an interference with the Appellant\\'s right to respect\\n           for her family life\"[14] it is very far from clear what findings\\n           of fact she is referring to. None of the facts  recited  in  the\\n           course of the decision that \"this is a  genuine  family  visit\",\\n           and the finding that  the  Appellant  met  the  requirements  of\\n           paragraph 41, would justify such  a  conclusion.  There  appears\\n           therefore to be, at best, some confusion  in  the  Judge\\'s  mind\\n           over whether  a  genuine  intention  to  visit  members  of  the\\n           extended family  constitutes  part  of  \"family  life\"  for  the\\n           purposes of Article 8, when plainly rather  more  than  that  is\\n           required.\\n       11. In my judgement what was required, but did  not  occur,  was  an\\n           analysis of the evidence to identify whether the ties  with  the\\n           individuals relied upon had the necessary quality  required,  or\\n           whether this was in truth  a  \"private  life\"  appeal,  and  the\\n           elements of that which were relied upon; Marckx v Belgium [1980]\\n           2EHRR 330 and Singh v ECO New Delhi [2004] EWCA  Civ  1075,  and\\n           Abbasi (visits - bereavement - Article 8 [2015] UKUT 463\\n       12. It follows that I must set aside the decision and remake  it.  I\\n           do so on the papers  filed  for  the  original  appeal,  as  the\\n           parties had requested the First Tier Tribunal to do, and as  the\\n           sponsor confirmed before me he was content that I should do.\\n\\n\\n      Decision remade\\n       13. I am not satisfied on the  balance  of  probabilities  that  the\\n           immigration decision under  appeal  did  engage  the  Article  8\\n           rights of the Appellant, because I am  not  satisfied  that  the\\n           documentary evidence she offered to the Judge  established  that\\n           she had a \"family  life\"  at  the  date  of  decision  with  the\\n           individuals she would be prevented from visiting as a result  of\\n           that  decision.  The  evidence  does  not  establish  that   the\\n           relationship she has with those relatives goes beyond the normal\\n           emotional ties between adult relatives of the type identified in\\n           the  evidence.   Whilst   her   desire   to   visit   them   was\\n           understandable and genuine, as  the  Judge  accepted,  and  thus\\n           could constitute a part of her  \"private  life\"  for  Article  8\\n           purposes, this was not a \"discrete facet\" situation of the  type\\n           rehearsed in Abbasi. Thus Article  8  was  not  engaged  by  the\\n           decision under appeal, and the Article 8 appeal  must  therefore\\n           be dismissed.\\n\\n\\n   DECISION\\n\\n\\n      The Decision of the First Tier Tribunal which  was  promulgated  on  1\\n      July 2015 did involve the making of an error of law in the decision to\\n      allow the appeal on human rights grounds that requires  that  decision\\n      to be set aside and remade.\\n\\n\\n      The appeal is dismissed.\\n\\n\\n\\n\\n\\nDirection regarding anonymity - Rule 14 Tribunal Procedure (Upper  Tribunal)\\nRules 2008\\n      Unless and until the Tribunal  directs  otherwise  the  Appellant  is\\n        granted anonymity. No report of these proceedings shall directly or\\n        indirectly  identify  her.  This  direction  applies  both  to  the\\n        Appellant and to  the  Respondent.  Failure  to  comply  with  this\\n        direction could lead to proceedings being brought for  contempt  of\\n        court.\\n\\n\\n\\n\\n\\n\\n\\nDeputy Upper Tribunal Judge JM Holmes\\n\\nDated: 28 January 2016\\n',\n",
       " 'ID': '2fd987e3-e4a0-43d8-892f-6404814af3b5',\n",
       " 'Code label:': 'VA',\n",
       " 'Heard at:': 'North Shields',\n",
       " 'Decision:': ['decision',\n",
       "  'the',\n",
       "  'decision',\n",
       "  'of',\n",
       "  'the',\n",
       "  'first',\n",
       "  'tier',\n",
       "  'tribunal',\n",
       "  'which',\n",
       "  'was',\n",
       "  'promulgated',\n",
       "  'on',\n",
       "  'july',\n",
       "  'did',\n",
       "  'involve',\n",
       "  'the',\n",
       "  'making',\n",
       "  'of',\n",
       "  'an',\n",
       "  'error',\n",
       "  'of',\n",
       "  'law',\n",
       "  'in',\n",
       "  'the',\n",
       "  'decision',\n",
       "  'to',\n",
       "  'allow',\n",
       "  'the',\n",
       "  'appeal',\n",
       "  'on',\n",
       "  'human',\n",
       "  'rights',\n",
       "  'grounds',\n",
       "  'that',\n",
       "  'requires',\n",
       "  'that',\n",
       "  'decision',\n",
       "  'to',\n",
       "  'be',\n",
       "  'set',\n",
       "  'aside',\n",
       "  'and',\n",
       "  'remade',\n",
       "  'the',\n",
       "  'appeal',\n",
       "  'is',\n",
       "  'dismissed',\n",
       "  'direction',\n",
       "  'regarding',\n",
       "  'anonymity',\n",
       "  'rule',\n",
       "  'tribunal',\n",
       "  'procedure',\n",
       "  'upper',\n",
       "  'tribunal',\n",
       "  'rules',\n",
       "  'unless',\n",
       "  'and',\n",
       "  'until',\n",
       "  'the',\n",
       "  'tribunal',\n",
       "  'directs',\n",
       "  'otherwise',\n",
       "  'the',\n",
       "  'appellant',\n",
       "  'is',\n",
       "  'granted'],\n",
       " 'Nationality:': 'Uganda',\n",
       " 'Representation:': ['for',\n",
       "  'the',\n",
       "  'appellant',\n",
       "  'sponsor',\n",
       "  'for',\n",
       "  'the',\n",
       "  'respondent',\n",
       "  'mr',\n",
       "  'diwnycz',\n",
       "  'home',\n",
       "  'office',\n",
       "  'presenting',\n",
       "  'officer'],\n",
       " 'Appellant:': 'ponsor',\n",
       " 'Respondent:': 'mr diwnycz home office presenting officer',\n",
       " 'Decision label:': 'Rejected',\n",
       " 'Appellant entity:': nan,\n",
       " 'Respondent entity:': 'Home Office'}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[22507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35305\n",
      "35305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "File               0\n",
       "Nationality    12537\n",
       "dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(files_nationalities))\n",
    "print(len(nationalities))\n",
    "#print(nationalities)\n",
    "dict_nationalities = {'File':files_nationalities,'Nationality':nationalities}\n",
    "df = pd.DataFrame(dict_nationalities, columns=['File','Nationality'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with empty/corrupt files that didn't upload a sentence string\n",
    "# Regex expression: What comes in between 'Before' and 'Between'\n",
    "# regex = '(?<=Before)([\\s\\S]*?)(?=Between)'\n",
    "regex = 'representation:([\\S\\s]*)for the respondent'\n",
    "catch = re.search(regex, decision_string.lower())\n",
    "#If the catch is successful\n",
    "if catch :\n",
    "    string = catch.group(0)\n",
    "    delimiters = ['|', '?', ':']\n",
    "    # Get rid of some table delimiters\n",
    "    for i in delimiters:\n",
    "        string = string.replace(i,'')\n",
    "    # Remove leading and trailing spaces\n",
    "    string = string.strip()\n",
    "    print(string)\n",
    "\n",
    "# Path to the txt documents\n",
    "txt_path = './data/processed/txt_files_test/'\n",
    "print(os.listdir(txt_path))\n",
    "# Loop over each text file and extract Court information\n",
    "for text in os.listdir(txt_path):\n",
    "    print(text)\n",
    "\n",
    "    with open(txt_path + text, 'r') as file:\n",
    "        decision_string = file.read()\n",
    "        # Regex expression: What comes after \"Heard at\" until hitting 3 balnks or new line\n",
    "        #regex = '(?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        #regex = 'Before([\\S\\s]*)Between'\n",
    "        regex = '(?<=Before)([\\s\\S]*?)(?=Between)'\n",
    "\n",
    "        catch = re.search(regex, decision_string)\n",
    "        #If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "\n",
    "            # Keep only alpha numeric\n",
    "            string = string.replace('|','')\n",
    "            #string = re.sub(r'[^A-Za-z0-9 ]+', '', string)\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            print(string)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "    # Loading string with court decision to data\n",
    "for txt_file in  tqdm(os.listdir(txt_path)):\n",
    "    \n",
    "    # Open file and obtain string and file_name\n",
    "    with open(txt_path + txt_file, 'r') as file:\n",
    "        string = file.read()\n",
    "        f_name, f_ext = os.path.splitext(file.name)\n",
    "        head, file_name = os.path.split(f_name)\n",
    "    # Search data list of dictionaries for dict where {\"File\":} = file_name\n",
    "    for d in data:\n",
    "        if d.get('File') == file_name:\n",
    "            # Add dictionary key 'String' with value string\n",
    "            d.update({'String': string})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for it in first half of string\n",
    "# GPE Countries, cities, states.\n",
    "# LOC Non-GPE locations, mountain ranges, bodies of water.\n",
    "#\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "# loop over every row in the 'Bio' column\n",
    "for text in df['Bio'].tolist():\n",
    "    # use spacy to extract the entities\n",
    "    doc = sp(text)\n",
    "    for ent in doc.ents:    \n",
    "        # check if entity is equal 'LOC' or 'GPE'\n",
    "        if ent.label_ in ['GPE']:\n",
    "            print(ent.text, ent.label_)  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da6e46dab1e0b3d9fa32aec1170dd2df7038a4f7be3a54c97a348d8ad782954"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

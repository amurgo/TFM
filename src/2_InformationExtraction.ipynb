{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction (28th October 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts additional information from the text of the tribunal decisions and stores it in the relevant dictionary.\n",
    "\n",
    "In particular, the notebook performs information extraction on:\n",
    "\n",
    "1. The label included in the name of the file.\n",
    "\n",
    "2. The court where the case was heard (\"Heard at\").\n",
    "\n",
    "3. The judges.\n",
    "\n",
    "4. The legal representation for the appellant and the respondent.\n",
    "\n",
    "5. The decision/ruling by the judge.\n",
    "\n",
    "Each of these fields is added to the dictionary of each judicial decision.\n",
    "\n",
    "The resulting data set - a list of updated dictionaries -  is serialised as a json object (jsonDataFinal.json).\n",
    "\n",
    "This notebook should run in the tfm environment, which can be created with the environment.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment: /Users/albertamurgopacheco/anaconda3/envs/tfm/bin/python\n",
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, getsize\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import whois\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import textract\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import stanza\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# What environment am I using?\n",
    "print(f'Current environment: {sys.executable}')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/albertamurgopacheco/Documents/GitHub/TFM')\n",
    "# What's my working directory?\n",
    "print(f'Current working directory: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directories in colab and local execution\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    docs_path = '/content/gdrive/MyDrive/TFM/data/raw'\n",
    "    input_path = '/content/gdrive/MyDrive/TFM'\n",
    "    output_path = '/content/gdrive/MyDrive/TFM/output'\n",
    "\n",
    "else:\n",
    "    docs_path = './data/raw'\n",
    "    input_path = '.'\n",
    "    output_path = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFORMATION EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The label included in the name of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two categories of cases: the reported and the unreported ones. The reported cases include richer data while the unreported ones (the vast majority of cases) miss several data fields due to a request for annonimity from any of the parties involved in the legal dispute.\n",
    "\n",
    "The first two letters in the file name seem to follow some logic. Inspecting the documents reveals the following meanings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 1588874.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each decision and extract first two characters of the file's name\n",
    "for decision in tqdm(data):\n",
    "    # Only 'unteported' decisions include this 2-letter code\n",
    "    if decision.get('Status of case:') == 'Unreported':\n",
    "        string_code = decision.get('File')[:2]\n",
    "    else:\n",
    "        string_code = 'NA'\n",
    "    \n",
    "    # Add dictionary key 'Code label' with value string to the dictionary\n",
    "    decision.update({'Code label:': string_code})\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The court where the case was heard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inspection of a sample of judicial decisions reveals that the name of the court is located in the first part of the document and it usually follows the expression \"Heard at\".\n",
    "\n",
    "The strategy to capture this field will consist of a search using regular expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:01<00:00, 27089.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    decision_string = decision.get('String')\n",
    "    # Deal with empty/corrupt files that didn't upload a sentence string\n",
    "    if decision_string:\n",
    "        # Regex expression: What comes after \"Heard at\" until hitting 3 balnks or new line\n",
    "        #regex = '(?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        regex = 'Heard at(.*)[\\S\\r\\n]| (?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        catch = re.search(regex, decision_string)\n",
    "\n",
    "        # If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "            # Remove ':' if included in the catch\n",
    "            string = string.replace(':','')\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            # Avoids picking up parts of tables and '|'\n",
    "            string = string.split('   ')\n",
    "            string = string[0]\n",
    "            # Remove 'Heard at' if included in the catch\n",
    "            string = string.replace('Heard at ','')\n",
    "            # Remove 'manually' some strings often included in the catch\n",
    "            string = string.replace('|Decision & Reasons Promulgated','')\n",
    "            string = string.replace('|Decision and Reasons Promulgated','')\n",
    "            string = string.replace('| Decision & Reasons Promulgated','')\n",
    "            string = string.replace('Decision Promulgated','')\n",
    "            string = string.replace('|Decision & Reasons promulgated','')\n",
    "            string = string.replace('|Determination Promulgated','')\n",
    "            string = string.replace('Decision and Reasons Promulgated','')\n",
    "            string = string.replace('|Decision & Reasons  Promulgated','')\n",
    "            string = string.replace(' on 4 July 2003','')\n",
    "            string = string.replace('Determination Promulgated','')\n",
    "            string = string.replace('Decision & Reasons Promulgated','')\n",
    "            string = string.replace('|Decisions and Reasons Promulgated','')\n",
    "            string = string.replace('|Decision and Reasons','')\n",
    "            string = string.replace('UT(IAC)','')\n",
    "            string = string.replace('UT (IAC) ','')\n",
    "            string = string.replace('Date of Hearing  9 December 2005','')\n",
    "            string = string.replace(' | |SS (Risk-Manastry) Iran CG [2003] UKIAT 00035 |','')\n",
    "            # Strip of often found trailing characters\n",
    "            string = string.rstrip(',')\n",
    "            string = string.rstrip('|')\n",
    "            # Remove leading and trailing spaces (again)\n",
    "            string = string.strip()\n",
    "            \n",
    "        else:\n",
    "            string = 'NA'\n",
    "        \n",
    "        #print(string)\n",
    "        # Add dictionary key 'Heard at' with value string to the dictionary\n",
    "        decision.update({'Heard at:': string})\n",
    "    else:\n",
    "        continue\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The judges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:01<00:00, 24965.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    decision_string = decision.get('String')\n",
    "    # Deal with empty/corrupt files that didn't upload a sentence string\n",
    "    if decision_string:\n",
    "        # Regex expression: What comes in between 'Before' and 'Between'\n",
    "        regex = '(?<=Before)([\\s\\S]*?)(?=Between)'\n",
    "        catch = re.search(regex, decision_string)\n",
    "        #If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "\n",
    "            # Get rid of some table delimiters\n",
    "            string = string.replace('|','')\n",
    "            string = string.replace('?','')\n",
    "            string = string.replace(',','')\n",
    "\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            \n",
    "            # Split strings (spaces > 3 usually indicates two \"joint\" names)\n",
    "            # Alternative approach tried and discarded base on sentence tokenization \n",
    "            # from nltk.tokenize import sent_tokenize\n",
    "            listNames = string.split(\"   \")\n",
    "            # Make list of names with strings containijng names\n",
    "            # Capitalize the first letter of each word & delete \n",
    "            listNames = [name.strip().title() for name in listNames if name.strip()]\n",
    "\n",
    "            # Discard content in brackets as it's mostly titles and clutter\n",
    "            listNames = [re.sub('[\\(\\[].*?[\\)\\]]', '', x).strip() for x in listNames]\n",
    "\n",
    "\n",
    "            # Finally, delete titles, positions held and other clutter around the name\n",
    "            clutter = ['Judge', 'Tribunal', 'Court', 'Upper', 'Deputy', 'Senior', 'Of', 'The', 'Mr', 'Dr', 'Vice', 'President',\n",
    "            ':', 'Honourable', 'Hon.', '', '- - - - - - - - - - - - - - - - - - - -', 'Ut', 'Trinbunal', '-And-', 'Mrs', 'President,',\n",
    "            'Tribnunal', '-', 'Hon', 'And', 'Chairman', 'Vice-President', 'Immigration', 'Asylum Chamber', '-Vice', '(Senior',\n",
    "            '...............', 'Designated', 'His Honour', 'Respondent Representation: For Appellant', 'Secretary State For Home Department',\n",
    "            'Appellant', 'Lord', 'Sir', 'In Matter An Application For Judicial Review', 'I) Eu Regulation Number 604/2013 Human',\n",
    "            'Miss', 'Ms.', ':-']\n",
    "\n",
    "            # \n",
    "            listNames = [' '.join(filter(lambda x: x not in clutter,  name.split())) for name in listNames]\n",
    "            # Remove remaining 'issues' with empty strings ''\n",
    "            listNames = list(filter(None, listNames))\n",
    "            # Add a . following individual letters\n",
    "\n",
    "            #print(listNames)\n",
    "            \n",
    "        else:\n",
    "            listNames = ['NA']\n",
    "        \n",
    "        #print(decision.get('File'))\n",
    "        #print(listNames)\n",
    "        # Add dictionary key 'Judges:' with value list of strings to the dictionary\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 2771630.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# 'Manually' fix some mistakes\n",
    "        \n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "\n",
    "    if decision.get('File') == '00046_ukut_iac_2020_ps_iran_cg':\n",
    "        listNames = ['J Barnes', 'A R Mackey', 'S L Batiste']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00393_ukut_iac_2019__jw_ors_ijr':\n",
    "        listNames = ['Rimington Jackson']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '2004_ukiat_00248_gh_iraq_cg':\n",
    "        listNames = ['Rintoul', 'Bruce']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00270_ukut_iac_2015_mmw_ijr':\n",
    "        listNames = ['Justice Mccloskey']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00271_ukut_iac_2015_bh_ijr':\n",
    "        listNames = ['Justice Mccloskey', \"O'Connor\"]\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == 'AA082212015':\n",
    "        listNames = ['Alis', 'I K']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The legal representation for the appellant and the respondent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The legal team consists of the representation for the appellant and the respondent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [5:05:36<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "representation = []\n",
    "files = []\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    decision_string = decision.get('String')\n",
    "    file_name = decision.get('File')\n",
    "    files.append(file_name)\n",
    "    #print(file_name)\n",
    "    # Use only first third of text\n",
    "    string = decision_string[:len(decision_string)//3]\n",
    "    # All text in lower\n",
    "    string = string.lower()\n",
    "    # Apply stanford nlp to string\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # List to store the ruling sentences\n",
    "    catch = []\n",
    "\n",
    "    # Make sentences\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sente = [token.text for token in sentence.tokens]\n",
    "        # Keep only the alpha tokens\n",
    "        sente = [e for e in sente if e.isalpha()]\n",
    "        catch.append(sente)\n",
    "        #print(catch)\n",
    "    \n",
    "    # Look for partial hits (representation_leads_part) in string \n",
    "    representation_leads_part = [['representation', 'for', 'the', 'appellant'], ['representation', 'for', 'the', 'claimant'],\n",
    "    ['for', 'the', 'appellant'], ['representation', 'for', 'the', 'appellants'], ['for', 'the', 'first', 'appellant']]\n",
    "    \n",
    "    # Representation has not been found yet (flag = 0)\n",
    "    flag = 0\n",
    "\n",
    "    for element in catch:\n",
    "        for part in representation_leads_part:\n",
    "            # find index of part hit\n",
    "            idx_part = representation_leads_part.index(part)\n",
    "            # Condition flag == 0 to avoid greedy behaviour (several matches) Only matters 1st hit\n",
    "            if sublist(representation_leads_part[idx_part], element) and flag == 0:\n",
    "                index = catch.index(element)\n",
    "                # representaion lead found in catch\n",
    "                flag = 1\n",
    "                # Keep only sentence with the hit (it includes all needed info)\n",
    "                new_catch = catch[index]\n",
    "                representation.append(new_catch)\n",
    "                decision.update({'Representation:': new_catch})\n",
    "                #print(new_catch)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    # If information on the representation has not been found (flag = 0)\n",
    "    if flag == 0:\n",
    "        #print(f'Did not find a nationality {file_name} in catch: {catch}')\n",
    "        representation.append(np.nan)\n",
    "        decision.update({'Representation:': np.nan})\n",
    "        #print('Did not find a representation')\n",
    "        #print(catch)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information on the legal representatives has been captured for a large number of decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File                 0\n",
       "Representation    3372\n",
       "dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_representation = {'File':files,'Representation':representation}\n",
    "\n",
    "df = pd.DataFrame(dict_representation, columns=['File','Representation'])\n",
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text dic includes a relatively long string with the information on the legal representatives. The following code breaks it down into two pieces:\n",
    "- The legal representation of the appellant (legalAppellant).\n",
    "- The legal representation of the defendant (legalDefendant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with empty/corrupt files that didn't upload a sentence string\n",
    "    # Regex expression: What comes in between 'Before' and 'Between'\n",
    "    # regex = '(?<=Before)([\\s\\S]*?)(?=Between)'\n",
    "    regex ='representation:([\\S\\s]*)for the respondent'\n",
    "    catch = re.search(regex, decision_string.lower())\n",
    "    #If the catch is successful\n",
    "    if catch :\n",
    "        string = catch.group(0)\n",
    "        delimiters = ['|', '?', ':']\n",
    "        # Get rid of some table delimiters\n",
    "        for i in delimiters:\n",
    "            string = string.replace(i,'')\n",
    "\n",
    "        # Remove leading and trailing spaces\n",
    "        string = string.strip()\n",
    "\n",
    "        print(string)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Path to the txt documents\n",
    "txt_path = './data/processed/txt_files_test/'\n",
    "print(os.listdir(txt_path))\n",
    "# Loop over each text file and extract Court information\n",
    "for text in os.listdir(txt_path):\n",
    "    print(text)\n",
    "\n",
    "    with open(txt_path + text, 'r') as file:\n",
    "        decision_string = file.read()\n",
    "        # Regex expression: What comes after \"Heard at\" until hitting 3 balnks or new line\n",
    "        #regex = '(?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        #regex = 'Before([\\S\\s]*)Between'\n",
    "        regex = '(?<=Before)([\\s\\S]*?)(?=Between)'\n",
    "\n",
    "        catch = re.search(regex, decision_string)\n",
    "        #If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "\n",
    "            # Keep only alpha numeric\n",
    "            string = string.replace('|','')\n",
    "            #string = re.sub(r'[^A-Za-z0-9 ]+', '', string)\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            print(string)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use regex on sample list\n",
    "l =['00010_ukait_2009_gs_afghanistan_cg.txt', '00003_ukait_2008_aa_others_pakistan.txt', \n",
    "'IA411142014.txt', 'IA417362014___Others.txt', 'PA047742016.txt', 'PA053522017.txt',\n",
    "'IA124652014.txt', 'IA125982015.txt', 'PA085102018.txt']\n",
    "\n",
    "# Use regex on entire list\n",
    "ll = os.listdir(txt_path)\n",
    "print(len(ll))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Loading string with court decision to data\n",
    "for txt_file in  tqdm(os.listdir(txt_path)):\n",
    "    \n",
    "    # Open file and obtain string and file_name\n",
    "    with open(txt_path + txt_file, 'r') as file:\n",
    "        string = file.read()\n",
    "        f_name, f_ext = os.path.splitext(file.name)\n",
    "        head, file_name = os.path.split(f_name)\n",
    "    # Search data list of dictionaries for dict where {\"File\":} = file_name\n",
    "    for d in data:\n",
    "        if d.get('File') == file_name:\n",
    "            # Add dictionary key 'String' with value string\n",
    "            d.update({'String': string})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The decision of the judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision of the judge is the most challenging piece of information to extract from the documents. # First isolate the part of the document most likely to include the decission the second half of the document. Second, get rid of annexes and appendixes. third, # classifying judgments is not the same as classifying cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 20:28:12 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-11-13 20:28:12 INFO: Use device: cpu\n",
      "2021-11-13 20:28:12 INFO: Loading: tokenize\n",
      "2021-11-13 20:28:12 INFO: Done loading processors!\n",
      "100%|██████████| 35305/35305 [2:00:38<00:00,  4.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to capture if all elements exist in a list\n",
    "def all_exist(avalue, bvalue):\n",
    "    \"\"\"\n",
    "    Given a list of sentences/lists all_exist checks whether avalue list exists in bvalue\n",
    "\n",
    "    :avalue: list to search\n",
    "    :bvalue: list to be searched\n",
    "    :return: the list with the match\n",
    "    \"\"\"\n",
    "    return all(any(x in y for y in bvalue) for x in avalue)\n",
    "\n",
    "# nlp sentence tokenizer with Stanford\n",
    "nlp = stanza.Pipeline(lang = 'en', processors = 'tokenize', tokenize_no_ssplit = True)\n",
    "\n",
    "# Store decisions in a list to make a df\n",
    "decisions = []\n",
    "files = []\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the full text of the court decision\n",
    "    string = decision.get('String')\n",
    "    file_name = decision.get('File')\n",
    "    files.append(file_name)\n",
    "\n",
    "    # Use only second half of text (skip references to annxes and appendixes)\n",
    "    string = string[len(string)//2:]\n",
    "\n",
    "    # Discard text following appendix and annexes\n",
    "    string = string.rsplit(\"appendix\", 1)\n",
    "    string = string[0]\n",
    "    string = string.rsplit(\"annex\", 1)\n",
    "    string = string[0]\n",
    "\n",
    "    # Narrow down the search from the end\n",
    "    # Split on last occurrence of \"Signed\"\n",
    "    string = string.rsplit(\"Signed\", 1)\n",
    "    string = string[0].lower()\n",
    "\n",
    "\n",
    "    # Keep a max of 2000 characters\n",
    "    string = string[ min(-2000, len(string)):]\n",
    "\n",
    "    # Get rid of text after the last occurrence of 'anonymity'\n",
    "    string = string.rsplit(\"anonymity\", 1)\n",
    "    string = string[0]\n",
    "\n",
    "    # Apply stanford nlp\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # List to store the ruling sentences\n",
    "    catch = []\n",
    "    # Flag = 1 when decision found\n",
    "    flag = 0\n",
    "        \n",
    "    # Make sentences\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sente = [token.text for token in sentence.tokens]\n",
    "        # Keep only the alpha tokens\n",
    "        sente = [e for e in sente if e.isalpha()]\n",
    "        #print(type(sente))\n",
    "        catch.append(sente)\n",
    "        \n",
    "    # Identify decision leads in sentences\n",
    "    decision_leads = [['notice', 'of', 'decision'], ['decision'], ['decisions'], ['conclusions'], ['conclusion']]\n",
    "        \n",
    "    # When decision lead found, trim catch and update flag value \n",
    "    for lead in decision_leads:\n",
    "        try:\n",
    "            # Find index of decision lead in ruling\n",
    "            index = catch.index(lead)\n",
    "            # Remove sentences before the decision lead sentence\n",
    "            del catch[0:index]\n",
    "            # Flatten the list of lists/sentences\n",
    "            flat_catch = [item for sublist in catch for item in sublist]\n",
    "            # Decision found\n",
    "            flag = 1\n",
    "            # Store decision in decisions list\n",
    "            decisions.append(flat_catch)\n",
    "            decision.update({'Decision:': flat_catch})\n",
    "            #print('Found decision 1')\n",
    "            #print(flat_catch)\n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # If a decision has not been found yet (flag = 0)\n",
    "    if flag == 0:\n",
    "    # Look for partial hits in text \n",
    "        decision_leads_part = [['for', 'the', 'above', 'reasons'], ['for', 'the', 'reasons', 'i', 'have', 'given'], ['general', 'conclusions'],\n",
    "        ['for', 'the', 'reasons', 'set', 'out', 'above'], ['for', 'all', 'of', 'these', 'reasons'], ['decision', 'and', 'directions'], ['conclusions'],\n",
    "        ['notice', 'of', 'decision'], ['decision','the', 'application', 'for', 'judicial', 'review', 'is'], ['there', 'is', 'no', 'material', 'error', 'of', 'law', 'in'],\n",
    "        ['decision', 'the', 'decision', 'of', 'tribunal', 'judge', 'dean', 'promulgated'], ['the', 'decision', 'of', 'the', 'ftt', 'is', 'set', 'aside'],\n",
    "        ['i', 'grant', 'permission', 'to', 'appeal', 'i', 'set', 'aside', 'the', 'decision', 'of', 'the', 'tribunal'], ['i', 'set', 'aside', 'that', 'decision'],\n",
    "        ['the', 'appellant', 'appeal', 'as', 'originally', 'brought', 'to', 'the', 'ftt', 'is', 'dismissed'], ['i', 'do', 'not', 'set', 'aside', 'the', 'decision']]\n",
    "            \n",
    "        for element in catch:\n",
    "            for part in decision_leads_part:\n",
    "                idx_part = decision_leads_part.index(part)\n",
    "                if all_exist(decision_leads_part[idx_part], element):\n",
    "                    index = catch.index(element)\n",
    "                    # Decision found in catch\n",
    "                    flag = 1\n",
    "                    # Remove sentences before the decision lead sentence\n",
    "                    del catch[0:index]\n",
    "                    # Flatten the list of lists/sentences\n",
    "                    flat_catch = [item for sublist in catch for item in sublist]\n",
    "                    #print('Found decision 2')\n",
    "                    #print(flat_catch)\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "        # If a decision has still not been found (flag = 0)\n",
    "        if flag == 0:\n",
    "            decisions.append(np.nan)\n",
    "            decision.update({'Decision:': np.nan})\n",
    "            #print('Did not find a decision')\n",
    "            #print(catch)\n",
    "        else:\n",
    "            # Store decision in decisions list\n",
    "            decisions.append(flat_catch)\n",
    "            decision.update({'Decision:': flat_catch})\n",
    "            continue\n",
    "\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35305\n",
      "35305\n",
      "['decision', 'the', 'determination', 'of', 'the', 'tribunal', 'having', 'been', 'found', 'to', 'contain', 'a', 'material', 'error', 'of', 'law', 'i', 'substitute', 'the', 'following', 'decision', 'the', 'appellant', 'appeal', 'is', 'allowed', 'under', 'the', 'immigration', 'rules']\n",
      "HU166042017\n",
      "['the', 'judge', 'also', 'went', 'on', 'to', 'consider', 'whether', 'or', 'not', 'any', 'exceptional', 'circumstances', 'existed', 'in', 'this', 'particular', 'case', 'the', 'findings', 'and', 'conclusions', 'are', 'comprehensive', 'and', 'when', 'the', 'decision', 'is', 'viewed', 'holistically', 'the', 'judge', 'consideration', 'is', 'entirely', 'sound', 'in', 'light', 'of', 'the', 'above', 'the', 'appellant', 'appeal', 'to', 'the', 'upper', 'tribunal', 'is', 'dismissed', 'and', 'the', 'decision', 'of', 'the', 'tribunal', 'stands', 'anonymity', 'i', 'make', 'no']\n",
      "PA098412018\n",
      "['notice', 'of', 'decision', 'for', 'the', 'above', 'reasons', 'the', 'decision', 'i', 'on', 'the', 'appellant', 'appeal', 'is', 'to', 'allow', 'it', 'on', 'asylum', 'grounds', 'to', 'conclude', 'as', 'found', 'in', 'my', 'previous', 'error', 'of', 'law', 'decision', 'the', 'ftt', 'judge', 'materially', 'erred', 'in', 'law', 'the', 'decision', 'i', 'is', 'to', 'allow', 'the', 'appellant', 'appeal', 'direction', 'regarding', 'anonymity', 'rule', 'of', 'the', 'tribunal', 'procedure', 'upper', 'tribunal', 'rules', 'unless', 'and', 'until', 'a', 'tribunal', 'or', 'court', 'directs', 'otherwise', 'the', 'appellant', 'is', 'granted']\n",
      "File           0\n",
      "Decision    5220\n",
      "dtype: int64\n",
      "35305\n",
      "35305\n"
     ]
    }
   ],
   "source": [
    "dict_decisions = {'File':files,'Decision':decisions}\n",
    "\n",
    "df = pd.DataFrame(dict_decisions, columns=['File','Decision'])\n",
    "df.isna().sum()\n",
    "#print(data[49])\n",
    "print(len(files))\n",
    "print(len(decisions))\n",
    "#print(decisions[32488])\n",
    "#print(files[5000])\n",
    "#rint(decisions[5000])\n",
    "#print(files[6000])\n",
    "print(decisions[6002])\n",
    "#print(df[df['Decision'].isnull()])\n",
    "print(df.isnull().sum(axis = 0))\n",
    "print(len(files))\n",
    "print(len(decisions))\n",
    "#print(df[df['Decision'].isnull()])\n",
    "#print(json.dumps(data[32554], indent = 4, sort_keys = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Sense of the decision.\n",
    "\n",
    "The decision has been isolated. However, no information on whether the sentence accepts/rejects or is neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense of decision depends on the appellent. If appellent is home office, then... The decision of the First-tier Tribunal did not involve the  making  of an error of law and I uphold it\n",
    "# is accepted, otherwise is rejected.\n",
    "\n",
    "# Rejected\n",
    "The appeal is dismissed\n",
    "The decision of the First-tier Tribunal stands\n",
    "not involve an error on \n",
    "not satisfied that  the  judge  erred \n",
    "decision stands\n",
    "did  not  involve  the making of a material error on a point of law\n",
    "I do not set aside the decision but order that it shall stand\n",
    "appeal is dismissed\n",
    "\n",
    "# Accepted\n",
    "The First-tier Tribunal erred in law\n",
    "I have remade the decision\n",
    "is set aside\n",
    "The appeal, as brought by the appellant to the First-tier  Tribunal,  is allowed.\n",
    "the appeal is remade and I allow the appeal\n",
    "Appeals allowed\n",
    "It  is  set  aside\n",
    "I allow the claimant's appeal\n",
    "The decision of the First-tier Tribunal has already  been  set  aside\n",
    "The original decision shall stand\n",
    "set aside the decision\n",
    "is set aside\n",
    "\n",
    "decision allowing the appeal on humanitarian protection grounds, as well as on human rights grounds\n",
    "\n",
    "# Neutral\n",
    "else\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Nationality of the appellant. \n",
    "The field country is empty to a large extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture if all elements exist in a specific order in a list\n",
    "def sublist(sublist, lst):\n",
    "    \"\"\"\n",
    "    Given a list of sentences/lists all_exist checks whether avalue list exists in bvalue\n",
    "\n",
    "    :sublist: list to search\n",
    "    :lst: list to be searched\n",
    "    :return: the list with the match\n",
    "    \"\"\"\n",
    "    if not isinstance(sublist, list):\n",
    "        raise ValueError(\"sublist must be a list\")\n",
    "    if not isinstance(lst, list):\n",
    "        raise ValueError(\"lst must be a list\")\n",
    "\n",
    "    sublist_len = len(sublist)\n",
    "    k=0\n",
    "    s=None\n",
    "\n",
    "    if (sublist_len > len(lst)):\n",
    "        return False\n",
    "    elif (sublist_len == 0):\n",
    "        return True\n",
    "\n",
    "    for x in lst:\n",
    "        if x == sublist[k]:\n",
    "            if (k == 0): s = x\n",
    "            elif (x != s): s = None\n",
    "            k += 1\n",
    "            if k == sublist_len:\n",
    "                return True\n",
    "        elif k > 0 and sublist[k-1] != s:\n",
    "            k = 0\n",
    "\n",
    "    return False\n",
    "\n",
    "countries = ['Afghanistan', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia', 'Bolivia, Plurinational State of', 'Bonaire', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Congo', 'Cook Islands', 'Costa Rica', \"Côte d'Ivoire\", 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', \"Korea, Democratic People's Republic of\", 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', 'Micronesia', 'Moldova, Republic of', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Palestine', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russian Federation', 'Russia', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syria', 'Syrian Arab Republic', 'Taiwan', 'Taiwan, Province of China', 'Tajikistan', 'Tanzania', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United States', 'United States Minor Outlying Islands', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Venezuela, Bolivarian Republic of', 'Vietnam', 'Viet Nam', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Wallis and Futuna', 'Yemen', 'Zambia', 'Zimbabwe', 'Pakistani', 'Iranian', 'Bangladeshi', 'Indian', 'Egyptian', 'Afghan', 'Albanian', 'Algerian', 'American', 'Andorran', 'Angolan', 'Antiguans', 'Argentinean', 'Armenian', 'Australian', 'Austrian', 'Azerbaijani', 'Bahamian', 'Bahraini', 'Bangladeshi', 'Barbadian', 'Barbudans', 'Batswana', 'Belarusian', 'Belgian', 'Belizean', 'Beninese', 'Bhutanese', 'Bolivian', 'Bosnian', 'Brazilian', 'Bruneian', 'Bulgarian', 'Burkinabe', 'Burmese', 'Burundian', 'Cambodian', 'Cameroonian', 'Canadian', 'Cape Verdean', 'Central African', 'Chadian', 'Chilean', 'Chinese', 'Colombian', 'Comoran', 'Congolese', 'Costa Rican', 'Croatian', 'Cuban', 'Cypriot', 'Czech', 'Danish', 'Djibouti', 'Dominican', 'Dutch', 'Dutchman', 'Dutchwoman', 'East Timorese', 'Ecuadorean', 'Egyptian', 'Emirian', 'Equatorial Guinean', 'Eritrean', 'Estonian', 'Ethiopian', 'Fijian', 'Filipino', 'Finnish', 'French', 'Gabonese', 'Gambian', 'Georgian', 'German', 'Ghanaian', 'Greek', 'Grenadian', 'Guatemalan', 'Guinea-Bissauan', 'Guinean', 'Guyanese', 'Haitian', 'Herzegovinian', 'Honduran', 'Hungarian', 'I-Kiribati', 'Icelander', 'Indian', 'Indonesian', 'Iranian', 'Iraqi', 'Irish', 'Israeli', 'Italian', 'Ivorian', 'Jamaican', 'Japanese', 'Jordanian', 'Kazakhstani', 'Kenyan', 'Kittian and Nevisian', 'Kuwaiti', 'Kyrgyz', 'Laotian', 'Latvian', 'Lebanese', 'Liberian', 'Libyan', 'Liechtensteiner', 'Lithuanian', 'Luxembourger', 'Macedonian', 'Malagasy', 'Malawian', 'Malaysian', 'Maldivan', 'Malian', 'Maltese', 'Marshallese', 'Mauritanian', 'Mauritian', 'Mexican', 'Micronesian', 'Moldovan', 'Monacan', 'Mongolian', 'Moroccan', 'Mosotho', 'Motswana', 'Mozambican', 'Namibian', 'Nauruan', 'Nepalese', 'Netherlander', 'New Zealander', 'Ni-Vanuatu', 'Nicaraguan', 'Nigerian', 'Nigerien', 'North Korean', 'Northern Irish', 'Norwegian', 'Omani', 'Pakistani', 'Palauan', 'Panamanian', 'Papua New Guinean', 'Paraguayan', 'Peruvian', 'Polish', 'Portuguese', 'Qatari', 'Romanian', 'Russian', 'Rwandan', 'Saint Lucian', 'Salvadoran', 'Samoan', 'San Marinese', 'Sao Tomean', 'Saudi', 'Scottish', 'Senegalese', 'Serbian', 'Seychellois', 'Sierra Leonean', 'Singaporean', 'Slovakian', 'Slovenian', 'Solomon Islander', 'Somali', 'South African', 'South Korean', 'Spanish', 'Sri Lankan', 'Sudanese', 'Surinamer', 'Swazi', 'Swedish', 'Swiss', 'Syrian', 'Taiwanese', 'Tajik', 'Tanzanian', 'Thai', 'Togolese', 'Tongan', 'Trinidadian or Tobagonian', 'Tunisian', 'Turkish', 'Tuvaluan', 'Ugandan', 'Ukrainian', 'Uruguayan', 'Uzbekistani', 'Venezuelan', 'Vietnamese', 'Welsh', 'Yemenite', 'Zambian', 'Zimbabwean']\n",
    "countriesLower = [x.lower() for x in countries]\n",
    "#print(countriesLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 18:00:58 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-11-14 18:00:58 INFO: Use device: cpu\n",
      "2021-11-14 18:00:58 INFO: Loading: tokenize\n",
      "2021-11-14 18:00:58 INFO: Done loading processors!\n",
      "100%|██████████| 35305/35305 [5:04:39<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Store decisions in a list to make a df\n",
    "nationalities = []\n",
    "files = []\n",
    "\n",
    "# nlp sentence tokenizer with Stanford\n",
    "nlp = stanza.Pipeline(lang = 'en', processors = 'tokenize', tokenize_no_ssplit = True)\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the full text of the court decision\n",
    "    string = decision.get('String')\n",
    "    file_name = decision.get('File')\n",
    "    files.append(file_name)\n",
    "\n",
    "    # Use only first third of text\n",
    "    string = string[:len(string)//3]\n",
    "    # All text in lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Apply stanford nlp\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # List to store the ruling sentences\n",
    "    catch = []\n",
    "\n",
    "    # Make sentences\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sente = [token.text for token in sentence.tokens]\n",
    "        # Keep only the alpha tokens\n",
    "        sente = [e for e in sente if e.isalpha()]\n",
    "        #print(type(sente))\n",
    "        catch.append(sente)\n",
    "            \n",
    "    # If a nationality has not been found yet (flag = 0)\n",
    "    # Look for partial hits in text \n",
    "    nationality_leads_part = [['the', 'appellant', 'is', 'a', 'national', 'of'], ['the', 'appellant', 'is', 'a', 'citizen', 'of'],\n",
    "    ['the', 'respondent', 'is', 'a', 'citizen', 'of'], ['the', 'appellants', 'are', 'all', 'citizens', 'of'], ['citizen', 'of'],\n",
    "    ['national', 'of'], ['citizens', 'of']]\n",
    "    \n",
    "    # Flag = 1 when nationality is found in catch\n",
    "    flag = 0\n",
    "\n",
    "    for element in catch:\n",
    "        for part in nationality_leads_part:\n",
    "            idx_part = nationality_leads_part.index(part)\n",
    "            if sublist(nationality_leads_part[idx_part], element):\n",
    "                #print(nationality_leads_part[idx_part])\n",
    "                index = catch.index(element)\n",
    "                # Nationality lead found in catch\n",
    "                #flag = 1\n",
    "                # Remove sentences before the decision lead sentence\n",
    "                new_catch = catch[index]\n",
    "                # flag2 = 1 when nationality is found in 1\n",
    "                for token in new_catch:\n",
    "                    #indx_country = countriesLower.index(country)\n",
    "                    if sublist([token], countriesLower):\n",
    "                        #print(f'FOUND A NATIONALITY {token} in {file_name}')\n",
    "                        flag = 1\n",
    "                        nationalities.append(token)\n",
    "                        decision.update({'Nationality:': token})\n",
    "                        break\n",
    "                    else:\n",
    "                        \n",
    "                        continue\n",
    "                    break\n",
    "\n",
    "            if flag == 1:\n",
    "                break\n",
    "                    #print(f'Did not find a nationality in {catch}')\n",
    "\n",
    "                        #indx = catch.index(country)\n",
    "                        #print(countriesLower[indx])\n",
    "                # Flatten the list of lists/sentences\n",
    "                #catch = [item for sublist in catch for item in sublist]\n",
    "                #catch = ' '.join(catch)\n",
    "                #print('Found nationality 2')\n",
    "                #print(flat_catch)\n",
    "                #print(catch)\n",
    "                # Load the model\n",
    "                #nlp = spacy.load(\"en_core_web_sm\")\n",
    "                #catch_spacy = nlp(catch)\n",
    "                #for ent in catch_spacy.ents:    \n",
    "                    # check if entity is equal 'LOC' or 'GPE'\n",
    "                #    if ent.label_ in ['GPE']:\n",
    "                #        print(ent.text, ent.label_)  \n",
    "                #break\n",
    "            #else:\n",
    "            #    continue\n",
    "        if flag == 1:\n",
    "            break   \n",
    "    # If a decision has still not been found (flag = 0)\n",
    "    if flag == 0:\n",
    "        #print(f'Did not find a nationality {file_name} in catch: {catch}')\n",
    "        nationalities.append(np.nan)\n",
    "        decision.update({'Nationality:': np.nan})\n",
    "        #print('Did not find a decision')\n",
    "        #print(catch)\n",
    "    else:\n",
    "            # Store decision in decisions list\n",
    "            #nationalities.append(flat_catch)\n",
    "            #decision.update({'Decision:': flat_catch})\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35305\n",
      "35305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "File               0\n",
       "Nationality    12537\n",
       "dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(files))\n",
    "print(len(nationalities))\n",
    "#print(nationalities)\n",
    "dict_nationalities = {'File':files,'Nationality':nationalities}\n",
    "\n",
    "df = pd.DataFrame(dict_nationalities, columns=['File','Nationality'])\n",
    "df.isna().sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for it in first half of string\n",
    "# GPE Countries, cities, states.\n",
    "# LOC Non-GPE locations, mountain ranges, bodies of water.\n",
    "#\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "# loop over every row in the 'Bio' column\n",
    "for text in df['Bio'].tolist():\n",
    "    # use spacy to extract the entities\n",
    "    doc = sp(text)\n",
    "    for ent in doc.ents:    \n",
    "        # check if entity is equal 'LOC' or 'GPE'\n",
    "        if ent.label_ in ['GPE']:\n",
    "            print(ent.text, ent.label_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Key word extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35305\n",
      "35305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "File               0\n",
       "Nationality    12537\n",
       "dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(files))\n",
    "print(len(nationalities))\n",
    "dict_nationalities = {'File':files,'Nationality':nationalities}\n",
    "\n",
    "df = pd.DataFrame(dict_nationalities, columns=['File','Nationality'])\n",
    "df.isna().sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decisions \n",
    "https://docs.microsoft.com/en-us/dotnet/api/system.text.regularexpressions.match?view=net-5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of decisions\n",
    "\n",
    "https://research.iclr.co.uk/blog/blackstone-goes-live\n",
    "\n",
    "\n",
    "# keyword extraction for issues in strategies\n",
    "https://www.airpair.com/nlp/keyword-extraction-tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision extraction\n",
    "\n",
    "f = open('./data/processed/txt_files/00003_ukait_2008_aa_others_pakistan.txt', \"r\")\n",
    "for number, paragraph in enumerate(f.read().split(\"\\n\\n\"), 1):\n",
    "    print(number)\n",
    "    print(paragraph)\n",
    "    pattern = \"Decisions\"\n",
    "    if paragraph.find(pattern) != -1:\n",
    "        print(\"save to file\") \n",
    "    else:\n",
    "        print(\"don't save to file\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"en_blackstone_proto\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da6e46dab1e0b3d9fa32aec1170dd2df7038a4f7be3a54c97a348d8ad782954"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction (28th October 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts additional information from the text of the tribunal decisions and stores it in the relevant dictionary.\n",
    "\n",
    "In particular, the notebook performs information extraction on:\n",
    "\n",
    "1. The label included in the name of the file.\n",
    "\n",
    "2. The court where the case was heard (\"Heard at\").\n",
    "\n",
    "3. The judges.\n",
    "\n",
    "4. The legal representation for the appellant and the respondent.\n",
    "\n",
    "5. The decision/ruling by the judge.\n",
    "\n",
    "Each of these fields is added to the dictionary of each judicial decision.\n",
    "\n",
    "The resulting data set - a list of updated dictionaries -  is serialised as a json object (jsonDataFinal.json).\n",
    "\n",
    "This notebook should run in the tfm environment, which can be created with the environment.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment: /Users/albertamurgopacheco/anaconda3/envs/tfm/bin/python\n",
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, getsize\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import whois\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import textract\n",
    "import re\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# What environment am I using?\n",
    "print(f'Current environment: {sys.executable}')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/albertamurgopacheco/Documents/GitHub/TFM')\n",
    "# What's my working directory?\n",
    "print(f'Current working directory: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directories in colab and local execution\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    docs_path = '/content/gdrive/MyDrive/TFM/data/raw'\n",
    "    input_path = '/content/gdrive/MyDrive/TFM'\n",
    "    output_path = '/content/gdrive/MyDrive/TFM/output'\n",
    "\n",
    "else:\n",
    "    docs_path = './data/raw'\n",
    "    input_path = '.'\n",
    "    output_path = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFORMATION EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The label included in the name of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two categories of cases: the reported and the unreported ones. The reported cases include richer data while the unreported ones (the vast majority of cases) miss several data fields due to a request for annonimity from any of the parties involved in the legal dispute.\n",
    "\n",
    "The first two letters in the file name seem to follow some logic. Inspecting the documents reveals the following meanings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 1588874.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each decision and extract first two characters of the file's name\n",
    "for decision in tqdm(data):\n",
    "    # Only 'unteported' decisions include this 2-letter code\n",
    "    if decision.get('Status of case:') == 'Unreported':\n",
    "        string_code = decision.get('File')[:2]\n",
    "    else:\n",
    "        string_code = 'NA'\n",
    "    \n",
    "    # Add dictionary key 'Code label' with value string to the dictionary\n",
    "    decision.update({'Code label:': string_code})\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The court where the case was heard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inspection of a sample of judicial decisions reveals that the name of the court is located in the first part of the document and it usually follows the expression \"Heard at\".\n",
    "\n",
    "The strategy to capture this field will consist of a search using regular expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:01<00:00, 27089.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    decision_string = decision.get('String')\n",
    "    # Deal with empty/corrupt files that didn't upload a sentence string\n",
    "    if decision_string:\n",
    "        # Regex expression: What comes after \"Heard at\" until hitting 3 balnks or new line\n",
    "        #regex = '(?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        regex = 'Heard at(.*)[\\S\\r\\n]| (?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        catch = re.search(regex, decision_string)\n",
    "\n",
    "        # If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "            # Remove ':' if included in the catch\n",
    "            string = string.replace(':','')\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            # Avoids picking up parts of tables and '|'\n",
    "            string = string.split('   ')\n",
    "            string = string[0]\n",
    "            # Remove 'Heard at' if included in the catch\n",
    "            string = string.replace('Heard at ','')\n",
    "            # Remove 'manually' some strings often included in the catch\n",
    "            string = string.replace('|Decision & Reasons Promulgated','')\n",
    "            string = string.replace('|Decision and Reasons Promulgated','')\n",
    "            string = string.replace('| Decision & Reasons Promulgated','')\n",
    "            string = string.replace('Decision Promulgated','')\n",
    "            string = string.replace('|Decision & Reasons promulgated','')\n",
    "            string = string.replace('|Determination Promulgated','')\n",
    "            string = string.replace('Decision and Reasons Promulgated','')\n",
    "            string = string.replace('|Decision & Reasons  Promulgated','')\n",
    "            string = string.replace(' on 4 July 2003','')\n",
    "            string = string.replace('Determination Promulgated','')\n",
    "            string = string.replace('Decision & Reasons Promulgated','')\n",
    "            string = string.replace('|Decisions and Reasons Promulgated','')\n",
    "            string = string.replace('|Decision and Reasons','')\n",
    "            string = string.replace('UT(IAC)','')\n",
    "            string = string.replace('UT (IAC) ','')\n",
    "            string = string.replace('Date of Hearing  9 December 2005','')\n",
    "            string = string.replace(' | |SS (Risk-Manastry) Iran CG [2003] UKIAT 00035 |','')\n",
    "            # Strip of often found trailing characters\n",
    "            string = string.rstrip(',')\n",
    "            string = string.rstrip('|')\n",
    "            # Remove leading and trailing spaces (again)\n",
    "            string = string.strip()\n",
    "            \n",
    "        else:\n",
    "            string = 'NA'\n",
    "        \n",
    "        #print(string)\n",
    "        # Add dictionary key 'Heard at' with value string to the dictionary\n",
    "        decision.update({'Heard at:': string})\n",
    "    else:\n",
    "        continue\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The judges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:01<00:00, 24965.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "    # Obtain the text of the court decision\n",
    "    decision_string = decision.get('String')\n",
    "    # Deal with empty/corrupt files that didn't upload a sentence string\n",
    "    if decision_string:\n",
    "        # Regex expression: What comes in between 'Before' and 'Between'\n",
    "        regex = '(?<=Before)([\\s\\S]*?)(?=Between)'\n",
    "        catch = re.search(regex, decision_string)\n",
    "        #If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "\n",
    "            # Get rid of some table delimiters\n",
    "            string = string.replace('|','')\n",
    "            string = string.replace('?','')\n",
    "            string = string.replace(',','')\n",
    "\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            \n",
    "            # Split strings (spaces > 3 usually indicates two \"joint\" names)\n",
    "            # Alternative approach tried and discarded base on sentence tokenization \n",
    "            # from nltk.tokenize import sent_tokenize\n",
    "            listNames = string.split(\"   \")\n",
    "            # Make list of names with strings containijng names\n",
    "            # Capitalize the first letter of each word & delete \n",
    "            listNames = [name.strip().title() for name in listNames if name.strip()]\n",
    "\n",
    "            # Discard content in brackets as it's mostly titles and clutter\n",
    "            listNames = [re.sub('[\\(\\[].*?[\\)\\]]', '', x).strip() for x in listNames]\n",
    "\n",
    "\n",
    "            # Finally, delete titles, positions held and other clutter around the name\n",
    "            clutter = ['Judge', 'Tribunal', 'Court', 'Upper', 'Deputy', 'Senior', 'Of', 'The', 'Mr', 'Dr', 'Vice', 'President',\n",
    "            ':', 'Honourable', 'Hon.', '', '- - - - - - - - - - - - - - - - - - - -', 'Ut', 'Trinbunal', '-And-', 'Mrs', 'President,',\n",
    "            'Tribnunal', '-', 'Hon', 'And', 'Chairman', 'Vice-President', 'Immigration', 'Asylum Chamber', '-Vice', '(Senior',\n",
    "            '...............', 'Designated', 'His Honour', 'Respondent Representation: For Appellant', 'Secretary State For Home Department',\n",
    "            'Appellant', 'Lord', 'Sir', 'In Matter An Application For Judicial Review', 'I) Eu Regulation Number 604/2013 Human',\n",
    "            'Miss', 'Ms.', ':-']\n",
    "\n",
    "            # \n",
    "            listNames = [' '.join(filter(lambda x: x not in clutter,  name.split())) for name in listNames]\n",
    "            # Remove remaining 'issues' with empty strings ''\n",
    "            listNames = list(filter(None, listNames))\n",
    "            # Add a . following individual letters\n",
    "\n",
    "            #print(listNames)\n",
    "            \n",
    "        else:\n",
    "            listNames = ['NA']\n",
    "        \n",
    "        #print(decision.get('File'))\n",
    "        #print(listNames)\n",
    "        # Add dictionary key 'Judges:' with value list of strings to the dictionary\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 2771630.50it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 'Manually' fix some mistakes\n",
    "        \n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data):\n",
    "\n",
    "    if decision.get('File') == '00046_ukut_iac_2020_ps_iran_cg':\n",
    "        listNames = ['J Barnes', 'A R Mackey', 'S L Batiste']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00393_ukut_iac_2019__jw_ors_ijr':\n",
    "        listNames = ['Rimington Jackson']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '2004_ukiat_00248_gh_iraq_cg':\n",
    "        listNames = ['Rintoul', 'Bruce']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00270_ukut_iac_2015_mmw_ijr':\n",
    "        listNames = ['Justice Mccloskey']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == '00271_ukut_iac_2015_bh_ijr':\n",
    "        listNames = ['Justice Mccloskey', \"O'Connor\"]\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "    if decision.get('File') == 'AA082212015':\n",
    "        listNames = ['Alis', 'I K']\n",
    "        decision.update({'Judges:': listNames})\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The legal representation for the appellant and the respondent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The legal team consists of the representation for the appellant and the respondent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the txt documents\n",
    "txt_path = './data/processed/txt_files_test/'\n",
    "print(os.listdir(txt_path))\n",
    "# Loop over each text file and extract Court information\n",
    "for text in os.listdir(txt_path):\n",
    "    print(text)\n",
    "\n",
    "    with open(txt_path + text, 'r') as file:\n",
    "        decision_string = file.read()\n",
    "        # Regex expression: What comes after \"Heard at\" until hitting 3 balnks or new line\n",
    "        #regex = '(?<=Heard at).*[^\\S\\r\\n]{3,}'\n",
    "        #regex = 'Before([\\S\\s]*)Between'\n",
    "        regex = '(?<=Before)([\\s\\S]*?)(?=Between)'\n",
    "\n",
    "        catch = re.search(regex, decision_string)\n",
    "        #If the catch is successful\n",
    "        if catch :\n",
    "            string = catch.group(0)\n",
    "\n",
    "            # Keep only alpha numeric\n",
    "            string = string.replace('|','')\n",
    "            #string = re.sub(r'[^A-Za-z0-9 ]+', '', string)\n",
    "            # Remove leading and trailing spaces\n",
    "            string = string.strip()\n",
    "            print(string)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use regex on sample list\n",
    "l =['00010_ukait_2009_gs_afghanistan_cg.txt', '00003_ukait_2008_aa_others_pakistan.txt', \n",
    "'IA411142014.txt', 'IA417362014___Others.txt', 'PA047742016.txt', 'PA053522017.txt',\n",
    "'IA124652014.txt', 'IA125982015.txt', 'PA085102018.txt']\n",
    "\n",
    "# Use regex on entire list\n",
    "ll = os.listdir(txt_path)\n",
    "print(len(ll))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Loading string with court decision to data\n",
    "for txt_file in  tqdm(os.listdir(txt_path)):\n",
    "    \n",
    "    # Open file and obtain string and file_name\n",
    "    with open(txt_path + txt_file, 'r') as file:\n",
    "        string = file.read()\n",
    "        f_name, f_ext = os.path.splitext(file.name)\n",
    "        head, file_name = os.path.split(f_name)\n",
    "    # Search data list of dictionaries for dict where {\"File\":} = file_name\n",
    "    for d in data:\n",
    "        if d.get('File') == file_name:\n",
    "            # Add dictionary key 'String' with value string\n",
    "            d.update({'String': string})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')                                                                                                                  \n",
    "sents = nlp('Dr H H Storey  (Senior Immigration Judge)') \n",
    "type(sents)\n",
    "\n",
    "for ent in sents.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Install package\n",
    "#python3.8 -m pip install stanza \n",
    "import stanza\n",
    "\n",
    "#stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n",
    "doc = nlp(\"DEPUTY UPPER TRIBUNAL JUDGE M A. HALL\") # run annotation over a sentence\n",
    "\n",
    "print(doc)\n",
    "print(doc.entities)\n",
    "\n",
    "import blackstone\n",
    "\n",
    "\n",
    "\n",
    "#string= 'Dr H H Storey  (Senior Immigration Judge)                         Mr I F Macdonald (Immigration Judge) '\n",
    "#string = 'UPPER TRIBUNAL JUDGE GRUBB (bullshit)'\n",
    "\n",
    "string = '                       Dr H H Storey (Vice President)                                 Mr P Bompas                                Mr S S Percy'\n",
    "\n",
    "\n",
    "# Split strings (spaces > 3 usually indicates two \"joint\" names)\n",
    "# Alternative approach tried and discarded base on sentence tokenization \n",
    "# from nltk.tokenize import sent_tokenize\n",
    "listNames = string.split(\"   \")\n",
    "# Make list of names with strings containijng names\n",
    "# Capitalize the first letter of each word & delete \n",
    "listNames = [name.strip().title() for name in listNames if name.strip()]\n",
    "\n",
    "# Discard content in brackets as it's mostly titles and clutter\n",
    "listNames = [re.sub('[\\(\\[].*?[\\)\\]]', '', x).strip() for x in listNames]\n",
    "\n",
    "\n",
    "# Finally, delete titles, positions held and other clutter around the name\n",
    "clutter = ['Judge', 'Tribunal', 'Court', 'Upper', 'Deputy', 'Senior', 'Of', 'The', 'Mr', 'Dr', 'Vice', 'President']\n",
    "\n",
    "# \n",
    "listNames = [' '.join(filter(lambda x: x not in clutter,  name.split())) for name in listNames]\n",
    "\n",
    "# Add a . following individual letters\n",
    "\n",
    "print(listNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The decision of the judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision of the judge is the most challenging piece of information to extract from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 13:41:48 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-11-13 13:41:48 INFO: Use device: cpu\n",
      "2021-11-13 13:41:48 INFO: Loading: tokenize\n",
      "2021-11-13 13:41:48 INFO: Done loading processors!\n",
      "100%|██████████| 35256/35256 [6:12:49<00:00,  1.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "File           0\n",
       "Decision    5217\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First isolate the part of the document most likely to include the decission\n",
    "\n",
    "# classifying judgments is not the same as classifying cases.\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import stanza\n",
    "\n",
    "def all_exist(avalue, bvalue):\n",
    "    return all(any(x in y for y in bvalue) for x in avalue)\n",
    "\n",
    "\n",
    "nlp = stanza.Pipeline(lang = 'en', processors = 'tokenize', tokenize_no_ssplit = True)\n",
    "\n",
    "# Path to the txt documents\n",
    "#txt_path = './data/processed/txt_files/'\n",
    "#print(os.listdir(txt_path))\n",
    "\n",
    "decisions = []\n",
    "files = []\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over each text file and extract Court information\n",
    "for decision in tqdm(data[49:]):\n",
    "    # Obtain the full text of the court decision\n",
    "    string = decision.get('String')\n",
    "    file_name = decision.get('File')\n",
    "    files.append(file_name)\n",
    "\n",
    "    # Use only second half of text (skip references to annxes and appendixes)\n",
    "    string = string[len(string)//2:]\n",
    "\n",
    "    # Discard text following appendix and annexes\n",
    "    string = string.rsplit(\"appendix\", 1)\n",
    "    string = string[0]\n",
    "    string = string.rsplit(\"annex\", 1)\n",
    "    string = string[0]\n",
    "\n",
    "    # Narrow down from the end\n",
    "    # Split on last occurrence of \"Signed\"\n",
    "    string = string.rsplit(\"Signed\", 1)\n",
    "    string = string[0].lower()\n",
    "\n",
    "\n",
    "    # Keep a max of 2000 characters\n",
    "    string = string[ min(-2000, len(string)):]\n",
    "\n",
    "    # Get rid of text after the last occurrence of 'anonymity'\n",
    "    string = string.rsplit(\"anonymity\", 1)\n",
    "    string = string[0]\n",
    "\n",
    "    # Apply stanford nlp\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # List to store the ruling sentences\n",
    "    catch = []\n",
    "    # Flag = 1 when decision found\n",
    "    flag = 0\n",
    "        \n",
    "    # Make sentences\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sente = [token.text for token in sentence.tokens]\n",
    "        # Keep only the alpha tokens\n",
    "        sente = [e for e in sente if e.isalpha()]\n",
    "        #print(type(sente))\n",
    "        catch.append(sente)\n",
    "        \n",
    "    # Identify decision leads in sentences\n",
    "    decision_leads = [['notice', 'of', 'decision'], ['decision'], ['decisions'], ['conclusions']]\n",
    "        \n",
    "    # When decision lead found, trim catch and update flag value \n",
    "    for lead in decision_leads:\n",
    "        try:\n",
    "            # Find index of decision lead in ruling\n",
    "            index = catch.index(lead)\n",
    "            # Remove sentences before the decision lead sentence\n",
    "            del catch[0:index]\n",
    "            # Flatten the list of lists/sentences\n",
    "            flat_catch = [item for sublist in catch for item in sublist]\n",
    "            # Decision found\n",
    "            flag = 1\n",
    "            # Store decision in decisions list\n",
    "            decisions.append(flat_catch)\n",
    "            decision.update({'Decision:': flat_catch})\n",
    "\n",
    "            #print('Found decision 1')\n",
    "            #print(flat_catch)\n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # If a decision has not been found yet (flag = 0)\n",
    "    if flag == 0:\n",
    "    # Look for partial hits in text \n",
    "        decision_leads_part = [['for', 'the', 'above', 'reasons'], ['for', 'the', 'reasons', 'i', 'have', 'given'], ['general', 'conclusions'],\n",
    "        ['for', 'the', 'reasons', 'set', 'out', 'above'], ['for', 'all', 'of', 'these', 'reasons'], ['decision', 'and', 'directions'], ['conclusions'],\n",
    "        ['notice', 'of', 'decision'], ['decision','the', 'application', 'for', 'judicial', 'review', 'is'], ['there', 'is', 'no', 'material', 'error', 'of', 'law', 'in'],\n",
    "        ['decision', 'the', 'decision', 'of', 'tribunal', 'judge', 'dean', 'promulgated'], ['the', 'decision', 'of', 'the', 'ftt', 'is', 'set', 'aside'],\n",
    "        ['i', 'grant', 'permission', 'to', 'appeal', 'i', 'set', 'aside', 'the', 'decision', 'of', 'the', 'tribunal'], ['i', 'set', 'aside', 'that', 'decision'],\n",
    "        ['the', 'appellant', 'appeal', 'as', 'originally', 'brought', 'to', 'the', 'ftt', 'is', 'dismissed']]\n",
    "            \n",
    "        for element in catch:\n",
    "            for part in decision_leads_part:\n",
    "                idx_part = decision_leads_part.index(part)\n",
    "                if all_exist(decision_leads_part[idx_part], element):\n",
    "                    index = catch.index(element)\n",
    "                    # Decision found in catch\n",
    "                    flag = 1\n",
    "                    # Remove sentences before the decision lead sentence\n",
    "                    del catch[0:index]\n",
    "                    # Flatten the list of lists/sentences\n",
    "                    flat_catch = [item for sublist in catch for item in sublist]\n",
    "                    #print('Found decision 2')\n",
    "                    #print(flat_catch)\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "        # If a decision has still not been found (flag = 0)\n",
    "        if flag == 0:\n",
    "            decisions.append(np.nan)\n",
    "            decision.update({'Decision:': np.nan})\n",
    "            #print('Did not find a decision')\n",
    "            #print(catch)\n",
    "        else:\n",
    "            # Store decision in decisions list\n",
    "            decisions.append(flat_catch)\n",
    "            decision.update({'Decision:': flat_catch})\n",
    "            continue\n",
    "\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File           0\n",
       "Decision    5217\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_decisions = {'File':files,'Decision':decisions}\n",
    "\n",
    "df = pd.DataFrame(dict_decisions, columns=['File','Decision'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Case title:': '', 'Appellant name:': '', 'Status of case:': 'Unreported', 'Hearing date:': '14 Jul 2021', 'Promulgation date:': '11 Oct 2021', 'Publication date:': '26 Oct 2021', 'Last updated on:': '26 Oct 2021', 'Country:': '', 'Judges:': '', 'Document': 'https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/73729/DC000912019___DC001272019.doc', 'Reference': ['DC/00091/2019 &amp; DC/00127/2019'], 'Download': 'Yes', 'File': 'DC000912019___DC001272019', 'ID': '91fff7ff-1af7-435f-a3fd-00889afc2456', 'Code label:': 'DC'}\n",
      "35256\n",
      "35256\n",
      "['decision', 'the', 'secretary', 'of', 'state', 'appeal', 'to', 'the', 'upper', 'tribunal', 'is', 'dismissed', 'the', 'tribunal', 'decision', 'to', 'allow', 'the', 'appellant', 'appeal', 'under', 'article', 'of', 'the', 'echr', 'and', 'on', 'the', 'basis', 'that', 'his', 'deportation', 'would', 'breach', 'the', 'refugee', 'convention', 'stand']\n",
      "HU122212017\n",
      "nan\n",
      "HU129662018___HU129712018\n",
      "['notice', 'of', 'decision', 'the', 'decision', 'did', 'involve', 'the', 'making', 'of', 'an', 'error', 'of', 'law', 'sufficient', 'to', 'require', 'the', 'decision', 'to', 'be', 'set', 'aside', 'on', 'all', 'grounds', 'and', 'reheard', 'accordingly', 'the', 'appeal', 'is', 'remitted', 'to', 'the', 'first', 'tier', 'tribunal', 'for', 'rehearing', 'de', 'novo', 'with', 'the', 'directions', 'set', 'out', 'above', 'direction', 'regarding', 'anonymity', 'rule', 'of', 'the', 'tribunal', 'procedure', 'upper', 'tribunal', 'rules', 'unless', 'and', 'until', 'a', 'tribunal', 'or', 'court', 'directs', 'otherwise', 'the', 'appellant', 'is', 'granted']\n"
     ]
    }
   ],
   "source": [
    "print(data[48])\n",
    "print(len(files))\n",
    "print(len(decisions))\n",
    "print(decisions[32488])\n",
    "print(files[5000])\n",
    "print(decisions[5000])\n",
    "print(files[6000])\n",
    "print(decisions[6002])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File          0\n",
       "Decision    100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(df[df['Decision'].isnull()])\n",
    "df.isnull().sum(axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "JR004822021\n",
      "nan\n",
      "                         File Decision\n",
      "28                PA116272019      NaN\n",
      "29                HU200422018      NaN\n",
      "31                LP001692020      NaN\n",
      "38  HU154862019___HU154872019      NaN\n",
      "41                HU110842019      NaN\n",
      "46                PA029442020      NaN\n",
      "47                HU115652019      NaN\n",
      "55                HU153562019      NaN\n",
      "56                PA066102019      NaN\n",
      "58  HU116682018___HU206172018      NaN\n",
      "59                IA089812015      NaN\n",
      "67                HU116682018      NaN\n",
      "74                JR050772019      NaN\n",
      "76                HU023242020      NaN\n",
      "81                HU183682019      NaN\n",
      "85                JR040732019      NaN\n",
      "87                JR019472020      NaN\n",
      "93       HU033052019___Others      NaN\n",
      "98                JR004822021      NaN\n"
     ]
    }
   ],
   "source": [
    "print(len(files))\n",
    "print(len(decisions))\n",
    "print(files[98])\n",
    "print(decisions[98])\n",
    "print(df[df['Decision'].isnull()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each text file and extract Court information\n",
    "for text in os.listdir(txt_path):\n",
    "    with open(txt_path + text, 'r') as file:\n",
    "        files.append(text)\n",
    "        #print(text)\n",
    "        decision_string = file.read()\n",
    "        # The strategy is to trim from both ends of the string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n",
      "{\n",
      "    \"Appellant name:\": \"\",\n",
      "    \"Case title:\": \"\",\n",
      "    \"Country:\": \"\",\n",
      "    \"Document\": \"https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/39898/DA000192013.doc\",\n",
      "    \"Download\": \"Yes\",\n",
      "    \"Hearing date:\": \"\",\n",
      "    \"Judges:\": \"\",\n",
      "    \"Last updated on:\": \"4 Dec 2013\",\n",
      "    \"Promulgation date:\": \"23 Oct 2013\",\n",
      "    \"Publication date:\": \"4 Dec 2013\",\n",
      "    \"Reference\": [\n",
      "        \"DA/00019/2013\"\n",
      "    ],\n",
      "    \"Status of case:\": \"Unreported\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(f'Current working directory: {os.getcwd()}')\n",
    "\n",
    "# Open jsonData file\n",
    "jsonData_path = os.path.join(os.getcwd(), 'data/jsonData.json')\n",
    "with open(jsonData_path) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(json.dumps(data[32554], indent = 4, sort_keys = True))\n",
    "\n",
    "#parsed = json.loads(jsonData)\n",
    "#print(json.dumps(parsed[16366], indent = 4, sort_keys = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Sense of the decision.\n",
    "\n",
    "The decision has been isolated. However, no information on whether the sentence accepts/rejects or is neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense of decision depends on the appellent. If appellent is home office, then... The decision of the First-tier Tribunal did not involve the  making  of an error of law and I uphold it\n",
    "# is accepted, otherwise is rejected.\n",
    "\n",
    "# Rejected\n",
    "The appeal is dismissed\n",
    "The decision of the First-tier Tribunal stands\n",
    "not involve an error on \n",
    "not satisfied that  the  judge  erred \n",
    "decision stands\n",
    "did  not  involve  the making of a material error on a point of law\n",
    "I do not set aside the decision but order that it shall stand\n",
    "appeal is dismissed\n",
    "\n",
    "# Accepted\n",
    "The First-tier Tribunal erred in law\n",
    "I have remade the decision\n",
    "is set aside\n",
    "The appeal, as brought by the appellant to the First-tier  Tribunal,  is allowed.\n",
    "the appeal is remade and I allow the appeal\n",
    "Appeals allowed\n",
    "It  is  set  aside\n",
    "I allow the claimant's appeal\n",
    "The decision of the First-tier Tribunal has already  been  set  aside\n",
    "The original decision shall stand\n",
    "set aside the decision\n",
    "is set aside\n",
    "\n",
    "decision allowing the appeal on humanitarian protection grounds, as well as on human rights grounds\n",
    "\n",
    "# Neutral\n",
    "else\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Nationality of the appellant. \n",
    "The field country is empty to a large extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for it in first half of string\n",
    "# GPE Countries, cities, states.\n",
    "# LOC Non-GPE locations, mountain ranges, bodies of water.\n",
    "#\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "# loop over every row in the 'Bio' column\n",
    "for text in df['Bio'].tolist():\n",
    "    # use spacy to extract the entities\n",
    "    doc = sp(text)\n",
    "    for ent in doc.ents:    \n",
    "        # check if entity is equal 'LOC' or 'GPE'\n",
    "        if ent.label_ in ['GPE']:\n",
    "            print(ent.text, ent.label_)  \n",
    "\n",
    "\n",
    "the appellant is a national of\n",
    "the appellant is a citizen of\n",
    "the respondent is a citizen of\n",
    "is a citizen of\n",
    "citizen of\n",
    "is a national of\n",
    "The appellants are all citizens of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Regex for the Appellant\n",
    "For the Appellant: ([\\S\\s]*)For the Respondent\n",
    "\n",
    "# Regex for the Respondent\n",
    "For the Respondent: (.*)\\n\\n \n",
    "# OR\n",
    "For the Respondent: (.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decisions \n",
    "https://docs.microsoft.com/en-us/dotnet/api/system.text.regularexpressions.match?view=net-5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of decisions\n",
    "\n",
    "https://research.iclr.co.uk/blog/blackstone-goes-live\n",
    "\n",
    "\n",
    "# keyword extraction for issues in strategies\n",
    "https://www.airpair.com/nlp/keyword-extraction-tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WAYS IN WHICH DECISIONS ARE INTRODUCED:\n",
    "\n",
    "1.- Throw away everything in ANNEX  or Appendix\n",
    "2.- Keep only last 150 words of document\n",
    "\n",
    "Apply rules with lemmas\n",
    "\n",
    "\n",
    "\n",
    "For all of these reasons,\n",
    "\n",
    "Notice of Decision      Signed\n",
    "Notice of Decision  BETWEEN THESE TWO WILL CATCH A FEW      Direction Regarding Anonymity\n",
    "\n",
    "DECISSION     Dated\n",
    "\n",
    "Decision     Signed\n",
    "\n",
    "For the above reasons:      Signed:\n",
    "\n",
    "For  the  above  reasons  we  conclude \n",
    "\n",
    "We have concluded that,\n",
    "\n",
    "For the above reasons \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision extraction\n",
    "\n",
    "f = open('./data/processed/txt_files/00003_ukait_2008_aa_others_pakistan.txt', \"r\")\n",
    "for number, paragraph in enumerate(f.read().split(\"\\n\\n\"), 1):\n",
    "    print(number)\n",
    "    print(paragraph)\n",
    "    pattern = \"Decisions\"\n",
    "    if paragraph.find(pattern) != -1:\n",
    "        print(\"save to file\") \n",
    "    else:\n",
    "        print(\"don't save to file\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"en_blackstone_proto\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da6e46dab1e0b3d9fa32aec1170dd2df7038a4f7be3a54c97a348d8ad782954"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing scraped data (11th December 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook manages the tribunal decision's data scraped in 0_dataScraping.ipynb\n",
    "\n",
    "In particular, the notebook:\n",
    "\n",
    "1. Stores the text of each tribunal decision in the corresponding object of jsonDataFinal (list of dict) with an uuid.\n",
    "\n",
    "2. Converts the 35305 downloaded word documents to text (from .doc/.docx to .txt)\n",
    "\n",
    "3. Provides some descriptive statistics on the downloaded files.\n",
    "\n",
    "The resulting data set (a list of dictionaries) is serialised as a json object (jsonDataFinal.json).\n",
    "\n",
    "This notebook should run in the tfm environment, which can be created with the environment.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment: /Users/albertamurgopacheco/anaconda3/envs/tfm/bin/python\n",
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, getsize\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import whois\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import textract\n",
    "import wget\n",
    "import uuid\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# What environment am I using?\n",
    "print(f'Current environment: {sys.executable}')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/albertamurgopacheco/Documents/GitHub/TFM')\n",
    "# What's my working directory?\n",
    "print(f'Current working directory: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directories in colab and local execution\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    docs_path = '/content/gdrive/MyDrive/TFM/data/raw'\n",
    "    input_path = '/content/gdrive/MyDrive/TFM'\n",
    "    output_path = '/content/gdrive/MyDrive/TFM/output'\n",
    "\n",
    "else:\n",
    "    docs_path = './data/raw'\n",
    "    input_path = '.'\n",
    "    output_path = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Storing tribunal decisions in jsonDataFinal\n",
    "A string with the tribunal decision's text is added to each dictionary in the jsonData list of dictionaries. The resulting collection is saved as jsonDataFinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to jsonData file\n",
    "jsonData_path = os.path.join(os.getcwd(), 'data/jsonData.json')\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open(jsonData_path) as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 49/35308 [00:46<7:08:56,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download file https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/73729/DC000912019___DC001272019.doc\n",
      "HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 32492/35308 [7:36:44<25:52,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download file https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/40081/IA083642010___IA083692010___IA083752010.DOC\n",
      "HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35308/35308 [8:13:21<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop over dictionaries and upload the string of the judicial decision\n",
    "for d in tqdm(data):\n",
    "    # Obtain the url to the file with the judicial decision\n",
    "    docLink = d.get('Document')\n",
    "    # Temp folder to store word file\n",
    "    docs_temp = './data/temp/'\n",
    "    # List of corrupted urls\n",
    "    corruptFiles = ['HU077022015', 'HU029682017']\n",
    "    \n",
    "    if d.get('File') not in corruptFiles:\n",
    "\n",
    "        try:\n",
    "            # Download file to temp folder\n",
    "            wget.download(url = docLink, out = docs_temp)\n",
    "            # Delete DS_Store files in folder\n",
    "            !find . -name '.DS_Store' -type f -delete\n",
    "            # Get name of downloaded file \n",
    "            filename = os.listdir(docs_temp)\n",
    "            filename = filename[0]\n",
    "            # Extract text from the file as a string\n",
    "            content = textract.process(os.path.join(docs_temp, filename))\n",
    "            # Add content to dict key 'String':\n",
    "            d.update({'String': content})\n",
    "            # Delete the file\n",
    "            os.remove(docs_temp + filename)\n",
    "        \n",
    "        # Handling exceptions \n",
    "        except Exception as err:\n",
    "            print(\"Could not download file {}\".format(docLink))\n",
    "            print(err)\n",
    "            downloaded = \"No\"\n",
    "            pass\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35308/35308 [00:00<00:00, 70497.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Decode in utf-8 the data saved in bytes in 'String'\n",
    "for d in tqdm(data):\n",
    "    string = d.get('String')\n",
    "    # The stored value is in bytes and should be decoded\n",
    "    if not string:\n",
    "        continue\n",
    "    else: \n",
    "        string = string.decode('utf-8')\n",
    "        # Add dictionary key 'String' with value string\n",
    "        d.update({'String': string})\n",
    "        #print(d)\n",
    "        \n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as a json file jsonData in data directory\n",
    "with open('./data/jsonDataFinalBackup.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle\n",
    "with open('./data/pickleDataFinal.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickle file \n",
    "with open('./data/pickleDataFinal.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop entries with corrupt word files. No text is available for these entries. ['HU077022015', 'HU029682017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 35305/35308 [00:00<00:00, 2498817.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict indexed 10481 has been removed from list.\n",
      "Dict indexed 12643 has been removed from list.\n",
      "Dict indexed 32489 has been removed from list.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# List of corrupted urls\n",
    "corruptFiles = ['HU077022015', 'HU029682017', 'IA083642010___IA083692010___IA083752010']\n",
    "\n",
    "# Loop over dictionaries and remove the corrupt files entries\n",
    "for d in tqdm(data):\n",
    "    if d['File'] in corruptFiles:\n",
    "        # Remove elements\n",
    "        idx = data.index(d)\n",
    "        del data[idx]\n",
    "        print(f'Dict indexed {idx} has been removed from list.')\n",
    "    \n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a unique ID to each dict in the jsonDataFinal using uuid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 233505.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loop over dictionaries and remove the corrupt files entries\n",
    "for d in tqdm(data):\n",
    "    # Generate random uuid\n",
    "    id = str(uuid.uuid4())\n",
    "    # Add content to dict key 'String':\n",
    "    d.update({'ID': id})\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Converting word documents to text (from .doc/.docx to .txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 35258 downloaded word documents (.doc/.docx) are converted to text (.txt) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35255/35255 [50:46<00:00, 11.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Delete DS_Store files in raw data folder\n",
    "!find . -name '.DS_Store' -type f -delete\n",
    "\n",
    "# Files HU077022015.doc & HU029682017.docx are corrupt. Manually deleted from data/raw \n",
    "# (textract not dealing with Shell Error exceptions)\n",
    "\n",
    "# Destination directory of txt files\n",
    "dest_files_path = os.path.join(os.getcwd(), 'data/processed/txt_files')\n",
    "\n",
    "# Loop to extract txt from word files (with decorator progress bar)\n",
    "for word_file in  tqdm(os.listdir(docs_path)):\n",
    "\n",
    "    file, extension = os.path.splitext(word_file)\n",
    "    \n",
    "    # Create txt file concatenating .txt extension to file name\n",
    "    dest_file_name = file + '.txt'\n",
    "    \n",
    "    # Extract text from the file\n",
    "    content = textract.process(os.path.join(docs_path, word_file))\n",
    "    \n",
    "    # Create and open new file & prepare to write the Binary Data (represented by wb - Write Binary)\n",
    "    write_text_file = open(os.path.join(dest_files_path, dest_file_name), \"wb\")\n",
    "    \n",
    "    # Write the content and close the newly created file\n",
    "    write_text_file.write(content)\n",
    "    write_text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing string for decision 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/txt_files/DC000912019___DC001272019.txt', 'r') as file:\n",
    "    decision_string = file.read()\n",
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Obtain the full text of the court decision\n",
    "data[48].update({'String': decision_string})\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Descriptive statistics on the downloaded files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides some descriptive statistics on the downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(filename):\n",
    "    st = os.stat(filename)\n",
    "    return st.st_size\n",
    "\n",
    "# Extract name and size of all files in docs_path\n",
    "files_name_list_raw = [f for f in listdir(docs_path) if isfile(join(docs_path, f))]\n",
    "files_size_list_raw = [get_size(join(docs_path, f)) for f in listdir(docs_path) if isfile(join(docs_path, f))]\n",
    "\n",
    "# Obtain/check number of files\n",
    "print(f'Number of files: {len(files_name_list_raw)}')\n",
    "\n",
    "# Unique files based on size file_name\n",
    "print(f'Number of unique file names: {len(set(files_name_list_raw))}')\n",
    "\n",
    "# Unique files based on size file_name\n",
    "print(f'Max file sizes: {max(files_size_list_raw)}')\n",
    "print(f'Min of unique file sizes: {min(files_size_list_raw)}')\n",
    "\n",
    "print(files_size_list_raw[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each text file and extract Court information\n",
    "for text in os.listdir(txt_path):\n",
    "    with open(txt_path + text, 'r') as file:\n",
    "        files.append(text)\n",
    "        #print(text)\n",
    "        decision_string = file.read()\n",
    "        # The strategy is to trim from both ends of the string\n",
    "\n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n",
    "\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "\n",
    "# Open jsonData file\n",
    "jsonData_path = os.path.join(os.getcwd(), 'data/jsonData.json')\n",
    "with open(jsonData_path) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(json.dumps(data[32554], indent = 4, sort_keys = True))\n",
    "\n",
    "#parsed = json.loads(jsonData)\n",
    "#print(json.dumps(parsed[16366], indent = 4, sort_keys = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Number of files\n",
    "\n",
    "do sentence and token count (max min, average, etc.)\n",
    "\n",
    "\n",
    "# Size of files\n",
    "https://realpython.com/working-with-files-in-python/#getting-file-attributes\n",
    "\n",
    "# doc vs docx\n",
    "https://realpython.com/working-with-files-in-python/#filename-pattern-matching\n",
    "\n",
    "# Number of dictionaries with sentence. Delete the rest?\n",
    "\n",
    "# Longest sentence\n",
    "\n",
    "# Shortest sentemce\n",
    "\n",
    "# Number of reported vs unreported cases (use the name of the file to discriminate them)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da6e46dab1e0b3d9fa32aec1170dd2df7038a4f7be3a54c97a348d8ad782954"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

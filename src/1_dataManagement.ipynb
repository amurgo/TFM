{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing scraped data (28th October 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook manages the tribunal decision's data scraped in 0_dataScraping.ipynb\n",
    "\n",
    "In particular, the notebook:\n",
    "\n",
    "1. Stores the text of each tribunal decision in the corresponding object of jsonDataFinal (list of dict).\n",
    "\n",
    "2. Converts the 35258 downloaded word documents to text (from .doc/.docx to .txt)\n",
    "\n",
    "3. Provides some descriptive statistics on the downloaded files.\n",
    "\n",
    "The resulting data set (a list of dictionaries) is serialised as a json object (jsonDataFinal.json).\n",
    "\n",
    "This notebook should run in the tfm environment, which can be created with the environment.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment: /Users/albertamurgopacheco/anaconda3/envs/tfm/bin/python\n",
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, getsize\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import whois\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import textract\n",
    "import wget\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# What environment am I using?\n",
    "print(f'Current environment: {sys.executable}')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/albertamurgopacheco/Documents/GitHub/TFM')\n",
    "# What's my working directory?\n",
    "print(f'Current working directory: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directories in colab and local execution\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    docs_path = '/content/gdrive/MyDrive/TFM/data/raw'\n",
    "    input_path = '/content/gdrive/MyDrive/TFM'\n",
    "    output_path = '/content/gdrive/MyDrive/TFM/output'\n",
    "\n",
    "else:\n",
    "    docs_path = './data/raw'\n",
    "    input_path = '.'\n",
    "    output_path = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Storing tribunal decisions in jsonDataFinal\n",
    "A string with the tribunal decision's text is added to each dictionary in the jsonData list of dictionaries. The resulting collection is saved as jsonDataFinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to jsonData file\n",
    "jsonData_path = os.path.join(os.getcwd(), 'data/jsonData.json')\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open(jsonData_path) as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 32492/35308 [8:33:16<32:53,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download file https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/40081/IA083642010___IA083692010___IA083752010.DOC\n",
      "HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35308/35308 [9:19:55<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop over dictionaries and upload the string of the judicial decision\n",
    "for d in tqdm(data):\n",
    "    # Obtain the url to the file with the judicial decision\n",
    "    docLink = d.get('Document')\n",
    "    # Temp folder to store word file\n",
    "    docs_temp = './data/temp/'\n",
    "    # List of corrupted urls\n",
    "    corruptFiles = ['HU077022015', 'HU029682017']\n",
    "    \n",
    "    if d.get('File') not in corruptFiles:\n",
    "\n",
    "        try:\n",
    "            # Download file to temp folder\n",
    "            wget.download(url = docLink, out = docs_temp)\n",
    "            # Delete DS_Store files in folder\n",
    "            !find . -name '.DS_Store' -type f -delete\n",
    "            # Get name of downloaded file \n",
    "            filename = os.listdir(docs_temp)\n",
    "            filename = filename[0]\n",
    "            # Extract text from the file as a string\n",
    "            content = textract.process(os.path.join(docs_temp, filename))\n",
    "            # Add content to dict key 'String':\n",
    "            d.update({'String': content})\n",
    "            # Delete the file\n",
    "            os.remove(docs_temp + filename)\n",
    "        \n",
    "        # Handling exceptions \n",
    "        except Exception as err:\n",
    "            print(\"Could not download file {}\".format(docLink))\n",
    "            print(err)\n",
    "            downloaded = \"No\"\n",
    "            pass\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35308/35308 [00:01<00:00, 32926.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Decode in utf-8 the data saved in bytes in 'String'\n",
    "for d in tqdm(data):\n",
    "    string = d.get('String')\n",
    "    # The stored value is in bytes and should be decoded\n",
    "    if not string:\n",
    "        continue\n",
    "    else: \n",
    "        string = string.decode('utf-8')\n",
    "        # Add dictionary key 'String' with value string\n",
    "        d.update({'String': string})\n",
    "        #print(d)\n",
    "        \n",
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle\n",
    "with open('./data/pickleDataFinal.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickle file \n",
    "with open('./data/pickleDataFinal.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Converting word documents to text (from .doc/.docx to .txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 35258 downloaded word documents (.doc/.docx) are converted to text (.txt) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35255/35255 [50:46<00:00, 11.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Delete DS_Store files in raw data folder\n",
    "!find . -name '.DS_Store' -type f -delete\n",
    "\n",
    "# Files HU077022015.doc & HU029682017.docx are corrupt. Manually deleted from data/raw \n",
    "# (textract not dealing with Shell Error exceptions)\n",
    "\n",
    "# Destination directory of txt files\n",
    "dest_files_path = os.path.join(os.getcwd(), 'data/processed/txt_files')\n",
    "\n",
    "# Loop to extract txt from word files (with decorator progress bar)\n",
    "for word_file in  tqdm(os.listdir(docs_path)):\n",
    "\n",
    "    file, extension = os.path.splitext(word_file)\n",
    "    \n",
    "    # Create txt file concatenating .txt extension to file name\n",
    "    dest_file_name = file + '.txt'\n",
    "    \n",
    "    # Extract text from the file\n",
    "    content = textract.process(os.path.join(docs_path, word_file))\n",
    "    \n",
    "    # Create and open new file & prepare to write the Binary Data (represented by wb - Write Binary)\n",
    "    write_text_file = open(os.path.join(dest_files_path, dest_file_name), \"wb\")\n",
    "    \n",
    "    # Write the content and close the newly created file\n",
    "    write_text_file.write(content)\n",
    "    write_text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Descriptive statistics on the downloaded files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides some descriptive statistics on the downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 35255\n",
      "Number of unique file names: 35255\n",
      "Number of unique file sizes: 4862464\n",
      "Number of unique file sizes: 1\n"
     ]
    }
   ],
   "source": [
    "def get_size(filename):\n",
    "    st = os.stat(filename)\n",
    "    return st.st_size\n",
    "\n",
    "# Extract name and size of all files in docs_path\n",
    "files_name_list_raw = [f for f in listdir(docs_path) if isfile(join(docs_path, f))]\n",
    "files_size_list_raw = [get_size(join(docs_path, f)) for f in listdir(docs_path) if isfile(join(docs_path, f))]\n",
    "\n",
    "# Obtain/check number of files\n",
    "print(f'Number of files: {len(files_name_list_raw)}')\n",
    "\n",
    "# Unique files based on size file_name\n",
    "print(f'Number of unique file names: {len(set(files_name_list_raw))}')\n",
    "\n",
    "# Unique files based on size file_name\n",
    "print(f'Max file sizes: {max(files_size_list_raw)}')\n",
    "print(f'Min of unique file sizes: {min(files_size_list_raw)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99328\n"
     ]
    }
   ],
   "source": [
    "print(files_size_list_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#  Number of files\n",
    "\n",
    "# Size of files\n",
    "https://realpython.com/working-with-files-in-python/#getting-file-attributes\n",
    "\n",
    "# doc vs docx\n",
    "https://realpython.com/working-with-files-in-python/#filename-pattern-matching\n",
    "\n",
    "# Number of dictionaries with sentence. Delete the rest?\n",
    "\n",
    "# Longest sentence\n",
    "\n",
    "# Shortest sentemce\n",
    "\n",
    "# Number of reported vs unreported cases (use the name of the file to discriminate them)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da6e46dab1e0b3d9fa32aec1170dd2df7038a4f7be3a54c97a348d8ad782954"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

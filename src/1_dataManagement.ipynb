{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing scraped data (27th October 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook manages the tribunal decision's data scraped in 0_dataScraping.ipynb\n",
    "\n",
    "In particular, the notebook:\n",
    "\n",
    "1. Converts the downloaded word (doc/docx) documents to .txt\n",
    "\n",
    "2. Stores the text of each judicial decision in the corresponding dictionary.\n",
    "    \n",
    "3. Provides some descriptive statistics of the downloaded files.\n",
    "\n",
    "The resulting data set (a list of dictionaries) is serialised as a json object (jsonDataFinal.json).\n",
    "\n",
    "This notebook should run in the tfm environment, which can be created with the environment.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment: /Users/albertamurgopacheco/anaconda3/envs/tfm/bin/python\n",
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, getsize\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import whois\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import textract\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# What environment am I using?\n",
    "print(f'Current environment: {sys.executable}')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/albertamurgopacheco/Documents/GitHub/TFM')\n",
    "# What's my working directory?\n",
    "print(f'Current working directory: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directories in colab and local execution\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    docs_path = '/content/gdrive/MyDrive/TFM/data/raw'\n",
    "    input_path = '/content/gdrive/MyDrive/TFM'\n",
    "    output_path = '/content/gdrive/MyDrive/TFM/output'\n",
    "\n",
    "else:\n",
    "    docs_path = './data/raw'\n",
    "    input_path = '.'\n",
    "    output_path = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all files in docs_path (name, size and last modification_time)\n",
    "files_name_list_raw = [f for f in listdir(docs_path) if isfile(join(docs_path, f))]\n",
    "files_size_list = [getsize(join(docs_path, f)) for f in listdir(docs_path) if isfile(join(docs_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 0\n",
      "Number of unique file names: 0\n",
      "Number of unique file sizes: 0\n"
     ]
    }
   ],
   "source": [
    "# Obtain/check number of files\n",
    "print('Number of files:', len(files_name_list_raw))\n",
    "# Unique files based on size file_name\n",
    "print('Number of unique file names:', len(set(files_name_list_raw)))\n",
    "# Unique files based on size file_name\n",
    "print('Number of unique file sizes:', len(set(files_name_list_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate something from the file name\n",
    "#files_name_list = [re.sub('^\\d*_|\\.\\w{3}\\Z', '', file_name) for file_name in files_name_list_raw]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to obtain the data from the court's decision detailed page using beautifulSoup.     TO DO: 2. Try with different files make sure it works, 3. Capture exceptions and 204 responses. 4. Create function, 5. How am I storing the dicts? In a list? 5. Try function with just a few obs. https://stackoverflow.com/questions/20638006/convert-list-of-dictionaries-to-a-pandas-dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35244/35244 [58:16<00:00, 10.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Delete DS_Store files in raw data folder\n",
    "!find . -name '.DS_Store' -type f -delete\n",
    "\n",
    "# Files HU077022015.doc & HU029682017.docx are corrupt. Manually deleted from data/raw \n",
    "# (textract not dealing with Shell Error execptions)\n",
    "\n",
    "# Destination directory of txt files\n",
    "dest_files_path = os.path.join(os.getcwd(), 'data/processed/txt_files')\n",
    "\n",
    "# Loop to extract txt from word files (with decorator progress bar)\n",
    "for word_file in  tqdm(os.listdir(docs_path)):\n",
    "\n",
    "    file, extension = os.path.splitext(word_file)\n",
    "    \n",
    "    # Create txt file concatenating .txt extension to file name\n",
    "    dest_file_name = file + '.txt'\n",
    "    \n",
    "    # Extract text from the file\n",
    "    content = textract.process(os.path.join(docs_path, word_file))\n",
    "    \n",
    "    # Create and open new file & prepare to write the Binary Data (represented by wb - Write Binary)\n",
    "    write_text_file = open(os.path.join(dest_files_path, dest_file_name), \"wb\")\n",
    "    \n",
    "    # Write the content and close the newly created file\n",
    "    write_text_file.write(content)\n",
    "    write_text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined all the necessary functions, we can open a browser and start scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the text of each decision to jsonData\n",
    "A string with sentence text is added to each object in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35087/35087 [05:32<00:00, 105.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Paths to jsonData & txt files\n",
    "jsonData_path = os.path.join(os.getcwd(), 'data/jsonData.json')\n",
    "txt_path = './data/processed/txt_files/'\n",
    "\n",
    "# Open jsonData file as data\n",
    "with open(jsonData_path) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Loading string with court decision to data\n",
    "for txt_file in  tqdm(os.listdir(txt_path)):\n",
    "    \n",
    "    # Open file and obtain string and file_name\n",
    "    with open(txt_path + txt_file, 'r') as file:\n",
    "        string = file.read()\n",
    "        f_name, f_ext = os.path.splitext(file.name)\n",
    "        head, file_name = os.path.split(f_name)\n",
    "    # Search data list of dictionaries for dict where {\"File\":} = file_name\n",
    "    for d in data:\n",
    "        if d.get('File') == file_name:\n",
    "            # Add dictionary key 'String' with value string\n",
    "            d.update({'String': string})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as a json file jsonDataFinal in data directory\n",
    "with open('./data/jsonDataFinal.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics on the files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of files\n",
    "\n",
    "# Longest sentence\n",
    "\n",
    "# Shortest sentemce\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da6e46dab1e0b3d9fa32aec1170dd2df7038a4f7be3a54c97a348d8ad782954"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

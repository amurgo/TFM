{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec approach (12th December 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TThis notebook applies word2vec to the corpus of tribunal decisions.\n",
    "\n",
    "In particular, the notebook does:\n",
    "\n",
    "1. Data preparation for word2vec.\n",
    "\n",
    "2. Implementation of word2vec averaged per document.\n",
    "\n",
    "3. Implementation of doc2vec.\n",
    "\n",
    "This notebook should run in the tfm environment, which can be created with the environment.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment: /Users/albertamurgopacheco/anaconda3/envs/tfm/bin/python\n",
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n"
     ]
    }
   ],
   "source": [
    "import ipykernel\n",
    "from os import listdir\n",
    "from os.path import isfile, join, getsize\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# What environment am I using?\n",
    "print(f'Current environment: {sys.executable}')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/albertamurgopacheco/Documents/GitHub/TFM')\n",
    "# What's my working directory?\n",
    "print(f'Current working directory: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directories in colab and local execution\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    docs_path = '/content/gdrive/MyDrive/TFM/data/raw'\n",
    "    input_path = '/content/gdrive/MyDrive/TFM'\n",
    "    output_path = '/content/gdrive/MyDrive/TFM/output'\n",
    "\n",
    "else:\n",
    "    docs_path = './data/raw'\n",
    "    input_path = '.'\n",
    "    output_path = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOC2VEC: PARTIAL & FULL TEXT OF THE RULING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The data and functions needed for the averaged word2vec and the doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing two corrupt files from the corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35305/35305 [00:00<00:00, 224197.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus includes 35305 documents.\n",
      "The documents are of type: <class 'str'>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Open jsonDataFinal file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# List to store the judicial decisions\n",
    "corpus = []\n",
    "\n",
    "corruptFiles = ['HU077022015', 'HU029682017']\n",
    "\n",
    "# Search data list of dictionaries for dict where {\"File\":} = file_name\n",
    "for d in tqdm(data):\n",
    "    # Dealing with corrupt and empty files\n",
    "    if d.get('File') not in corruptFiles:\n",
    "        doc = d.get('String')\n",
    "        #dec = d.get('Decision:')\n",
    "        #dec_label = d.get('Decision label:')\n",
    "        if doc:\n",
    "            corpus.append(doc)\n",
    "            #decisions.append(dec)\n",
    "            #decisions_labels.append(dec_label)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print(f'The corpus includes {len(corpus)} documents.')\n",
    "#print(f'The number of documents with a decision: {len(decisions)}.')\n",
    "#print(f'The number of decisions with a label: {len(decisions_labels)} documents.')\n",
    "\n",
    "print(f'The documents are of type: {type(corpus[0])}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text of the decisions (and the labels) is clean. It was cleaned during the information extraction process because the exctraction required tokenization (stanza). The text of the strategies (corpus) needs to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim-implemented filters for preprocessing data\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, \n",
    "strip_multiple_whitespaces, strip_non_alphanum, strip_numeric, remove_stopwords]\n",
    "\n",
    "# List storing the preprocessed documents\n",
    "corpus_clean = [preprocess_string(doc, CUSTOM_FILTERS) for doc in corpus]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "\n",
    "\n",
    "def mbkmeans_clusters(\n",
    "\tX, \n",
    "    k, \n",
    "    mb, \n",
    "    print_silhouette_values, \n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AVERAGED WORD2VEC ON THE FULL TEXT OF THE RULING\n",
    "\n",
    "The strategy consists on training a word2vec model using the entire text of all the 35305 rulings. The results are averaged per document. Clustering algorithms are used to identify types of documents and their topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35305, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of processing cores\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "w2v_model = Word2Vec(min_count = 20,\n",
    "                     window = 3,\n",
    "                     vector_size = 300,\n",
    "                     sample = 6e-5,\n",
    "                     alpha = 0.03,\n",
    "                     min_alpha = 0.0007,\n",
    "                     negative = 20,\n",
    "                     workers = cores-1)\n",
    "\n",
    "model = Word2Vec.load('./output/gensim-model-All')\n",
    "\n",
    "vectorized_docs = vectorize(corpus_clean, model = model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open jsonData file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Create df with data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Replace np.nan values for empty list where no decision \n",
    "df['Decision:'] = [ [] if isinstance(x, float) else x for x in df['Decision:']]\n",
    "\n",
    "# Original texts and labels of the decisions tokenized and ready to train a w2v\n",
    "decisions = df['Decision:'].tolist()\n",
    "decisions_labels = df['Decision label:'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35305, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "vectorized_decisions = vectorize(decisions, model = model)\n",
    "len(vectorized_decisions), len(vectorized_decisions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2\n",
      "Silhouette coefficient: 0.42\n",
      "Inertia:423048.1420920639\n",
      "Silhouette values:\n",
      "    Cluster 1: Size:9190 | Avg:0.60 | Min:0.03 | Max: 0.78\n",
      "    Cluster 0: Size:26115 | Avg:0.35 | Min:-0.13 | Max: 0.53\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "docs = df['Decision:'].tolist()\n",
    "decision_labels = df['Decision label:'].tolist()\n",
    "\n",
    "\n",
    "clustering, cluster_labels = mbkmeans_clusters(\n",
    "\tX = vectorized_decisions,\n",
    "    k = 2,\n",
    "    mb = 500,\n",
    "    print_silhouette_values = True,\n",
    ")\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": docs,\n",
    "    \"tokens\": [\" \".join(text) for text in corpus_clean],\n",
    "    \"cluster\": cluster_labels,\n",
    "    \"label\": decision_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cluster</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[notice, of, decision, directions, the, decisi...</td>\n",
       "      <td>pic iac fh ck v upper tribunal immigration asy...</td>\n",
       "      <td>1</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[in, light, of, my, conclusions, on, that, poi...</td>\n",
       "      <td>utijr jr upper tribunal immigration asylum cha...</td>\n",
       "      <td>1</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[notice, of, decision, the, decision, of, the,...</td>\n",
       "      <td>pic upper tribunal immigration asylum chamber ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[notice, of, decision, the, decision, of, the,...</td>\n",
       "      <td>pic upper tribunal immigration asylum chamber ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[decision, the, decision, of, tribunal, judge,...</td>\n",
       "      <td>pic upper tribunal immigration asylum chamber ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[decision, the, judge, materially, erred, in, ...</td>\n",
       "      <td>pic upper tribunal immigration asylum chamber ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[decision, the, judge, materially, erred, in, ...</td>\n",
       "      <td>pic upper tribunal immigration asylum chamber ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[from, the, point, at, which, the, applicant, ...</td>\n",
       "      <td>upper tribunal pic jr field house breams build...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[]</td>\n",
       "      <td>pic case jr v upper tribunal immigration asylu...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[notice, of, decision, the, decision, of, the,...</td>\n",
       "      <td>pic upper tribunal immigration asylum chamber ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   [notice, of, decision, directions, the, decisi...   \n",
       "1   [in, light, of, my, conclusions, on, that, poi...   \n",
       "2   [notice, of, decision, the, decision, of, the,...   \n",
       "3   [notice, of, decision, the, decision, of, the,...   \n",
       "4   [decision, the, decision, of, tribunal, judge,...   \n",
       "..                                                ...   \n",
       "95  [decision, the, judge, materially, erred, in, ...   \n",
       "96  [decision, the, judge, materially, erred, in, ...   \n",
       "97  [from, the, point, at, which, the, applicant, ...   \n",
       "98                                                 []   \n",
       "99  [notice, of, decision, the, decision, of, the,...   \n",
       "\n",
       "                                               tokens  cluster     label  \n",
       "0   pic iac fh ck v upper tribunal immigration asy...        1  Accepted  \n",
       "1   utijr jr upper tribunal immigration asylum cha...        1  Rejected  \n",
       "2   pic upper tribunal immigration asylum chamber ...        0  Rejected  \n",
       "3   pic upper tribunal immigration asylum chamber ...        0  Accepted  \n",
       "4   pic upper tribunal immigration asylum chamber ...        0  Accepted  \n",
       "..                                                ...      ...       ...  \n",
       "95  pic upper tribunal immigration asylum chamber ...        0  Accepted  \n",
       "96  pic upper tribunal immigration asylum chamber ...        0  Accepted  \n",
       "97  upper tribunal pic jr field house breams build...        1   Neutral  \n",
       "98  pic case jr v upper tribunal immigration asylu...        1       NaN  \n",
       "99  pic upper tribunal immigration asylum chamber ...        0  Accepted  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clusters.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representative terms per cluster (based on centroids):\n",
      "Cluster 0: decision tribunal appeal tier aside dismiss remade determination ftt remake \n",
      "Cluster 1: decision tribunal appeal tier ftt accordingly aside judge determination error \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_7/fl0v9rpx6zg6s4j03vcw0fc40000gn/T/ipykernel_3191/89129640.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokens_per_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmost_representative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost_representative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtokens_per_cluster\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"{t[0]} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(50):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=10)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AVERAGED WORD2VEC ON TEXT OF THE RULING DESCRIBING THE DECISION\n",
    "\n",
    "The strategy consists on training a word2vec model using the part of the text describing the judges decision from each the 35305 rulings. The results are averaged per document. Clustering algorithms are used to identify types of documents and their topics. There should be three types of documents: \"Accepted\", \"Rejected\" and \"Salomonic/cannot tell\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of processing cores\n",
    "model_judgements = Word2Vec.load('./output/gensim-model-corpus_judgements')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35305, 300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_decisions = [x for x in decisions if x != None]\n",
    "\n",
    "vectorized_decisions = vectorize(decisions, model = model_judgements)\n",
    "\n",
    "len(vectorized_decisions), len(vectorized_decisions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3\n",
      "Silhouette coefficient: 0.34\n",
      "Inertia:120377.10873388828\n",
      "Silhouette values:\n",
      "    Cluster 2: Size:9810 | Avg:0.55 | Min:0.02 | Max: 0.73\n",
      "    Cluster 1: Size:4238 | Avg:0.40 | Min:-0.02 | Max: 0.57\n",
      "    Cluster 0: Size:21257 | Avg:0.23 | Min:-0.12 | Max: 0.43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clustering, cluster_labels = mbkmeans_clusters(\n",
    "\tX = vectorized_decisions,\n",
    "    k = 3,\n",
    "    mb = 500,\n",
    "    print_silhouette_values = True,\n",
    ")\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": decisions,\n",
    "    \"tokens\": [\" \".join(text) for text in decisions],\n",
    "    \"cluster\": cluster_labels,\n",
    "    \"label\": decisions_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representative terms per cluster (based on centroids):\n",
      "Cluster 0: decision the aside set of law appeal error tribunal notice making i point involved remake \n",
      "Cluster 1: tribunal procedure direction anonymity rule unless until directs upper rules regarding otherwise the of court \n",
      "Cluster 2: the of i to aside decision appeal set it is law tribunal be that tier \n"
     ]
    }
   ],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(3):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model_judgements.wv.most_similar(positive = [clustering.cluster_centers_[i]], topn = 15)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cluster</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[notice, of, decision, directions, the, decisi...</td>\n",
       "      <td>notice of decision directions the decision of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[in, light, of, my, conclusions, on, that, poi...</td>\n",
       "      <td>in light of my conclusions on that point littl...</td>\n",
       "      <td>2</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[notice, of, decision, the, decision, of, the,...</td>\n",
       "      <td>notice of decision the decision of the tribuna...</td>\n",
       "      <td>1</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[notice, of, decision, the, decision, of, the,...</td>\n",
       "      <td>notice of decision the decision of the tribuna...</td>\n",
       "      <td>0</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[decision, the, decision, of, tribunal, judge,...</td>\n",
       "      <td>decision the decision of tribunal judge malcol...</td>\n",
       "      <td>0</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35300</th>\n",
       "      <td>[conclusions, a, northern, cyprus, is, not, ca...</td>\n",
       "      <td>conclusions a northern cyprus is not capable o...</td>\n",
       "      <td>2</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35301</th>\n",
       "      <td>[for, the, reasons, we, have, given, we, dismi...</td>\n",
       "      <td>for the reasons we have given we dismiss this ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35302</th>\n",
       "      <td>[our, conclusions, on, the, general, issues, r...</td>\n",
       "      <td>our conclusions on the general issues relating...</td>\n",
       "      <td>2</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35303</th>\n",
       "      <td>[conclusions, for, each, of, the, main, reason...</td>\n",
       "      <td>conclusions for each of the main reason a and ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35304</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35305 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      [notice, of, decision, directions, the, decisi...   \n",
       "1      [in, light, of, my, conclusions, on, that, poi...   \n",
       "2      [notice, of, decision, the, decision, of, the,...   \n",
       "3      [notice, of, decision, the, decision, of, the,...   \n",
       "4      [decision, the, decision, of, tribunal, judge,...   \n",
       "...                                                  ...   \n",
       "35300  [conclusions, a, northern, cyprus, is, not, ca...   \n",
       "35301  [for, the, reasons, we, have, given, we, dismi...   \n",
       "35302  [our, conclusions, on, the, general, issues, r...   \n",
       "35303  [conclusions, for, each, of, the, main, reason...   \n",
       "35304                                                 []   \n",
       "\n",
       "                                                  tokens  cluster     label  \n",
       "0      notice of decision directions the decision of ...        2  Accepted  \n",
       "1      in light of my conclusions on that point littl...        2  Rejected  \n",
       "2      notice of decision the decision of the tribuna...        1  Rejected  \n",
       "3      notice of decision the decision of the tribuna...        0  Accepted  \n",
       "4      decision the decision of tribunal judge malcol...        0  Accepted  \n",
       "...                                                  ...      ...       ...  \n",
       "35300  conclusions a northern cyprus is not capable o...        2  Accepted  \n",
       "35301  for the reasons we have given we dismiss this ...        2  Rejected  \n",
       "35302  our conclusions on the general issues relating...        2  Rejected  \n",
       "35303  conclusions for each of the main reason a and ...        2  Accepted  \n",
       "35304                                                           2       NaN  \n",
       "\n",
       "[35305 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_tagged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_7/fl0v9rpx6zg6s4j03vcw0fc40000gn/T/ipykernel_3191/3022189486.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_dmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.065\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.065\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_dmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tagged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_tagged' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da6e46dab1e0b3d9fa32aec1170dd2df7038a4f7be3a54c97a348d8ad782954"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping UK asylum data (28th October 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scraps the tribunal decision's data related to asylum applications from https://tribunalsdecisions.service.gov.uk/\n",
    "\n",
    "In particular, the notebook:\n",
    "\n",
    "1. Defines the needed web scraping functions.\n",
    "\n",
    "2. Scraps the contents of tribunalsdecisions.service.gov.uk/utiac . The scraping strategy consists of three steps:\n",
    "- First, launching a search session and scraping the general information obtained in the 1178 pages of results (using selenium to navigate through the results). A list of 35299 urls is obtained.\n",
    "- Second, accessing each of the 35299 urls ans scraping all the detailed information available.\n",
    "- Third, downloading the word (doc/docx) document with the judicial decision.\n",
    "\n",
    "3. Stores the scraped material in a list of dictionaries where each dictionary contains all the data scraped for a given judicial decision. The resulting data set is serialised in json and pickle. Json (jsonData.json) and a picle (pickleData.pkl) objects are created.\n",
    "\n",
    "This notebook should run in the tfm environment, which can be created with the environment.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment: /Users/albertamurgopacheco/anaconda3/envs/tfm/bin/python\n",
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import whois\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import *\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import urllib.request\n",
    "import wget\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# What environment am I using?\n",
    "print(f'Current environment: {sys.executable}')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/albertamurgopacheco/Documents/GitHub/TFM')\n",
    "# What's my working directory?\n",
    "print(f'Current working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directories in colab and local execution\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    docs_path = '/content/gdrive/MyDrive/TFM/data/raw'\n",
    "    input_path = '/content/gdrive/MyDrive/TFM'\n",
    "    output_path = '/content/gdrive/MyDrive/TFM/output'\n",
    "\n",
    "else:\n",
    "    docs_path = './data/raw'\n",
    "    input_path = '.'\n",
    "    output_path = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define scraping functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to scrap the general data in the UIATC landing page using beautifulSoup.\n",
    "\n",
    "More specifically, the general data refers to: 1) the url pointing to a page with detailed information for each sentence, and 2) the date of the judicial sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(htmlSource):\n",
    "    \"\"\"\n",
    "    getData gets the mouse over links to the tribunal decisions & the dates\n",
    "\n",
    "    :param htmlSource: Source HTML for the page \n",
    "    :return: data as a list of tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    # Scraping tribunal decision names with BeautifulSoup\n",
    "    soup = BeautifulSoup(htmlSource, 'html')\n",
    "    \n",
    "    # Scraping mouse-over urls\n",
    "    linksList = re.findall(r'(<a href=\"/utiac/(.*?)\">)', htmlSource)\n",
    "    linksList = [i[1] for i in linksList]\n",
    "    linksList = list(set(linksList))\n",
    "    #print(\"Number of links:\",len(linksList))\n",
    "    #print(linksList)\n",
    "    \n",
    "    # Scraping dates\n",
    "\n",
    "    # Find by class the dates and store in list\n",
    "    datesList = list(soup.find_all(\"td\", class_=\"date\"))\n",
    "    # Convert the list elements to string  \n",
    "    datesList = [str(i) for i in datesList]\n",
    "    # Slice the part of string including data (date format yyyy-mm-dd)\n",
    "    datesList = [i[33:43] for i in datesList]\n",
    "    #print(datesList)\n",
    "    #print(\"Number of dates %s\",len(datesList))\n",
    "\n",
    "    # Assign data to tuples: # get the list of tuples from two lists and merge them by using zip()\n",
    "    tuplesList = list(zip(linksList, datesList))\n",
    "    \n",
    "    return tuplesList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to scrap the detailed data in each of the decision's pages. The functions uses the library requests to make a direct call from each url.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetailedData(url):\n",
    "    \"\"\"\n",
    "    getDetailedData gets the detailed data linked to a tribunal decision\n",
    "    and saves a doc file in /Users/albertamurgopacheco/Documents/GitHub/TFM/data/raw\n",
    "\n",
    "    :param url: url (link) to the page containing the detailed info \n",
    "    :return: dictionary \n",
    "    \"\"\"\n",
    "    \n",
    "    # START WITH URL\n",
    "    try:\n",
    "        response = requests.get(url = url)\n",
    "        \n",
    "        # if response status code is 200 OK, then\n",
    "        if response.status_code == 200:\n",
    "        # load the data\n",
    "            data = response.text\n",
    "            soup = BeautifulSoup(data, 'html')\n",
    "    \n",
    "            # Scrape the reference number\n",
    "            refList = list(soup.find_all(\"h1\"))\n",
    "            # Convert the list elements to string  \n",
    "            refList = [str(i) for i in refList]\n",
    "            # Remove leading <h1> and trailing </h1>\n",
    "            refList = [i.replace('</h1>', '') for i in refList]\n",
    "            refList = [i.replace('<h1>', '') for i in refList]\n",
    "            #print(refList)\n",
    "    \n",
    "            # Find the link (docLink) to the document\n",
    "            lnk = re.findall(r'(<a class=\"doc-file\" href=\"https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/(.*?)\")', data)\n",
    "            # Build link using the second element in regex result (list of tuples)\n",
    "            docLink = \"https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/\" + lnk[0][1]\n",
    "            # Download files to raw folder\n",
    "            try:\n",
    "                filename = wget.download(url = docLink, out = docs_path)\n",
    "                downloaded = \"Yes\"\n",
    "            # Handle download exceptions \n",
    "            except Exception as err:\n",
    "                print(\"Could not download file {}\".format(docLink))\n",
    "                print(err)\n",
    "                downloaded = \"No\"\n",
    "                pass\n",
    "     \n",
    "            # Find detailed information\n",
    "            res = [item.get_text() for item in soup.select(\"span\")]\n",
    "            # Remove \\xa0 from strings\n",
    "            res = [elem if '\\xa0' not in elem else elem.replace('\\xa0', '') for elem in res]\n",
    "            # Remove trailing and leading spaces and \\n\n",
    "            res = [elem.strip() for elem in res]\n",
    "            #print(res)\n",
    "\n",
    "            # Split list of results into two lists (keys & values)\n",
    "            keysList = res[::2] # Keys: Elements from res starting from 0 iterating by 2\n",
    "            valuesList = res[1::2] # Values: Elements from res starting from 1 iterating by 2\n",
    "            #print(keysList)\n",
    "            #print(valuesList)\n",
    "    \n",
    "            # Create dictionary with results (resDict)\n",
    "            zip_iterator = zip(keysList, valuesList)\n",
    "            resDict = dict(zip_iterator)\n",
    "    \n",
    "            # Add reference number and link to document to the dictionary\n",
    "            resDict[\"Document\"] = docLink\n",
    "            resDict[\"Reference\"] = refList\n",
    "            resDict[\"Download\"] = downloaded\n",
    "            resDict[\"File\"] = lnk[0][1]\n",
    "            #print(resDict)\n",
    "            \n",
    "        else:\n",
    "            resDict = {\"URL not working:\": str(url)}\n",
    "            print(f\"URL not working: {url}\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:  # Capture exceptions\n",
    "        print (e.response.text)\n",
    "        raise SystemExit(e)   \n",
    "    \n",
    "    return resDict  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing the web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the necessary functions have been defined, open a firefox browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tribunal decisions\n"
     ]
    }
   ],
   "source": [
    "# Using selenium, open the tribunal decision's website in firefox\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://tribunalsdecisions.service.gov.uk/\")\n",
    "\n",
    "# Getting current URL source code \n",
    "get_title = driver.title \n",
    "  \n",
    "# Printing the title of the URL \n",
    "print(get_title) \n",
    "assert \"Tribunal decisions\" in driver.title\n",
    "\n",
    "# Getting current URL source code \n",
    "get_source = driver.page_source\n",
    "time.sleep(2)\n",
    "#print(get_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start scraping some general information (url and date) for each sentence. There are 1778 pages to go through. The urls will be used to scrap the detailed information. The date is scraped for discrimination purposes in case different sentences shared the same url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scrap current page data and browse to next page\n",
    "\n",
    "# List of tuples to store the results from getData()\n",
    "a = []\n",
    "\n",
    "#while True:\n",
    "i=1\n",
    "while i<1178:\n",
    "\n",
    "    # Getting current URL source code \n",
    "    get_source = driver.page_source\n",
    "    # Scrape the data\n",
    "    b = getData(get_source)\n",
    "    # Append list data b to list data a\n",
    "    a += b\n",
    "    i+=1\n",
    "    \n",
    "    # Click on next page\n",
    "    try:\n",
    "        delay = 15 # seconds\n",
    "        #element_present = EC.presence_of_element_located((By.CLASS_NAME, 'next_page'))\n",
    "        #WebDriverWait(driver, delay).until(element_present)\n",
    "        \n",
    "        wait = WebDriverWait(driver, delay)\n",
    "        element = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'next_page')))\n",
    "        element.click()\n",
    "            # wait       \n",
    "    except TimeoutException:\n",
    "        print(\"Loading took too much time!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the list of urls to iterate with the scraping function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of urls to scrap detailed data from: 35310\n",
      "New number of urls to scrap detailed data from: 35308\n"
     ]
    }
   ],
   "source": [
    "# List of links with the decision files\n",
    "decisionLinks = [tple[0] for tple in a]\n",
    "\n",
    "# Number of urls to scrap detailed data\n",
    "print(f'Number of urls to scrap detailed data from: {len(decisionLinks)}')\n",
    "\n",
    "# Create a list of urls from links\n",
    "urls = [ \"https://tribunalsdecisions.service.gov.uk/utiac/\"+decision for decision in decisionLinks]\n",
    "\n",
    "# Attempted download item urls[35072] crashes the loop\n",
    "# https://tribunalsdecisions.service.gov.uk/utiac/2003-ukiat-7478 (no doc available)\n",
    "# print(urls.index('https://tribunalsdecisions.service.gov.uk/utiac/2003-ukiat-7478'))\n",
    "urls.pop(35072)\n",
    "# print(urls.index('https://tribunalsdecisions.service.gov.uk/utiac/hu-02724-2015'))\n",
    "urls.pop(16561)\n",
    "print(f'New number of urls to scrap detailed data from: {len(urls)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the detailed data for each of the tribunal decission while also downloading the word document of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download file https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/40081/IA083642010___IA083692010___IA083752010.DOC\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "# List of dict where each dict contains scraped detailed data\n",
    "scrapedList = []\n",
    "\n",
    "# Scrap detailed data from all urls\n",
    "for url in urls:\n",
    "    scrapedItem = getDetailedData(url)\n",
    "    #print(scrapedItem)\n",
    "    scrapedList.append(scrapedItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 35308 elements have been scraped\n",
      "{'Case title:': '', 'Appellant name:': '', 'Status of case:': 'Unreported', 'Hearing date:': '27 Aug 2021', 'Promulgation date:': '11 Oct 2021', 'Publication date:': '26 Oct 2021', 'Last updated on:': '26 Oct 2021', 'Country:': '', 'Judges:': '', 'Document': 'https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/73739/HU202322019.doc', 'Reference': ['HU/20232/2019'], 'Download': 'Yes', 'File': '73739/HU202322019.doc'}\n"
     ]
    }
   ],
   "source": [
    "# Number of scraped court decisions\n",
    "print(f'A total of {len(scrapedList)} elements have been scraped')\n",
    "\n",
    "# Print an example\n",
    "print(scrapedList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Case title:': '', 'Appellant name:': '', 'Status of case:': 'Unreported', 'Hearing date:': '27 Aug 2021', 'Promulgation date:': '11 Oct 2021', 'Publication date:': '26 Oct 2021', 'Last updated on:': '26 Oct 2021', 'Country:': '', 'Judges:': '', 'Document': 'https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/73739/HU202322019.doc', 'Reference': ['HU/20232/2019'], 'Download': 'Yes', 'File': 'HU202322019'}\n"
     ]
    }
   ],
   "source": [
    "# Function to fix the name of the File\n",
    "def fix_File(string):\n",
    "    \"\"\"\n",
    "    Given a string of dictionaries search_dictionariesgets obtains the dictionary matching a key/value pair\n",
    "\n",
    "    :string: a string incorporating path and file name with extension\n",
    "    :return: clean string without path/extension\n",
    "    \"\"\"\n",
    "    head, tail = os.path.split(string)\n",
    "    file_name, file_ext = os.path.splitext(tail)\n",
    "    return file_name\n",
    "\n",
    "# Function to fix each value of the key File \n",
    "def update_File(dicc):\n",
    "    \"\"\"\n",
    "    Given a dictionary apply function fix_File() to key: File\n",
    "    \n",
    "    :dicc: a pyhon dict with key File\n",
    "    :return: updated python dict\n",
    "    \"\"\"\n",
    "    val = dicc.get('File')\n",
    "    new_val = fix_File(val)\n",
    "    dicc.update({'File': new_val})\n",
    "    return dicc\n",
    "\n",
    "# Fix the values of {File} key in each dictionary\n",
    "[update_File(x) for x in scrapedList]\n",
    "\n",
    "# Print example to show fix\n",
    "print(scrapedList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Storing the scraped data in a list of dictionaries (jsonData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, serialise the dataset in json (jsonData.json) and pickle (pickleData.pkl) objects and print a sample for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a json file jsonData in data directory\n",
    "with open('./data/jsonData.json', 'w') as fout:\n",
    "    json.dump(scrapedList, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Case title:': '', 'Appellant name:': '', 'Status of case:': 'Unreported', 'Hearing date:': '27 Aug 2021', 'Promulgation date:': '11 Oct 2021', 'Publication date:': '26 Oct 2021', 'Last updated on:': '26 Oct 2021', 'Country:': '', 'Judges:': '', 'Document': 'https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/73739/HU202322019.doc', 'Reference': ['HU/20232/2019'], 'Download': 'Yes', 'File': 'HU202322019'}\n"
     ]
    }
   ],
   "source": [
    "# Open jsonData file and print a sample\n",
    "jsonData_path = os.path.join(os.getcwd(), './data/jsonData.json')\n",
    "with open(jsonData_path) as json_file:\n",
    "    data = json.load(json_file)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle\n",
    "with open('./data/pickleData.pkl', 'wb') as f:\n",
    "    pickle.dump(scrapedList, f, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Case title:': '', 'Appellant name:': '', 'Status of case:': 'Unreported', 'Hearing date:': '27 Aug 2021', 'Promulgation date:': '11 Oct 2021', 'Publication date:': '26 Oct 2021', 'Last updated on:': '26 Oct 2021', 'Country:': '', 'Judges:': '', 'Document': 'https://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/73739/HU202322019.doc', 'Reference': ['HU/20232/2019'], 'Download': 'Yes', 'File': 'HU202322019'}\n"
     ]
    }
   ],
   "source": [
    "# Open pickle file and print a sample\n",
    "with open('./data/pickleData.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)\n",
    "print(d[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "File ttps://moj-tribunals-documents-prod.s3.amazonaws.com/decision/doc_file/40081/IA083642010___IA083692010___IA083752010.DOC could not be downloaded due to a HTTP Error 403: Forbidden\n",
    "This corresponds to url: https://tribunalsdecisions.service.gov.uk/utiac/2003-ukiat-7478"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da6e46dab1e0b3d9fa32aec1170dd2df7038a4f7be3a54c97a348d8ad782954"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

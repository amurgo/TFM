{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction (30th October 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook applies word2vec to the corpus of tribunal decisions.\n",
    "\n",
    "In particular, the notebook does:\n",
    "\n",
    "1. Data preparation for word2vec.\n",
    "\n",
    "2. Implementation of word2vec.\n",
    "\n",
    "3. Topic model with Latent Dirichlet Allocation (LDA) and Latent Semantic Indexing (LSI).\n",
    "\n",
    "The resulting trained model is... .\n",
    "\n",
    "This notebook should run in the tfm environment, which can be created with the environment.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment: /Users/albertamurgopacheco/anaconda3/envs/tfm/bin/python\n",
      "Current working directory: /Users/albertamurgopacheco/Documents/GitHub/TFM\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, getsize\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import whois\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import textract\n",
    "import gensim\n",
    "import spacy\n",
    "import scipy as sp\n",
    "import sys\n",
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import logging\n",
    "\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# What environment am I using?\n",
    "print(f'Current environment: {sys.executable}')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/albertamurgopacheco/Documents/GitHub/TFM')\n",
    "# What's my working directory?\n",
    "print(f'Current working directory: {os.getcwd()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directories in colab and local execution\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    docs_path = '/content/gdrive/MyDrive/TFM/data/raw'\n",
    "    input_path = '/content/gdrive/MyDrive/TFM'\n",
    "    output_path = '/content/gdrive/MyDrive/TFM/output'\n",
    "\n",
    "else:\n",
    "    docs_path = './data/raw'\n",
    "    input_path = '.'\n",
    "    output_path = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data to a list of documents (corpus) where each document is a judicial decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35308/35308 [00:00<00:00, 880564.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus includes 35305 documents.\n",
      "The documents are <class 'str'>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Open jsonDataFinal file as data\n",
    "with open('./data/jsonDataFinal.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# List to store the judicial decisions\n",
    "corpus = []\n",
    "\n",
    "corruptFiles = ['HU077022015', 'HU029682017']\n",
    "\n",
    "# Search data list of dictionaries for dict where {\"File\":} = file_name\n",
    "for d in tqdm(data):\n",
    "    # Dealing with corrupt and empty files\n",
    "    if d.get('File') not in corruptFiles:\n",
    "        doc = d.get('String')\n",
    "        if doc:\n",
    "            corpus.append(doc)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print(f'The corpus includes {len(corpus)} documents.')\n",
    "print(f'The documents are {type(corpus[0])}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLeaning each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim-implemented filters for preprocessing data\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, \n",
    "strip_multiple_whitespaces, strip_non_alphanum, strip_numeric, remove_stopwords]\n",
    "\n",
    "# List storing thr preprocessed documents\n",
    "corpus_clean = [preprocess_string(doc, CUSTOM_FILTERS) for doc in corpus]\n",
    "\n",
    "# Removing non-numerical characters\n",
    "#brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in corpus_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cores 4.\n"
     ]
    }
   ],
   "source": [
    "# Number of available processing cores\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(f'Available cores {cores}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing and removing the stopwords and non-alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 587.13 mins\n"
     ]
    }
   ],
   "source": [
    "# IF RAN, DO IT BEFORE CLEANING... \n",
    "\n",
    "\n",
    "import spacy\n",
    "#nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "#nlp = spacy.load('en_core_web_sm') # disabling Named Entity Recognition for speed\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def stem_doc(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    stem_doc = [porter_stemmer.stem(wd) for wd in doc]\n",
    "    # if len(wd) > 2\n",
    "    return stem_doc\n",
    "\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "\n",
    "#txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size = 50, n_process = cores -1)]\n",
    "stemmed_corpus = [stem_doc(doc) for doc in corpus_clean]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pic', 'iac', 'fh', 'ck', 'v', 'upper', 'tribun', 'immigr', 'asylum', 'chamber', 'appeal', 'number', 'hu', 'immigr', 'act']\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_corpus[0][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer_doc(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    lemm_doc = [wordnet_lemmatizer.lemmatize(wd) for wd in doc]\n",
    "    # if len(wd) > 2\n",
    "    return lemm_doc\n",
    "\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "\n",
    "#txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size = 50, n_process = cores -1)]\n",
    "lemmatized_corpus = [lemmatizer_doc(doc) for doc in stemmed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pic', 'iac', 'fh', 'ck', 'v', 'upper', 'tribun', 'immigr', 'asylum', 'chamber', 'appeal', 'number', 'hu', 'immigr', 'act']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_corpus = lemmatized\n",
    "print(lemmatized_corpus[0][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting common phrases (multi-word or collocations) expressions from the stream of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fc/h07gydfn7nd0zq3mglwpn3r40000gn/T/ipykernel_34371/3788727072.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphrases\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPhraser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmatized_corpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mphrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fc/h07gydfn7nd0zq3mglwpn3r40000gn/T/ipykernel_34371/3788727072.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphrases\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPhraser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmatized_corpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mphrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "sent = [row.split() for row in corpus if row]\n",
    "\n",
    "phrases = Phrases(sent, min_count = 30, progress_per = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90189"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict  # For word frequency\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "#sentences = bigram[corpus_clean]\n",
    "sentences = bigram[lemmatized_corpus]\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['appel',\n",
       " 's',\n",
       " 'judg',\n",
       " 'decis',\n",
       " 'tribun',\n",
       " 'appeal',\n",
       " 'evid',\n",
       " 'state',\n",
       " 'respond',\n",
       " 'mr',\n",
       " 'applic',\n",
       " 'reason',\n",
       " 'paragraph',\n",
       " 'case',\n",
       " 'immigr',\n",
       " 'tier',\n",
       " 'consid',\n",
       " 'famili',\n",
       " 'law',\n",
       " 'rule']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key = word_freq.get, reverse = True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation of word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding model is a model that can provide numerical vectors for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(min_count = 20,\n",
    "                     window = 10,\n",
    "                     vector_size = 300,\n",
    "                     sample = 6e-5,\n",
    "                     alpha = 0.03,\n",
    "                     min_alpha = 0.0007,\n",
    "                     negative = 20,\n",
    "                     workers = cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build the vocabulary: 1.12 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per = 10000)\n",
    "\n",
    "print('Time to build the vocabulary: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 40.47 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs = 30, report_delay = 1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('./output/gensim-model-All_bigrams_lemmatized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06634754"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('refuge', 'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('refugee', 'error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('find', 0.6643282175064087),\n",
       " ('er', 0.640164315700531),\n",
       " ('arguabl', 0.6103781461715698),\n",
       " ('determin', 0.6081475019454956),\n",
       " ('ground', 0.6040764451026917),\n",
       " ('second_ground', 0.5902823805809021),\n",
       " ('reason', 0.5790618658065796),\n",
       " ('error', 0.578513503074646),\n",
       " ('tier', 0.5736409425735474),\n",
       " ('properli', 0.5588219165802002)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive = [\"judg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('humanitarian', 0.6074868440628052),\n",
       " ('geneva', 0.5982367992401123),\n",
       " ('convent', 0.571603536605835),\n",
       " ('protect', 0.5548973083496094),\n",
       " ('cessat', 0.5113057494163513),\n",
       " ('refoul', 0.4869057834148407),\n",
       " ('persecut', 0.465663343667984),\n",
       " ('stateless_person', 0.463575541973114),\n",
       " ('seek_refuge', 0.4553128778934479),\n",
       " ('found', 0.4405970871448517)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive = [\"refuge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brp', 0.507870078086853),\n",
       " ('biograph', 0.4448765218257904),\n",
       " ('student_migrant', 0.40723922848701477),\n",
       " ('aadhaar', 0.3919180929660797),\n",
       " ('inid', 0.38970157504081726),\n",
       " ('invalid', 0.382639080286026),\n",
       " ('valid', 0.3679819107055664),\n",
       " ('permit', 0.3635072410106659),\n",
       " ('applic', 0.3578976094722748),\n",
       " ('centim', 0.34924787282943726)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"biometr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of gensim tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unit', 0.6758801341056824),\n",
       " ('kingdom', 0.6745372414588928),\n",
       " ('remain', 0.6378411650657654),\n",
       " ('year', 0.5883458256721497),\n",
       " ('leav', 0.5810246467590332),\n",
       " ('live', 0.5688135623931885),\n",
       " ('british', 0.5632806420326233),\n",
       " ('continu', 0.5620144009590149),\n",
       " ('enter', 0.5400956273078918),\n",
       " ('visitor', 0.5124759078025818)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"uk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odd-One-Out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match(['jimbo', 'milhouse', 'kearney'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "data = [d for d in dataset]\n",
    "print(type(data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1613 unique tokens: ['abandoned', 'ability', 'able', 'absence', 'accept']...)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in corpus_pre]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the word to id map\n",
    "#print(dictionary.token2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a bag of words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in corpus_pre]\n",
    "\n",
    "# Create the Corpus\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "#pprint(mycorpus)\n",
    "\n",
    "word_counts = [[(mydict[id], count) for id, count in line] for line in mycorpus]\n",
    "#pprint(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match(['jimbo', 'milhouse', 'kearney'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic model with Latent Dirichlet Allocation (LDA) and Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of topic models is to extract the underlying topics from a collection of text documents. Each document in the text is considered as a combination of topics and each topic is considered as a combination of related words.\n",
    "\n",
    "Topic modeling can be done by algorithms like Latent Dirichlet Allocation (LDA) and Latent Semantic Indexing (LSI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Latent Dirichlet Allocation (LDA)\n",
    "Each document can be described by a distribution of topics and each topic can be described by a distribution of words. https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, LdaMulticore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fc/h07gydfn7nd0zq3mglwpn3r40000gn/T/ipykernel_34371/437228251.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# See trigram example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigram_mod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbigram_mod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_words' is not defined"
     ]
    }
   ],
   "source": [
    "# Starting from corpus_clean\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(corpus_clean, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[corpus_clean], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See trigram example\n",
    "#print(trigram_mod[bigram_mod[corpus_clean[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pic', 'upper', 'tribunal', 'immigration', 'asylum', 'chamber', 'appeal', 'number', 'immigration', 'act', 'hear', 'field_house', 'decision', 'reason', 'promulgate', 'extempore', 'upper', 'tribunal', 'judge', 'anonymity', 'direction', 'appellant', 'respondent', 'counsel_instructe', 'consultant', 'office', 'present', 'officer', 'decision', 'reason', 'appellant', 'bear', 'section', 'nationality', 'act', 'decision', 'secretary', 'state', 'refuse', 'application', 'leave', 'remain', 'appeal', 'decision', 'hear', 'tier', 'tribunal', 'march', 'reason', 'set', 'decision', 'refuse', 'reason', 'set', 'decision', 'copy', 'annex', 'decision', 'set', 'aside', 'appellant', 'enter', 'visit', 'visa', 'remain', 'leave', 'form', 'relationship', 'refer', 'sponsor', 'son', 'british', 'previous', 'relationship', 'case', 'long', 'contact', 'child', 'father', 'appellant', 'form', 'parental', 'relationship', 'appellant', 'family', 'live', 'relatively', 'recently', 'sponsor', 'employ', 'secretary', 'state', 'case', 'tier', 'tribunal', 'requirement', 'immigration', 'rule', 'accept', 'appellant', 'establish', 'parental', 'relationship', 'judge', 'tier', 'tribunal', 'number', 'finding', 'requirement', 'immigration', 'rule', 'meet', 'challenge', 'subsist', 'relationship', 'appellant', 'sponsor', 'genuine_subsiste', 'relationship', 'enjoy', 'family', 'best_interest', 'remain', 'balance', 'reasonable_expect', 'leave', 'united', 'away', 'tie', 'judge', 'interference', 'appellant', 'right', 'respect', 'family', 'life', 'leave', 'dismiss', 'appeal', 'basis', 'appellant', 'seek', 'permission', 'appeal', 'decision', 'grant', 'tier', 'tribunal', 'judge', 'subsequent', 'secretary', 'state', 'letter', 'pursuant', 'rule', 'procedure', 'rule', 'state', 'oppose', 'application', 'permission', 'ask', 'tribunal', 'determine', 'appeal', 'fresh', 'oral', 'decision', 'basis', 'decision', 'tier', 'tribunal', 'involve', 'make', 'error', 'law', 'need', 'hear', 'certain', 'finding', 'tier', 'tribunal', 'preserve', 'subsist', 'family', 'life', 'appellant', 'partner', 'identify', 'necessary', 'consider', 'parental', 'relationship', 'appellant', 'hear', 'evidence', 'appellant', 'sponsor', 'adopt', 'witness', 'statement', 'give', 'evidence', 'assistance', 'court', 'interpreter', 'cross_examine', 'tufan', 'behalf', 'secretary', 'state', 'evidence', 'appellant', 'confusing', 'wife', 'work', 'nature', 'work', 'unfortunately', 'documentary', 'evidence', 'nature', 'work', 'payslip', 'take', 'evidence', 'look', 'round', 'accept', 'cease', 'work', 'work', 'confusion', 'online', 'company', 'think', 'company', 'fulfil', 'online', 'order', 'matter', 'entirely', 'clear', 'accept', 'sponsor', 'evidence', 'familiar', 'computer', 'work', 'online', 'capacity', 'misgiving', 'evidence', 'point', 'satisfied', 'relationship', 'subsist', 'accept', 'sponsor', 'receipt', 'sense', 'give', 'previously', 'employ', 'dispute', 'receipt', 'child', 'benefit', 'normal', 'circumstance', 'term', 'article', 'accept', 'meet', 'requirement', 'immigration', 'rule', 'start', 'point', 'assessment', 'position', 'respect', 'article', 'right', 'case', 'regard', 'section', 'act', 'start', 'point', 'appellant', 'requirement', 'immigration', 'rule', 'unlawfully', 'expectation', 'able', 'stay', 'take', 'starting', 'point', 'consider', 'subparagraph', 'section', 'apply', 'start', 'point', 'normally', 'heavy', 'weight', 'attach', 'favour', 'removal', 'give', 'failure', 'meet', 'requirement', 'immigration', 'rule', 'appellant', 'show', 'ability', 'speak', 'english', 'matter', 'financially_independent', 'factor', 'normally', 'weigh', 'similarly', 'private', 'family', 'life', 'little', 'weight', 'attach', 'give', 'term', 'section', 'question', 'turn', 'section', 'require', 'find', 'fact', 'parental', 'relationship', 'exist', 'biological', 'parent', 'child', 'fact', 'sensitive', 'matter', 'number', 'factor', 'account', 'doubt', 'accept', 'appellant', 'sponsor', 'live', 'married', 'couple', 'accept', 'child', 'contact', 'biological', 'father', 'accept', 'evidence', 'somewhat', 'unusual', 'age', 'take', 'school', 'appellant', 'sufficient', 'evidence', 'witness', 'statement', 'evidence', 'close', 'relationship', 'appellant', 'stepson', 'factor', 'tend', 'relatively', 'recent', 'relationship', 'year', 'look', 'evidence', 'satisfy', 'balance_probabilitie', 'parental', 'relationship', 'exist', 'appellant', 'stepson', 'follow', 'basis', 'satisfied', 'section', 'act', 'apply', 'case', 'secretary', 'state', 'urge', 'dismiss', 'appeal', 'basis', 'primarily', 'decision', 'youna', 'ukut', 'basis', 'proportionate', 'expect', 'appellant', 'application', 'entry_clearance', 'point', 'youna', 'distinguished', 'basis', 'tribunal', 'youna', 'section', 'apply', 'establish', 'child', 'leave', 'fact', 'case', 'different', 'child', 'younger', 'secondly', 'importantly', 'tribunal', 'note', 'younas', 'section', 'effect', 'standalone', 'provision', 'describe', 'self', 'contain', 'discussion', 'youna', 'revolve', 'tribunal', 'establish', 'reasonable_expect', 'child', 'leave', 'conclude', 'case', 'find', 'distinguishe', 'appeal', 'turn', 'recent', 'decision', 'court', 'appeal', 'bangladesh', 'ewca_civ', 'important', 'note', 'say', 'case', 'paragraph', 'paragraph', 'court', 'appeal', 'case', 'agree', 'say', 'important', 'emphasise', 'approach', 'approve', 'provide', 'presumption', 'opposite', 'direction', 'represent', 'common', 'sense', 'start', 'point', 'adopt', 'reason', 'give', 'paras', 'judgment', 'remain', 'necessary', 'case', 'evaluate', 'circumstance', 'order', 'establish', 'reasonable_expect', 'child', 'leave', 'parent', 'emphasise', 'follow', 'passage', 'conclusion', 'evaluation', 'reasonable', 'hypothesis', 'parent', 'leave', 'abandon', 'family', 'entitle', 'leave', 'remain', 'spell', 'case', 'qualify', 'child', 'paragraph', 'ade', 'case', 'parent', 'article', 'apply', 'section', 'applying', 'case', 'section', 'engage', 'public', 'require', 'appellant', 'leave', 'express', 'requirement', 'section', 'give', 'public', 'factor', 'set', 'remainder', 'section', 'attract', 'materially', 'adverse', 'weight', 'submission', 'proceed', 'basis', 'public', 'removal', 'preserve', 'finding', 'section', 'meet', 'proportionate', 'remove', 'respect', 'fact', 'section', 'meet', 'mean', 'public', 'removal', 'basis', 'difficult', 'understand', 'circumstance', 'case', 'proportionate', 'give', 'appellant', 'able', 'stay', 'family', 'long', 'stay', 'give', 'red', 'list', 'unclear', 'unclear', 'long', 'able', 'return', 'certainly', 'reasonable', 'require', 'wife', 'stay', 'give', 'reason', 'difficult', 'live', 'return', 'proper', 'period', 'significant', 'difficulty', 'certainly', 'advice', 'far', 'understand', 'current', 'advice', 'government', 'go', 'list', 'good', 'reason', 'fact', 'appellant', 'parental', 'relationship', 'child', 'expect', 'leave', 'united', 'give', 'section', 'apply', 'sufficiently', 'compelling', 'circumstance', 'removal', 'proportionate', 'reason', 'take', 'account', 'factor', 'relevant', 'section', 'conclude', 'require', 'appellant', 'leave', 'amount', 'interference', 'article', 'right', 'allow', 'appeal', 'basis', 'conclusion', 'decision', 'tier', 'tribunal', 'involve', 'make', 'error', 'law', 'set', 'aside', 'remake', 'decision', 'allow', 'appeal', 'article', 'ground', 'notice', 'decision', 'decision', 'tier', 'tribunal', 'involve', 'make', 'error', 'law', 'set', 'aside', 'remake', 'appeal', 'allow', 'appeal', 'human', 'right', 'ground', 'anonymity', 'direction', 'upper', 'tribunal', 'error', 'law', 'decision', 'pic', 'upper', 'tribunal', 'immigration', 'asylum', 'chamber', 'appeal', 'number', 'immigration', 'act', 'decide', 'rule', 'decision', 'reason', 'promulgate', 'hear', 'field_house', 'upper', 'tribunal', 'iqbal', 'anonymity', 'direction', 'appellant', 'respondent', 'decision', 'reason', 'appellant', 'appeal', 'permission', 'decision', 'tier', 'tribunal', 'judge', 'phull', 'promulgate', 'dismiss', 'appeal', 'nationality', 'act', 'decision', 'respondent', 'refuse', 'leave', 'remain', 'human', 'right', 'claim', 'seek', 'leave', 'remain', 'basis', 'relationship', 'partner', 'settle', 'son', 'early', 'relationship', 'british', 'judge', 'appellant', 'establish', 'family', 'life', 'partner', 'son', 'reasonable_expect', 'son', 'leave', 'removal', 'proportionate', 'sponsor', 'support', 'application', 'return', 'appellant', 'seek', 'permission', 'appeal', 'ground', 'err', 'make', 'find', 'parental', 'relationship', 'material', 'act', 'engage', 'reasonable_expect', 'son', 'leave', 'err', 'assessment', 'proportionality', 'tier', 'tribunal', 'judge', 'adio', 'grant', 'permission', 'ground', 'upper', 'tribunal', 'judge', 'give', 'direction', 'provide', 'matter', 'review', 'file', 'case', 'light', 'present', 'need_precaution', 'overriding_objective', 'express', 'procedure', 'rule', 'reach', 'provisional', 'view', 'case', 'appropriate', 'determine', 'follow', 'question', 'hear', 'make', 'tier', 'tribunal', 'decision', 'involve', 'make', 'error', 'law', 'decision', 'set', 'aside', 'follow', 'direction', 'appellant', 'submission', 'support', 'assertion', 'error', 'law', 'question', 'tier', 'tribunal', 'decision', 'set', 'error', 'law', 'file', 'serve', 'party', 'later', 'day', 'notice', 'send', 'date', 'send', 'cover', 'letter', 'covering_email', 'party', 'file_serve', 'submission', 'response', 'later', 'day', 'notice', 'send', 'iii', 'submission', 'accordance', 'party', 'seek', 'permission', 'appeal', 'file_serve', 'reply', 'late', 'day', 'notice', 'send', 'iv', 'submission', 'rely', 'document', 'previously', 'provide', 'party', 'electronic', 'form', 'accompany', 'electronic', 'copy', 'document', 'party', 'consider', 'forego', 'direction', 'hear', 'necessary', 'consider', 'question', 'set', 'paragraph', 'submit', 'reason', 'view', 'later', 'day', 'notice', 'send', 'take', 'account', 'tribunal', 'direction', 'paragraph', 'comply', 'case', 'respondent', 'reply', 'state', 'oppose', 'application', 'permission', 'invite', 'upper', 'tribunal', 'determine', 'appeal', 'oral', 'hearing', 'consider', 'parental', 'relationship', 'exist', 'appellant', 'partner', 'son', 'tribunal', 'power', 'decision', 'hear', 'rule', 'procedure', 'rule', 'rule', 'require', 'regard', 'view', 'party', 'overriding_objective', 'rule', 'enable', 'tribunal', 'deal', 'case', 'fairly_justly', 'bearing_mind', 'concession', 'respondent', 'satisfy', 'particular', 'circumstance', 'case', 'correct', 'decision', 'absence', 'hear', 'satisfied', 'judge', 'err', 'reach', 'decision', 'claim', 'ground', 'appeal', 'accept', 'respondent', 'decision', 'clearly', 'involve', 'make', 'error', 'law', 'claim', 'error', 'go', 'weight', 'attach', 'public', 'operation', 'section', 'act', 'require', 'find', 'parental', 'relationship', 'exist', 'appellant', 'partner', 'son', 'error', 'infect', 'finding', 'proportionality', 'set', 'judge', 'err', 'consider', 'reasonable_expect', 'appellant', 'partner', 'give', 'finding', 'reasonable_expect', 'son', 'leave', 'consider', 'finding', 'family', 'life', 'judge', 'phull', 'preserve', 'necessary', 'upper', 'tribunal', 'finding', 'exist', 'parental', 'relationship', 'fresh', 'finding', 'proportionality', 'event', 'give', 'find', 'reasonable_expect', 'appellant', 'partner', 'leave', 'notice', 'decision', 'direction', 'decision', 'tier', 'tribunal', 'involve', 'make', 'error', 'law', 'set', 'appeal', 'remade', 'upper', 'tribunal', 'date', 'fix', 'regard', 'pilot_practice', 'direction', 'utiac', 'guidance', 'note', 'upper', 'tribunal', 'provisionally', 'view', 'forthcoming', 'hearing', 'appeal', 'hold', 'face', 'face', 'date', 'fix', 'necessary', 'oral', 'evidence', 'court', 'interpreter', 'party', 'wishing', 'adduce', 'evidence', 'serve', 'work', 'day', 'hear', 'accompany', 'application', 'pursuant', 'rule', 'tribunal', 'procedure', 'upper', 'tribunal', 'rule', 'explain', 'permit', 'sign', 'date', 'upper', 'tribunal', 'judge', 'rintoul', 'overriding_objective', 'enable', 'upper', 'tribunal', 'deal', 'case', 'fairly_justly', 'rule', 'tribunal', 'procedure', 'upper', 'tribunal', 'rule', 'rule']]\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(corpus_clean)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en_core_web_sm' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corp = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corp[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corp[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corp,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corp))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corp, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building LDA Mallet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building LDA Mallet Model\n",
    "\n",
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "mallet_path = './data/mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What topic each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most representative document for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da6e46dab1e0b3d9fa32aec1170dd2df7038a4f7be3a54c97a348d8ad782954"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tfm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
